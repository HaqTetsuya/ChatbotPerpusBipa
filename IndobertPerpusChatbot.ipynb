{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HaqTetsuya/ChatbotPerpusBipa/blob/main/IndobertPerpusChatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baq_vbCGocRh",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title download dependancy\n",
        "!pip install transformers torch pandas scikit-learn matplotlib seaborn tqdm deep-translator fuzzywuzzy Levenshtein\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8eolkt8hn4u",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title import dependency, load drive, and github {\"form-width\":\"20%\"}\n",
        "!git clone https://github.com/HaqTetsuya/ChatbotPerpusBipa.git\n",
        "\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import seaborn as sns\n",
        "from google.colab import drive, files\n",
        "from plotly.subplots import make_subplots\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
        "#from tqdm import tqdm\n",
        "from tqdm.auto import tqdm  # If you need both tqdm and tqdm.auto\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "FName = \"models\" #@param {type:\"string\"}\n",
        "\n",
        "# Update MODEL_SAVE_PATH with user input\n",
        "MODEL_SAVE_PATH = f\"/content/drive/MyDrive/{FName}\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Kelas Dataset untuk IndoBERT\n",
        "class IntentDataset(Dataset):\n",
        "    \"\"\"Dataset untuk klasifikasi intent dengan IndoBERT\"\"\"\n",
        "\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Convert dict of tensors to flat tensors\n",
        "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
        "        item['labels'] = torch.tensor(label)\n",
        "\n",
        "        return item"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FWHhQIZ43RjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqQ8ahtVzC6r",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title load data\n",
        "\n",
        "def load_csv_data(csv_path, label_encoder=None, show_distribution=False):\n",
        "    \"\"\"Memuat data intent dari file CSV. Bisa untuk train/test tanpa split.\"\"\"\n",
        "    print(f\"\\nMemuat data dari: {csv_path}\")\n",
        "\n",
        "    if not os.path.exists(csv_path):\n",
        "        raise FileNotFoundError(f\"File tidak ditemukan: {csv_path}\")\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    if 'text' not in df.columns or 'intent' not in df.columns:\n",
        "        raise ValueError(\"Kolom 'text' dan 'intent' harus ada di CSV\")\n",
        "\n",
        "    if label_encoder is None:\n",
        "        label_encoder = LabelEncoder()\n",
        "        df['intent_encoded'] = label_encoder.fit_transform(df['intent'])\n",
        "        intent_classes = label_encoder.classes_\n",
        "        print(f\"Label encoder baru dibuat dari data {csv_path}\")\n",
        "    else:\n",
        "        df['intent_encoded'] = label_encoder.transform(df['intent'])\n",
        "        intent_classes = label_encoder.classes_\n",
        "        print(f\"Menggunakan label encoder yang sudah ada\")\n",
        "\n",
        "    if show_distribution:\n",
        "        intent_counts = df['intent'].value_counts()\n",
        "        print(\"\\nDistribusi intent:\")\n",
        "        for intent, count in intent_counts.items():\n",
        "            print(f\"  {intent}: {count}\")\n",
        "\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        sns.barplot(x=intent_counts.index, y=intent_counts.values, palette=\"viridis\")\n",
        "        plt.xlabel(\"Intent\")\n",
        "        plt.ylabel(\"Jumlah Sampel\")\n",
        "        plt.title(\"Distribusi Intent\")\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.pie(intent_counts, labels=intent_counts.index, autopct='%1.1f%%', colors=sns.color_palette(\"viridis\", len(intent_counts)))\n",
        "        plt.title(\"Proporsi Intent\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    texts = df['text'].values\n",
        "    labels = df['intent_encoded'].values\n",
        "\n",
        "    return texts, labels, intent_classes, label_encoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMP8y1EczkWo",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title  setup model IndoBERT\n",
        "def setup_indobert_for_intent(num_labels):\n",
        "    \"\"\"Load model IndoBERT untuk klasifikasi intent\"\"\"\n",
        "\n",
        "    print(\"Memuat model IndoBERT...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p2\")\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        \"indobenchmark/indobert-base-p2\",\n",
        "        num_labels=num_labels\n",
        "    )\n",
        "    print(\"Model berhasil dimuat\")\n",
        "\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title OOD\n",
        "def enhanced_calibrate_ood(model, tokenizer, val_dataloader, save_path, temperature=1.0, percentile=85, margin=0.1):\n",
        "    \"\"\"\n",
        "    Calibrate and save OOD thresholds with adjustable tolerance\n",
        "    \"\"\"\n",
        "    print(\"Calibrating threshold for OOD detection...\")\n",
        "    thresholds = calibrate_ood_detection(model, tokenizer, val_dataloader,\n",
        "                                        temperature=temperature,\n",
        "                                        percentile=percentile,\n",
        "                                        margin=margin)\n",
        "\n",
        "    print(f\"Energy threshold: {thresholds['energy_threshold']:.4f}\")\n",
        "    print(f\"MSP threshold: {thresholds['msp_threshold']:.4f}\")\n",
        "\n",
        "    # Save thresholds\n",
        "    save_ood_thresholds(thresholds, save_path)\n",
        "\n",
        "    return thresholds\n",
        "\n",
        "def calibrate_ood_detection(model, tokenizer, dataloader, temperature=1.0, percentile=70, margin=0.1):\n",
        "    \"\"\"\n",
        "    Calibrate thresholds for OOD detection using in-distribution data\n",
        "    with Energy-based and MSP (Maximum Softmax Probability) methods\n",
        "    \"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "\n",
        "    # For Energy method and MSP method\n",
        "    energy_scores = []\n",
        "    msp_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Calibrating OOD detection\"):\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            # Energy score (higher values for OOD)\n",
        "            energy = -temperature * torch.logsumexp(logits / temperature, dim=1)\n",
        "            energy_scores.extend(energy.cpu().numpy())\n",
        "\n",
        "            # MSP score (lower values for OOD)\n",
        "            softmax_probs = F.softmax(logits, dim=1)\n",
        "            max_probs, _ = torch.max(softmax_probs, dim=1)\n",
        "            msp_scores.extend(max_probs.cpu().numpy())\n",
        "\n",
        "    # Calculate threshold for Energy with margin (make more tolerant)\n",
        "    base_energy_threshold = np.percentile(energy_scores, percentile)\n",
        "    # Apply margin to make more tolerant (increase threshold)\n",
        "    energy_threshold = base_energy_threshold * (1 + margin)\n",
        "\n",
        "    # Calculate threshold for MSP with margin\n",
        "    base_msp_threshold = np.percentile(msp_scores, 100 - percentile)\n",
        "    # Apply margin to make more tolerant (decrease threshold)\n",
        "    msp_threshold = base_msp_threshold * (1 - margin)\n",
        "\n",
        "    return {\n",
        "        \"energy_threshold\": float(energy_threshold),\n",
        "        \"msp_threshold\": float(msp_threshold)\n",
        "    }\n",
        "\n",
        "def predict_with_ood_detection(model, tokenizer, text, thresholds, temperature=1.0, tolerance_factor=1.0):\n",
        "    \"\"\"\n",
        "    Predict with adjustable OOD detection tolerance\n",
        "    \"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "\n",
        "    # Apply the tolerance factor to thresholds\n",
        "    energy_threshold = thresholds[\"energy_threshold\"] * tolerance_factor\n",
        "    msp_threshold = thresholds[\"msp_threshold\"] / tolerance_factor if thresholds[\"msp_threshold\"] else None\n",
        "\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Energy score\n",
        "        energy = -temperature * torch.logsumexp(logits / temperature, dim=1)\n",
        "        energy_score = energy.item()\n",
        "\n",
        "        # MSP score\n",
        "        softmax_probs = F.softmax(logits, dim=1)\n",
        "        max_probs, predicted_class = torch.max(softmax_probs, dim=1)\n",
        "        msp_score = max_probs.item()\n",
        "\n",
        "        # OOD detection\n",
        "        is_ood_energy = energy_score > energy_threshold\n",
        "        is_ood_msp = msp_score < msp_threshold if msp_threshold else False\n",
        "\n",
        "        # Combined OOD detection (can adjust this logic for tolerance)\n",
        "        is_ood = is_ood_energy  # You can use different combinations\n",
        "\n",
        "        return {\n",
        "            \"prediction\": predicted_class.item(),\n",
        "            \"confidence\": msp_score,\n",
        "            \"energy_score\": energy_score,\n",
        "            \"is_ood\": is_ood,\n",
        "            \"is_ood_energy\": is_ood_energy,\n",
        "            \"is_ood_msp\": is_ood_msp\n",
        "        }\n",
        "\n",
        "def save_ood_thresholds(thresholds, save_path):\n",
        "    \"\"\"\n",
        "    Save OOD thresholds to JSON file\n",
        "    \"\"\"\n",
        "    threshold_file = os.path.join(save_path, \"ood_thresholds.json\")\n",
        "    with open(threshold_file, 'w') as f:\n",
        "        json.dump(thresholds, f, indent=4)\n",
        "    print(f\"OOD thresholds saved at {threshold_file}\")\n",
        "    return threshold_file\n",
        "\n",
        "def load_ood_thresholds(model_path):\n",
        "    \"\"\"\n",
        "    Load OOD thresholds from JSON file\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(os.path.join(model_path, \"ood_thresholds.json\"), 'r') as f:\n",
        "            thresholds = json.load(f)\n",
        "            return thresholds\n",
        "    except FileNotFoundError:\n",
        "        try:\n",
        "            with open(os.path.join(model_path, \"ood_threshold.json\"), 'r') as f:\n",
        "                threshold_data = json.load(f)\n",
        "                return {\n",
        "                    \"energy_threshold\": threshold_data[\"energy_threshold\"],\n",
        "                    \"msp_threshold\": None\n",
        "                }\n",
        "        except FileNotFoundError:\n",
        "            print(\"Warning: OOD threshold files not found. Using default thresholds.\")\n",
        "            return {\n",
        "                \"energy_threshold\": 0.0,\n",
        "                \"msp_threshold\": 0.5\n",
        "            }"
      ],
      "metadata": {
        "id": "APTL6Ht96u9Q",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Focal LOSS\n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss\n"
      ],
      "metadata": {
        "id": "VFbqGGCtvhG5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNCPJ3uFzp2V",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Training function\n",
        "\n",
        "def train_intent_classifier(model, tokenizer, train_texts, train_labels, val_texts, val_labels,\n",
        "                           batch_size=16, epochs=10, learning_rate=2e-5, weight_decay=0.01,\n",
        "                           save_path=MODEL_SAVE_PATH, use_class_weights=True, patience=3, class_names=None):\n",
        "    \"\"\"\n",
        "    Melatih model IndoBERT untuk klasifikasi intent dengan perbaikan:\n",
        "    - Enhanced visualization (interactive and static)\n",
        "    - Per-class metric tracking\n",
        "    - Confusion matrix generation\n",
        "    - Batch-level metric tracking\n",
        "    - Learning rate visualization\n",
        "    \"\"\"\n",
        "\n",
        "    # Persiapkan dataset\n",
        "    print(\"Menyiapkan dataset...\")\n",
        "    train_dataset = IntentDataset(train_texts, train_labels, tokenizer)\n",
        "    val_dataset = IntentDataset(val_texts, val_labels, tokenizer)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Compute class weights if needed\n",
        "    class_weights = None\n",
        "    if use_class_weights:\n",
        "        unique_classes = np.unique(train_labels)\n",
        "        weights = compute_class_weight(\n",
        "            class_weight='balanced',\n",
        "            classes=unique_classes,\n",
        "            y=train_labels\n",
        "        )\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        class_weights = torch.FloatTensor(weights).to(device)\n",
        "        print(f\"Menggunakan class weights: {weights}\")\n",
        "\n",
        "    # Optimizer dengan weight decay untuk regularisasi\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    # Scheduler dengan warmup\n",
        "    num_training_steps = len(train_dataloader) * epochs\n",
        "    num_warmup_steps = int(0.1 * num_training_steps)  # 10% warmup\n",
        "    scheduler = get_scheduler(\"cosine\", optimizer=optimizer,\n",
        "                             num_warmup_steps=num_warmup_steps,\n",
        "                             num_training_steps=num_training_steps)\n",
        "\n",
        "    # Cek untuk GPU\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Menggunakan device: {device}\")\n",
        "    model.to(device)\n",
        "    print(f\"Mulai pelatihan model...\")\n",
        "    print(f\"Total epoch: {epochs}, batch size: {batch_size}, learning rate: {learning_rate}, weight decay: {weight_decay}\")\n",
        "\n",
        "    # Create loss function with class weights if needed\n",
        "    # Create Focal Loss with class weights\n",
        "    loss_fn = FocalLoss(alpha=class_weights, gamma=2.0)\n",
        "    print(\"Menggunakan Focal Loss dengan gamma=2.0\")\n",
        "\n",
        "    # Initialize training history\n",
        "    history = initialize_training_history()\n",
        "    best_val_loss = float('inf')\n",
        "    counter = 0  # Counter for early stopping\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        train_loss = run_training_epoch(model, device, train_dataloader, optimizer,\n",
        "                                        scheduler, loss_fn, epoch, epochs, history)\n",
        "        history['train_loss'].append(train_loss)\n",
        "\n",
        "        # Validation phase\n",
        "        val_metrics = run_validation_epoch(model, device, val_dataloader, loss_fn, epoch, epochs)\n",
        "        update_history_with_validation_metrics(history, val_metrics)\n",
        "\n",
        "        # Update per-class metrics if class names are provided\n",
        "        if class_names is not None:\n",
        "            update_per_class_metrics(history, val_metrics['all_labels'], val_metrics['all_preds'])\n",
        "\n",
        "            # Generate confusion matrix for this epoch\n",
        "            plot_confusion_matrix(val_metrics['all_labels'], val_metrics['all_preds'], class_names, epoch, save_path)\n",
        "\n",
        "        # Print detailed metrics\n",
        "        print_epoch_metrics(epoch, epochs, train_loss, val_metrics)\n",
        "\n",
        "        # Early stopping and model saving logic\n",
        "        counter = handle_early_stopping(model, tokenizer, val_metrics['avg_val_loss'],\n",
        "                                      best_val_loss, counter, patience,\n",
        "                                      save_path, val_metrics['all_labels'],\n",
        "                                      val_metrics['all_preds'])\n",
        "\n",
        "        if counter >= patience:\n",
        "            print(f\"Early stopping triggered setelah {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "        # Update best validation loss if improved\n",
        "        if val_metrics['avg_val_loss'] < best_val_loss:\n",
        "            best_val_loss = val_metrics['avg_val_loss']\n",
        "\n",
        "    print(f\"Pelatihan selesai! Model terbaik disimpan di {save_path}\")\n",
        "\n",
        "    # Generate enhanced visualizations\n",
        "    enhanced_plot_training_results(history, save_path, class_names=class_names)\n",
        "\n",
        "    # Simpan history ke file JSON\n",
        "    save_enhanced_history(history, save_path)\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def initialize_training_history():\n",
        "    \"\"\"Initialize the training history dictionary with all required keys\"\"\"\n",
        "    return {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'val_accuracy': [],\n",
        "        'val_f1': [],\n",
        "        'val_precision': [],\n",
        "        'val_recall': [],\n",
        "        'batch_metrics': {\n",
        "            'iteration': [],\n",
        "            'loss': [],\n",
        "            'epoch': [],\n",
        "            'progress': [],\n",
        "            'learning_rates': []\n",
        "        },\n",
        "        'class_f1': [],  # Per-class F1 scores\n",
        "        'class_precision': [],  # Per-class precision\n",
        "        'class_recall': []  # Per-class recall\n",
        "    }\n",
        "\n",
        "def run_training_epoch(model, device, train_dataloader, optimizer, scheduler, loss_fn, epoch, epochs, history):\n",
        "    \"\"\"Run a single training epoch and return average loss\"\"\"\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    print(f\"\\nEpoch {epoch+1}/{epochs} - Training dimulai...\")\n",
        "    progress_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader),\n",
        "                      desc=f\"Epoch {epoch+1}/{epochs} [Training]\", leave=False)\n",
        "\n",
        "    for batch_idx, batch in progress_bar:\n",
        "        try:\n",
        "            # Pindahkan batch ke device\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Forward pass with custom loss function\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(**inputs)\n",
        "            loss = loss_fn(outputs.logits, labels)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Track batch-level metrics\n",
        "            update_batch_metrics(history, batch_idx, loss.item(), epoch, len(train_dataloader), optimizer)\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            if \"out of memory\" in str(e):\n",
        "                print(\"Peringatan: Kehabisan memori! Membersihkan cache...\")\n",
        "                torch.cuda.empty_cache()\n",
        "                continue\n",
        "            else:\n",
        "                raise e\n",
        "\n",
        "    return train_loss / len(train_dataloader)\n",
        "\n",
        "def update_batch_metrics(history, batch_idx, loss_item, epoch, iterations_per_epoch, optimizer):\n",
        "    \"\"\"Update history with batch-level metrics\"\"\"\n",
        "    global_iteration = epoch * iterations_per_epoch + batch_idx\n",
        "    progress = (epoch + (batch_idx / iterations_per_epoch)) * 100\n",
        "\n",
        "    history['batch_metrics']['iteration'].append(global_iteration)\n",
        "    history['batch_metrics']['loss'].append(loss_item)\n",
        "    history['batch_metrics']['epoch'].append(epoch)\n",
        "    history['batch_metrics']['progress'].append(progress)\n",
        "    history['batch_metrics']['learning_rates'].append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "def run_validation_epoch(model, device, val_dataloader, loss_fn, epoch, epochs):\n",
        "    \"\"\"Run a single validation epoch and return metrics\"\"\"\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Validasi dimulai...\")\n",
        "    progress_bar = tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Validation]\", leave=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in progress_bar:\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            loss = loss_fn(outputs.logits, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            predictions = torch.argmax(outputs.logits, dim=1)\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            # Collect predictions and labels for metrics\n",
        "            all_preds.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
        "\n",
        "    # Calculate metrics\n",
        "    avg_val_loss = val_loss / len(val_dataloader)\n",
        "    accuracy = correct / total\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "\n",
        "    return {\n",
        "        'avg_val_loss': avg_val_loss,\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'all_preds': all_preds,\n",
        "        'all_labels': all_labels\n",
        "    }\n",
        "\n",
        "def update_history_with_validation_metrics(history, val_metrics):\n",
        "    \"\"\"Update history with validation metrics\"\"\"\n",
        "    history['val_loss'].append(val_metrics['avg_val_loss'])\n",
        "    history['val_accuracy'].append(val_metrics['accuracy'])\n",
        "    history['val_f1'].append(val_metrics['f1'])\n",
        "    history['val_precision'].append(val_metrics['precision'])\n",
        "    history['val_recall'].append(val_metrics['recall'])\n",
        "\n",
        "def update_per_class_metrics(history, all_labels, all_preds):\n",
        "    \"\"\"Update per-class metrics in history\"\"\"\n",
        "    class_f1 = f1_score(all_labels, all_preds, average=None, zero_division=0)\n",
        "    class_precision = precision_score(all_labels, all_preds, average=None, zero_division=0)\n",
        "    class_recall = recall_score(all_labels, all_preds, average=None, zero_division=0)\n",
        "\n",
        "    history['class_f1'].append(class_f1.tolist())\n",
        "    history['class_precision'].append(class_precision.tolist())\n",
        "    history['class_recall'].append(class_recall.tolist())\n",
        "\n",
        "def print_epoch_metrics(epoch, epochs, avg_train_loss, val_metrics):\n",
        "    \"\"\"Print detailed metrics for an epoch\"\"\"\n",
        "    print(f\"Epoch {epoch+1}/{epochs}:\")\n",
        "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
        "    print(f\"  Val Loss: {val_metrics['avg_val_loss']:.4f}, Val Accuracy: {val_metrics['accuracy']*100:.2f}%\")\n",
        "    print(f\"  Val F1: {val_metrics['f1']:.4f}, Val Precision: {val_metrics['precision']:.4f}, Val Recall: {val_metrics['recall']:.4f}\")\n",
        "    if 'all_labels' in val_metrics and 'all_preds' in val_metrics:\n",
        "        print(f\"\\nClass-wise precision/recall/F1 setelah epoch {epoch+1}:\")\n",
        "        print(classification_report(val_metrics['all_labels'], val_metrics['all_preds'], digits=4))\n",
        "def handle_early_stopping(model, tokenizer, avg_val_loss, best_val_loss, counter, patience, save_path, all_labels, all_preds):\n",
        "    \"\"\"Handle early stopping logic and model saving\"\"\"\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        counter = 0  # Reset early stopping counter\n",
        "\n",
        "        if not os.path.exists(save_path):\n",
        "            os.makedirs(save_path)\n",
        "\n",
        "        print(f\"Menyimpan model terbaik ke {save_path}\")\n",
        "        model.save_pretrained(save_path)\n",
        "        tokenizer.save_pretrained(save_path)\n",
        "\n",
        "        # Save classification report for best model\n",
        "        report = classification_report(all_labels, all_preds, output_dict=True)\n",
        "        with open(os.path.join(save_path, \"classification_report.json\"), 'w') as f:\n",
        "            json.dump(report, f, indent=4)\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f\"Validation loss tidak membaik. Early stopping counter: {counter}/{patience}\")\n",
        "\n",
        "    return counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWFt6WSazwgT",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Evaluation\n",
        "def evaluate_model_enhanced(model, tokenizer, val_texts, val_labels, intent_classes, save_path):\n",
        "    \"\"\"Enhanced model evaluation with better visualizations\"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    # Prepare dataset and dataloader\n",
        "    val_dataset = IntentDataset(val_texts, val_labels, tokenizer)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Evaluation loop\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_dataloader, desc=\"Evaluasi Model\"):\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            predictions = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "            all_preds.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Generate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Generate classification report\n",
        "    report = classification_report(all_labels, all_preds,\n",
        "                                 target_names=intent_classes,\n",
        "                                 output_dict=True)\n",
        "\n",
        "    # Create dataframe for visualization\n",
        "    report_df = pd.DataFrame(report).transpose()\n",
        "    report_df = report_df.round(3)\n",
        "\n",
        "    # Filter for class metrics only (exclude summary rows)\n",
        "    class_df = report_df.loc[intent_classes]\n",
        "    metrics = ['precision', 'recall', 'f1-score']\n",
        "\n",
        "    # Save classification metrics as CSV\n",
        "    report_df.to_csv(os.path.join(save_path, \"classification_report.csv\"))\n",
        "\n",
        "    # Create visualizations\n",
        "    create_static_visualizations(cm, class_df, metrics, intent_classes, save_path)\n",
        "    create_interactive_visualizations(cm, class_df, metrics, intent_classes, save_path)\n",
        "\n",
        "    # Print report summary\n",
        "    print(\"\\nModel Evaluation Report:\")\n",
        "    print(f\"Overall Accuracy: {report['accuracy']:.4f}\")\n",
        "    print(f\"Macro F1-score: {report['macro avg']['f1-score']:.4f}\")\n",
        "    print(f\"Weighted F1-score: {report['weighted avg']['f1-score']:.4f}\")\n",
        "\n",
        "    return report, cm\n",
        "\n",
        "def create_static_visualizations(cm, class_df, metrics, intent_classes, save_path):\n",
        "    \"\"\"Create and save static matplotlib visualizations\"\"\"\n",
        "    # Confusion matrix\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "               xticklabels=intent_classes, yticklabels=intent_classes)\n",
        "    plt.title('Confusion Matrix - Final Model')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_path, \"final_confusion_matrix.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    # Classification report heatmap\n",
        "    plt.figure(figsize=(12, len(intent_classes)*0.5 + 3))\n",
        "    sns.heatmap(class_df[metrics], annot=True, cmap='YlGnBu', fmt='.3f',\n",
        "               yticklabels=intent_classes, cbar=True)\n",
        "    plt.title('Performance Metrics by Intent Class')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_path, \"class_performance_metrics.png\"))\n",
        "    plt.close()\n",
        "\n",
        "def create_interactive_visualizations(cm, class_df, metrics, intent_classes, save_path):\n",
        "    \"\"\"Create and save interactive plotly visualizations\"\"\"\n",
        "    # Interactive confusion matrix\n",
        "    fig_cm = px.imshow(cm,\n",
        "                   labels=dict(x=\"Predicted Label\", y=\"True Label\", color=\"Count\"),\n",
        "                   x=intent_classes, y=intent_classes,\n",
        "                   text_auto=True,\n",
        "                   color_continuous_scale='Blues')\n",
        "\n",
        "    fig_cm.update_layout(\n",
        "        title='Confusion Matrix (Interactive)',\n",
        "        width=900,\n",
        "        height=800\n",
        "    )\n",
        "    fig_cm.write_html(os.path.join(save_path, \"interactive_confusion_matrix.html\"))\n",
        "\n",
        "    # Interactive performance metrics\n",
        "    fig_perf = px.imshow(class_df[metrics],\n",
        "                   labels=dict(x=\"Metric\", y=\"Intent Class\", color=\"Score\"),\n",
        "                   x=metrics, y=intent_classes,\n",
        "                   text_auto=True,\n",
        "                   color_continuous_scale='YlGnBu')\n",
        "\n",
        "    fig_perf.update_layout(\n",
        "        title='Performance Metrics by Intent Class (Interactive)',\n",
        "        width=800,\n",
        "        height=max(400, len(intent_classes)*30)\n",
        "    )\n",
        "    fig_perf.write_html(os.path.join(save_path, \"interactive_class_performance.html\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title plot diagram\n",
        "def enhanced_plot_training_results(history, save_path, class_names=None):\n",
        "    \"\"\"\n",
        "    Enhanced function to plot training results with more detailed visualizations\n",
        "\n",
        "    Args:\n",
        "        history: Dictionary containing training history metrics\n",
        "        save_path: Path to save visualization files\n",
        "        class_names: Optional list of class names for confusion matrix\n",
        "    \"\"\"\n",
        "    # Create the static plots (same as before for compatibility)\n",
        "    static_plot_training_results(history, save_path)\n",
        "\n",
        "    # Create interactive plotly visualizations\n",
        "    interactive_plot_training_results(history, save_path)\n",
        "\n",
        "    # If we have class metrics in our history, plot those too\n",
        "    if 'class_f1' in history and class_names is not None:\n",
        "        plot_class_metrics(history, save_path, class_names)\n",
        "\n",
        "    # If we tracked learning rates, plot those\n",
        "    if 'batch_metrics' in history and 'learning_rates' in history['batch_metrics']:\n",
        "        plot_learning_rate(history, save_path)\n",
        "\n",
        "def static_plot_training_results(history, save_path):\n",
        "    \"\"\"Plot and save training metrics using matplotlib (static)\"\"\"\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Plot 1: Loss\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(history['train_loss'], label='Training Loss', marker='o')\n",
        "    plt.plot(history['val_loss'], label='Validation Loss', marker='o')\n",
        "    plt.title('Loss selama Training')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot 2: Accuracy\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(history['val_accuracy'], label='Validation Accuracy', marker='o', color='green')\n",
        "    plt.title('Akurasi selama Training')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot 3: F1 Score\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(history['val_f1'], label='Validation F1', marker='o', color='purple')\n",
        "    plt.title('F1 Score selama Training')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('F1 Score')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot 4: Precision & Recall\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(history['val_precision'], label='Validation Precision', marker='o', color='orange')\n",
        "    plt.plot(history['val_recall'], label='Validation Recall', marker='o', color='brown')\n",
        "    plt.title('Precision & Recall selama Training')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the plot\n",
        "    plt.savefig(os.path.join(save_path, \"training_metrics.png\"))\n",
        "    plt.close()\n",
        "\n",
        "def interactive_plot_training_results(history, save_path):\n",
        "    \"\"\"Create interactive plotly visualizations of training metrics\"\"\"\n",
        "    # Create epochs list for x-axis\n",
        "    epochs = list(range(1, len(history['train_loss']) + 1))\n",
        "\n",
        "    # Create a DataFrame for easier plotting\n",
        "    df = pd.DataFrame({\n",
        "        'Epoch': epochs,\n",
        "        'Training Loss': history['train_loss'],\n",
        "        'Validation Loss': history['val_loss'],\n",
        "        'Validation Accuracy': history['val_accuracy'],\n",
        "        'Validation F1': history['val_f1'],\n",
        "        'Validation Precision': history['val_precision'],\n",
        "        'Validation Recall': history['val_recall']\n",
        "    })\n",
        "\n",
        "    # Create subplot figure\n",
        "    fig = make_subplots(\n",
        "        rows=2, cols=2,\n",
        "        subplot_titles=('Loss', 'Accuracy', 'F1 Score', 'Precision & Recall'),\n",
        "        vertical_spacing=0.15,\n",
        "        horizontal_spacing=0.1\n",
        "    )\n",
        "\n",
        "    # Add traces for each metric\n",
        "    # Loss plot\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=epochs, y=history['train_loss'], mode='lines+markers', name='Training Loss'),\n",
        "        row=1, col=1\n",
        "    )\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=epochs, y=history['val_loss'], mode='lines+markers', name='Validation Loss'),\n",
        "        row=1, col=1\n",
        "    )\n",
        "\n",
        "    # Accuracy plot\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=epochs, y=history['val_accuracy'], mode='lines+markers', name='Validation Accuracy', line=dict(color='green')),\n",
        "        row=1, col=2\n",
        "    )\n",
        "\n",
        "    # F1 Score plot\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=epochs, y=history['val_f1'], mode='lines+markers', name='Validation F1', line=dict(color='purple')),\n",
        "        row=2, col=1\n",
        "    )\n",
        "\n",
        "    # Precision & Recall plot\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=epochs, y=history['val_precision'], mode='lines+markers', name='Validation Precision', line=dict(color='orange')),\n",
        "        row=2, col=2\n",
        "    )\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=epochs, y=history['val_recall'], mode='lines+markers', name='Validation Recall', line=dict(color='brown')),\n",
        "        row=2, col=2\n",
        "    )\n",
        "\n",
        "    # Update layout\n",
        "    fig.update_layout(\n",
        "        height=800,\n",
        "        width=1200,\n",
        "        title_text=\"Training Metrics (Interactive)\",\n",
        "        hovermode=\"x unified\"\n",
        "    )\n",
        "\n",
        "    # Save interactive plot as HTML\n",
        "    fig.write_html(os.path.join(save_path, \"interactive_training_metrics.html\"))\n",
        "\n",
        "    # Create a combined metrics plot for better trend comparison\n",
        "    fig_combined = px.line(\n",
        "        df,\n",
        "        x='Epoch',\n",
        "        y=['Training Loss', 'Validation Loss', 'Validation Accuracy', 'Validation F1', 'Validation Precision', 'Validation Recall'],\n",
        "        title='All Training Metrics',\n",
        "        labels={'value': 'Metric Value', 'variable': 'Metric'}\n",
        "    )\n",
        "\n",
        "    fig_combined.update_layout(height=600, width=1000, hovermode=\"x unified\")\n",
        "    fig_combined.write_html(os.path.join(save_path, \"combined_metrics.html\"))\n",
        "\n",
        "def plot_confusion_matrix(all_labels, all_preds, class_names, epoch, save_path):\n",
        "    \"\"\"Plot and save confusion matrix for the epoch\"\"\"\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt='d',\n",
        "        cmap='Blues',\n",
        "        xticklabels=class_names,\n",
        "        yticklabels=class_names\n",
        "    )\n",
        "    plt.title(f'Confusion Matrix - Epoch {epoch+1}')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the confusion matrix\n",
        "    cm_dir = os.path.join(save_path, \"confusion_matrices\")\n",
        "    os.makedirs(cm_dir, exist_ok=True)\n",
        "    plt.savefig(os.path.join(cm_dir, f\"cm_epoch_{epoch+1}.png\"))\n",
        "    plt.close()\n",
        "\n",
        "def plot_class_metrics(history, save_path, class_names):\n",
        "    \"\"\"Plot per-class performance metrics\"\"\"\n",
        "    # Create directory for class metrics\n",
        "    os.makedirs(os.path.join(save_path, \"class_metrics\"), exist_ok=True)\n",
        "\n",
        "    # Plot F1 per class if available\n",
        "    if 'class_f1' in history:\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        # Convert dictionary structure to usable format\n",
        "        epochs = len(history['class_f1'])\n",
        "        x_epochs = list(range(1, epochs + 1))\n",
        "\n",
        "        for i, class_name in enumerate(class_names):\n",
        "            class_f1 = [history['class_f1'][epoch][i] for epoch in range(epochs)]\n",
        "            plt.plot(x_epochs, class_f1, marker='o', label=f'{class_name}')\n",
        "\n",
        "        plt.title('F1 Score per Class')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('F1 Score')\n",
        "        plt.grid(True)\n",
        "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(save_path, \"class_metrics\", \"f1_per_class.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        # Interactive version with plotly\n",
        "        fig = go.Figure()\n",
        "        for i, class_name in enumerate(class_names):\n",
        "            class_f1 = [history['class_f1'][epoch][i] for epoch in range(epochs)]\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=x_epochs,\n",
        "                y=class_f1,\n",
        "                mode='lines+markers',\n",
        "                name=class_name\n",
        "            ))\n",
        "\n",
        "        fig.update_layout(\n",
        "            title='F1 Score per Class (Interactive)',\n",
        "            xaxis_title='Epoch',\n",
        "            yaxis_title='F1 Score',\n",
        "            height=600,\n",
        "            width=1000,\n",
        "            hovermode=\"x unified\"\n",
        "        )\n",
        "        fig.write_html(os.path.join(save_path, \"class_metrics\", \"f1_per_class.html\"))\n",
        "\n",
        "def plot_learning_rate(history, save_path):\n",
        "    \"\"\"Plot learning rate changes over training\"\"\"\n",
        "    # Extract learning rates from batch metrics\n",
        "    iterations = history['batch_metrics']['iteration']\n",
        "    learning_rates = history['batch_metrics']['learning_rates']\n",
        "    epochs = history['batch_metrics']['epoch']\n",
        "\n",
        "    # Create the plot\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(iterations, learning_rates)\n",
        "    plt.title('Learning Rate Schedule')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_path, \"learning_rate_schedule.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    # Interactive version\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=iterations,\n",
        "        y=learning_rates,\n",
        "        mode='lines',\n",
        "        name='Learning Rate'\n",
        "    ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title='Learning Rate Schedule (Interactive)',\n",
        "        xaxis_title='Iteration',\n",
        "        yaxis_title='Learning Rate',\n",
        "        height=500,\n",
        "        width=900\n",
        "    )\n",
        "    fig.write_html(os.path.join(save_path, \"learning_rate_schedule.html\"))\n",
        "\n",
        "def save_enhanced_history(history, save_path):\n",
        "    \"\"\"Save enhanced training history with additional visualizations\"\"\"\n",
        "    # Convert numpy arrays to lists for JSON serialization\n",
        "    processed_history = {}\n",
        "\n",
        "    for key, value in history.items():\n",
        "        if isinstance(value, dict):\n",
        "            processed_history[key] = {}\n",
        "            for subkey, subvalue in value.items():\n",
        "                processed_history[key][subkey] = convert_to_serializable(subvalue)\n",
        "        else:\n",
        "            processed_history[key] = convert_to_serializable(value)\n",
        "\n",
        "    # Save the enhanced history\n",
        "    with open(os.path.join(save_path, \"enhanced_training_history.json\"), 'w') as f:\n",
        "        json.dump(processed_history, f, indent=4)\n",
        "\n",
        "    print(f\"Enhanced training history saved to {os.path.join(save_path, 'enhanced_training_history.json')}\")\n",
        "\n",
        "def convert_to_serializable(value):\n",
        "    \"\"\"Convert numpy arrays and other non-serializable types to JSON-compatible types\"\"\"\n",
        "    if isinstance(value, np.ndarray):\n",
        "        return value.tolist()\n",
        "    elif isinstance(value, list):\n",
        "        if value and isinstance(value[0], np.ndarray):\n",
        "            return [item.tolist() if isinstance(item, np.ndarray) else item for item in value]\n",
        "        else:\n",
        "            return value\n",
        "    else:\n",
        "        return value"
      ],
      "metadata": {
        "id": "5y2_yA7p1I_E",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Upp9G0xn0fN7",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title PredictionIntent\n",
        "def predict_intent_with_enhanced_ood(text, model, tokenizer, intent_classes,\n",
        "                                     energy_threshold, msp_threshold=None,\n",
        "                                     temperature=1.0, method='combined',\n",
        "                                     label_encoder=None, device=None,\n",
        "                                     return_logits=False):\n",
        "    \"\"\"\n",
        "    Memprediksi intent dari teks input dengan deteksi Out-of-Distribution yang ditingkatkan\n",
        "\n",
        "    Args:\n",
        "        text: Teks input untuk diprediksi\n",
        "        model: Model yang sudah dilatih\n",
        "        tokenizer: Tokenizer untuk model\n",
        "        intent_classes: List nama intent\n",
        "        energy_threshold: Threshold untuk energy-based OOD detection\n",
        "        msp_threshold: Threshold untuk MSP-based OOD detection\n",
        "        temperature: Parameter temperature untuk energy\n",
        "        method: Metode deteksi OOD - 'energy', 'msp', atau 'combined'\n",
        "        label_encoder: Label encoder untuk intent classes\n",
        "        device: Device untuk inference\n",
        "        return_logits: Jika True, mengembalikan logits asli\n",
        "\n",
        "    Returns:\n",
        "        dict: Hasil prediksi dengan detail OOD detection\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    if isinstance(text, str):\n",
        "        text = [text]  # Convert single text to list\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenisasi input\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # List untuk menyimpan hasil setiap input\n",
        "    results = []\n",
        "\n",
        "    # Prediksi\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Hitung energy score: -T*log(sum(exp(logits/T)))\n",
        "        energy = -temperature * torch.logsumexp(logits / temperature, dim=1)\n",
        "\n",
        "        # Hitung confidence dengan softmax\n",
        "        probabilities = torch.softmax(logits, dim=1)\n",
        "        predictions = torch.argmax(probabilities, dim=1)\n",
        "        max_probs = torch.max(probabilities, dim=1)[0]\n",
        "\n",
        "        for i in range(len(text)):\n",
        "            prediction = predictions[i].item()\n",
        "            energy_score = energy[i].item()\n",
        "            confidence = max_probs[i].item()\n",
        "\n",
        "            # Deteksi OOD berdasarkan metode yang dipilih\n",
        "            is_ood_energy = energy_score > energy_threshold if energy_threshold is not None else False\n",
        "            is_ood_msp = confidence < msp_threshold if msp_threshold is not None else False\n",
        "\n",
        "            if method == 'energy':\n",
        "                is_ood = is_ood_energy\n",
        "            elif method == 'msp':\n",
        "                is_ood = is_ood_msp\n",
        "            else:  # 'combined'\n",
        "                is_ood = is_ood_energy and is_ood_msp\n",
        "\n",
        "            # Tentukan intent berdasarkan hasil OOD detection\n",
        "            if is_ood:\n",
        "                predicted_intent = \"unknown\"\n",
        "                topk_intents = [(\"unknown\", 1.0)]  # Unknown intent dengan confidence 100%\n",
        "            else:\n",
        "                predicted_intent = intent_classes[prediction]\n",
        "\n",
        "                # Dapatkan top 3 intent dengan confidence tertinggi\n",
        "                top_k = min(3, len(intent_classes))\n",
        "                topk_values, topk_indices = torch.topk(probabilities[i], top_k)\n",
        "                topk_intents = [(intent_classes[idx.item()], val.item())\n",
        "                                for idx, val in zip(topk_indices, topk_values)]\n",
        "\n",
        "            # Buat hasil untuk input ini\n",
        "            result = {\n",
        "                \"text\": text[i],\n",
        "                \"intent\": predicted_intent,\n",
        "                \"confidence\": confidence,\n",
        "                \"energy_score\": energy_score,\n",
        "                \"is_ood\": is_ood,\n",
        "                \"is_ood_energy\": is_ood_energy,\n",
        "                \"is_ood_msp\": is_ood_msp if msp_threshold is not None else None,\n",
        "                \"top_intents\": topk_intents\n",
        "            }\n",
        "\n",
        "            if return_logits:\n",
        "                result[\"logits\"] = logits[i].cpu().numpy()\n",
        "\n",
        "            results.append(result)\n",
        "\n",
        "    # Jika hanya satu input, kembalikan hasil langsung tanpa list\n",
        "    if len(text) == 1:\n",
        "        return results[0]\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDr3k0Ir0rOU",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Run pipeline\n",
        "def run_full_pipeline_enhanced(\n",
        "    use_drive=True,\n",
        "    percentile=95,\n",
        "    ood_method='combined',\n",
        "    split_dataset=\"no\",\n",
        "    val_split=0.2,\n",
        "    batch_size=16,\n",
        "    epochs=10,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    patience=3,\n",
        "    train_csv_path=\"train.csv\",\n",
        "    val_csv_path=\"val.csv\"\n",
        "):\n",
        "    \"\"\"Jalankan pipeline lengkap dengan enhanced OOD detection dan visualisasi yang ditingkatkan\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    use_drive : bool\n",
        "        Apakah menggunakan Google Drive untuk penyimpanan\n",
        "    percentile : int\n",
        "        Persentil untuk threshold OOD detection\n",
        "    ood_method : str\n",
        "        Metode OOD detection ('msp', 'energy', 'combined')\n",
        "    split_dataset : str\n",
        "        Mode pemisahan dataset (\"yes\" untuk split dari train.csv, \"no\" untuk file terpisah)\n",
        "    val_split : float\n",
        "        Proporsi data validasi jika split_dataset=\"yes\"\n",
        "    batch_size : int\n",
        "        Ukuran batch untuk training dan evaluasi\n",
        "    epochs : int\n",
        "        Jumlah epoch training\n",
        "    learning_rate : float\n",
        "        Learning rate optimizer\n",
        "    weight_decay : float\n",
        "        Weight decay untuk regularisasi\n",
        "    patience : int\n",
        "        Early stopping patience\n",
        "    \"\"\"\n",
        "\n",
        "    os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
        "\n",
        "    train_texts, train_labels, val_texts, val_labels, intent_classes, label_encoder = prepare_data(\n",
        "        split_dataset, val_split, train_csv_path, val_csv_path)\n",
        "\n",
        "    num_labels = len(intent_classes)\n",
        "\n",
        "    model, tokenizer = setup_indobert_for_intent(num_labels)\n",
        "\n",
        "    model, history = train_intent_classifier(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        train_texts,\n",
        "        train_labels,\n",
        "        val_texts,\n",
        "        val_labels,\n",
        "        class_names=intent_classes,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        learning_rate=learning_rate,\n",
        "        weight_decay=weight_decay,\n",
        "        patience=patience\n",
        "    )\n",
        "\n",
        "    thresholds = calibrate_and_evaluate(model, tokenizer, val_texts, val_labels,\n",
        "                                       intent_classes, percentile)\n",
        "\n",
        "    save_model_artifacts(model, tokenizer, intent_classes, label_encoder, history)\n",
        "\n",
        "    print_summary(num_labels, intent_classes, thresholds)\n",
        "\n",
        "    run_prediction_demo_enhanced(model, tokenizer, intent_classes, label_encoder, method=ood_method)\n",
        "\n",
        "    return model, tokenizer, intent_classes, label_encoder\n",
        "\n",
        "\n",
        "def prepare_data(split_dataset, val_split, train_csv_path=\"train.csv\", val_csv_path=\"val.csv\"):\n",
        "    \"\"\"Prepare training and validation data based on split mode\"\"\"\n",
        "\n",
        "    if split_dataset.lower() == \"yes\":\n",
        "        all_texts, all_labels, intent_classes, label_encoder = load_csv_data(train_csv_path, show_distribution=True)\n",
        "\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "            all_texts, all_labels, test_size=val_split, random_state=42, stratify=all_labels\n",
        "        )\n",
        "\n",
        "        print(f\"\\n✅ Dataset telah dibagi: {len(train_texts)} data training dan {len(val_texts)} data validasi\")\n",
        "    else:\n",
        "        train_texts, train_labels, intent_classes, label_encoder = load_csv_data(train_csv_path, show_distribution=True)\n",
        "        val_texts, val_labels, _, _ = load_csv_data(val_csv_path, label_encoder=label_encoder, show_distribution=True)\n",
        "\n",
        "    return train_texts, train_labels, val_texts, val_labels, intent_classes, label_encoder\n",
        "\n",
        "\n",
        "def calibrate_and_evaluate(model, tokenizer, val_texts, val_labels, intent_classes, percentile):\n",
        "    \"\"\"Calibrate OOD detection and evaluate model\"\"\"\n",
        "    # Prepare validation dataloader for OOD calibration\n",
        "    val_dataset = IntentDataset(val_texts, val_labels, tokenizer)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "    # Calibrate OOD detection\n",
        "    thresholds = enhanced_calibrate_ood(model, tokenizer, val_dataloader, MODEL_SAVE_PATH, percentile=percentile)\n",
        "\n",
        "    # Evaluate model and generate visualizations\n",
        "    report, cm = evaluate_model_enhanced(model, tokenizer, val_texts, val_labels, intent_classes, MODEL_SAVE_PATH)\n",
        "\n",
        "    return thresholds\n",
        "\n",
        "\n",
        "def save_model_artifacts(model, tokenizer, intent_classes, label_encoder, history):\n",
        "    \"\"\"Save model artifacts and visualizations\"\"\"\n",
        "    # Generate enhanced visualizations for the final model\n",
        "    enhanced_plot_training_results(history, MODEL_SAVE_PATH, class_names=intent_classes)\n",
        "    save_enhanced_history(history, MODEL_SAVE_PATH)\n",
        "\n",
        "    # Save intent classes & label encoder\n",
        "    with open(f\"{MODEL_SAVE_PATH}/intent_classes.pkl\", \"wb\") as f:\n",
        "        pickle.dump(intent_classes, f)\n",
        "\n",
        "    with open(f\"{MODEL_SAVE_PATH}/label_encoder.pkl\", \"wb\") as f:\n",
        "        pickle.dump(label_encoder, f)\n",
        "\n",
        "    print(f\"\\n✅ Model telah berhasil dilatih dan disimpan di {MODEL_SAVE_PATH}\")\n",
        "\n",
        "def print_summary(num_labels, intent_classes, thresholds):\n",
        "    \"\"\"Print summary information about the trained model\"\"\"\n",
        "    print(f\"Jumlah intent: {num_labels}\")\n",
        "    print(f\"Intent yang didukung: {', '.join(intent_classes)}\")\n",
        "    print(f\"OOD detection thresholds: Energy={thresholds['energy_threshold']:.4f}, MSP={thresholds['msp_threshold']:.4f}\")\n",
        "    print(f\"Visualisasi training telah disimpan di {MODEL_SAVE_PATH}\")\n",
        "    print(f\"- Interactive plots dapat dibuka pada file HTML di folder tersebut\")\n",
        "    print(f\"- Static plots tersedia dalam format PNG\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dq-WV98s0zgH",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Run Prediksi\n",
        "def run_prediction_demo_enhanced(model=None, tokenizer=None, intent_classes=None, label_encoder=None, model_path=None, method='combined', test_texts=None):\n",
        "    \"\"\"Jalankan demo prediksi intent dengan model yang telah dilatih dan enhanced OOD detection\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : Model object, optional\n",
        "        Model yang sudah dilatih\n",
        "    tokenizer : Tokenizer object, optional\n",
        "        Tokenizer yang sesuai dengan model\n",
        "    intent_classes : list, optional\n",
        "        Daftar kelas intent\n",
        "    label_encoder : LabelEncoder, optional\n",
        "        Label encoder yang digunakan saat training\n",
        "    model_path : str, optional\n",
        "        Path ke model tersimpan (digunakan jika model=None)\n",
        "    method : str, optional\n",
        "        Metode OOD detection ('msp', 'energy', 'combined')\n",
        "    test_texts : list, optional\n",
        "        Daftar teks untuk diprediksi secara batch. Setelah batch, akan lanjut ke mode interaktif.\n",
        "    \"\"\"\n",
        "\n",
        "    if model_path is None:\n",
        "        model_path = MODEL_SAVE_PATH\n",
        "\n",
        "    # Jika model tidak diberikan, muat dari path penyimpanan\n",
        "    if model is None or tokenizer is None or intent_classes is None:\n",
        "        if not os.path.exists(model_path):\n",
        "            print(f\"Error: Model tidak ditemukan di {model_path}\")\n",
        "            print(\"Jalankan run_full_pipeline() terlebih dahulu untuk melatih model\")\n",
        "            return\n",
        "\n",
        "        # Muat model dan tokenizer\n",
        "        print(f\"Memuat model dari {model_path}...\")\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "        # Muat intent classes\n",
        "        import pickle\n",
        "        with open(f\"{model_path}/intent_classes.pkl\", \"rb\") as f:\n",
        "            intent_classes = pickle.load(f)\n",
        "            print(f\"Intent yang didukung: {', '.join(intent_classes)}\")\n",
        "\n",
        "    # Load OOD thresholds\n",
        "    thresholds = load_ood_thresholds(model_path)\n",
        "    energy_threshold = thresholds[\"energy_threshold\"]\n",
        "    msp_threshold = thresholds.get(\"msp_threshold\")\n",
        "\n",
        "    if msp_threshold is not None:\n",
        "        print(f\"OOD thresholds loaded: Energy={energy_threshold:.4f}, MSP={msp_threshold:.4f}\")\n",
        "    else:\n",
        "        print(f\"OOD thresholds loaded: Energy={energy_threshold:.4f}, MSP=None\")\n",
        "\n",
        "    print(f\"Menggunakan metode deteksi OOD: {method}\")\n",
        "\n",
        "    print(\"\\nDemo Prediksi Intent dengan Enhanced OOD Detection:\")\n",
        "    print(\"====================================================\")\n",
        "\n",
        "    # Helper function untuk memprediksi dan menampilkan hasil\n",
        "    def predict_and_display(text):\n",
        "        result = predict_intent_with_enhanced_ood(\n",
        "            text,\n",
        "            model,\n",
        "            tokenizer,\n",
        "            intent_classes,\n",
        "            energy_threshold,\n",
        "            msp_threshold,\n",
        "            method=method\n",
        "        )\n",
        "\n",
        "        if result[\"is_ood\"]:\n",
        "            print(f\"⚠️ Intent terdeteksi: unknown\")\n",
        "            print(f\"   Energy score: {result['energy_score']:.4f} (threshold: {energy_threshold:.4f})\")\n",
        "            if msp_threshold:\n",
        "                print(f\"   Confidence score: {result['confidence']:.4f} (threshold: {msp_threshold:.4f})\")\n",
        "        else:\n",
        "            print(f\"✓ Intent terdeteksi: {result['intent']} (confidence: {result['confidence']:.4f})\")\n",
        "\n",
        "        print(\"\\nTop 3 intent:\")\n",
        "        for i, (intent_name, score) in enumerate(result[\"top_intents\"]):\n",
        "            print(f\"  {i+1}. {intent_name}: {score:.4f}\")\n",
        "\n",
        "        print(\"\\nDetail OOD detection:\")\n",
        "        print(f\"  Energy-based: {'OOD' if result['is_ood_energy'] else 'In-Distribution'} ({result['energy_score']:.4f})\")\n",
        "        if msp_threshold:\n",
        "            print(f\"  MSP-based: {'OOD' if result['is_ood_msp'] else 'In-Distribution'} ({result['confidence']:.4f})\")\n",
        "        print(f\"  Final decision: {'OOD' if result['is_ood'] else 'In-Distribution'}\")\n",
        "\n",
        "    # Jika test_texts diberikan, lakukan prediksi batch\n",
        "    if test_texts is not None and isinstance(test_texts, list) and len(test_texts) > 0:\n",
        "        print(f\"\\nMemprediksi {len(test_texts)} contoh teks:\")\n",
        "        print(\"----------------------------\")\n",
        "\n",
        "        for i, text in enumerate(test_texts):\n",
        "            print(f\"\\nContoh #{i+1}: \\\"{text}\\\"\")\n",
        "            predict_and_display(text)\n",
        "\n",
        "        print(\"\\n----------------------------\")\n",
        "        print(\"Selesai memprediksi contoh teks. Beralih ke mode interaktif.\")\n",
        "\n",
        "    # Mode interaktif\n",
        "    print(\"\\nMode Interaktif - Masukkan teks untuk prediksi intent\")\n",
        "    print(\"Ketik 'exit' untuk keluar\")\n",
        "    print(\"----------------------------\")\n",
        "\n",
        "    # Prediksi input pengguna\n",
        "    while True:\n",
        "        user_input = input(\"\\nMasukkan teks: \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        predict_and_display(user_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpUFrU9D-qPt"
      },
      "outputs": [],
      "source": [
        "# Cell 13: Jalankan pipeline (uncomment untuk menjalankan)\n",
        "\n",
        "model, tokenizer, intent_classes, label_encoder = run_full_pipeline_enhanced(\n",
        "    use_drive=True,\n",
        "    percentile=90,\n",
        "    ood_method='combined',\n",
        "    split_dataset=\"yes\",\n",
        "    val_split=0.20,\n",
        "    batch_size=32,\n",
        "    epochs=12,\n",
        "    learning_rate=2.5e-5,\n",
        "    weight_decay=0.01,\n",
        "    patience=3,\n",
        "    train_csv_path=\"train.csv\",\n",
        "    val_csv_path=\"val.csv\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Test Set\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "# ==== Step 1: Load test data ====\n",
        "test_df = pd.read_csv('tests.csv')\n",
        "test_texts = test_df['text'].tolist()\n",
        "y_true = test_df['intent'].tolist()  # Assuming the true labels column is named 'intent'\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_SAVE_PATH)\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_SAVE_PATH)\n",
        "# ==== Step 2: Load label encoder and intent_classes ====\n",
        "with open(f\"{MODEL_SAVE_PATH}/label_encoder.pkl\", \"rb\") as f:\n",
        "    label_encoder = pickle.load(f)\n",
        "with open(f\"{MODEL_SAVE_PATH}/intent_classes.pkl\", \"rb\") as f: # Load intent_classes\n",
        "    intent_classes = pickle.load(f)\n",
        "\n",
        "# Encode the ground truth labels into numerical format\n",
        "y_true_encoded = label_encoder.transform(y_true)\n",
        "\n",
        "# ==== Step 3: Define prediction function ====\n",
        "def get_model_predictions(texts):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    for text in texts:\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "        with torch.no_grad():  # Important to avoid unnecessary gradient calculations\n",
        "            outputs = model(**inputs)\n",
        "            probs = outputs.logits.softmax(dim=1)\n",
        "            pred = probs.argmax(dim=1).item()\n",
        "            predictions.append(pred)\n",
        "    return predictions\n",
        "\n",
        "# ==== Step 4: Get predictions ====\n",
        "y_pred = get_model_predictions(test_texts)\n",
        "y_pred_labels = [intent_classes[i] for i in y_pred]\n",
        "\n",
        "# ==== Step 5: Print Classification Report ====\n",
        "print(\"=== Classification Report ===\")\n",
        "print(classification_report(y_true, y_pred_labels, target_names=intent_classes))\n",
        "\n",
        "# ==== Step 6: Confusion Matrix (encoded labels for alignment) ====\n",
        "cm = confusion_matrix(y_true_encoded, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=intent_classes)\n",
        "\n",
        "# ==== Step 7: Plot Confusion Matrix ====\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "disp.plot(xticks_rotation='vertical', ax=ax, cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm_normalized, annot=True, cmap=\"Blues\", xticklabels=intent_classes, yticklabels=intent_classes, fmt=\".2f\")\n",
        "plt.title(\"Normalized Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Create a list to store misclassified examples\n",
        "misclassified_examples = []\n",
        "\n",
        "for i, (true, pred_idx, text) in enumerate(zip(y_true_encoded, y_pred, test_texts)):\n",
        "    if true != pred_idx:\n",
        "        misclassified_examples.append({\n",
        "            'text': text,\n",
        "            'true_label': intent_classes[true],\n",
        "            'predicted_label': intent_classes[pred_idx]\n",
        "        })\n",
        "\n",
        "# Save misclassified examples to CSV\n",
        "if misclassified_examples:\n",
        "    misclassified_df = pd.DataFrame(misclassified_examples)\n",
        "    misclassified_csv_path = f\"{MODEL_SAVE_PATH}/misclassified_examples.csv\"\n",
        "    misclassified_df.to_csv(misclassified_csv_path, index=False)\n",
        "    print(f\"Saved {len(misclassified_examples)} misclassified examples to {misclassified_csv_path}\")\n",
        "else:\n",
        "    print(\"No misclassified examples found.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ZLOKcYqVT-yl",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Enhanced Test Set with Confidence Thresholding\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# ==== Step 1: Load test data ====\n",
        "test_df = pd.read_csv('tests.csv')\n",
        "test_texts = test_df['text'].tolist()\n",
        "y_true = test_df['intent'].tolist()  # Assuming the true labels column is named 'intent'\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_SAVE_PATH)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_SAVE_PATH)\n",
        "\n",
        "# ==== Step 2: Load label encoder and intent_classes ====\n",
        "with open(f\"{MODEL_SAVE_PATH}/label_encoder.pkl\", \"rb\") as f:\n",
        "    label_encoder = pickle.load(f)\n",
        "with open(f\"{MODEL_SAVE_PATH}/intent_classes.pkl\", \"rb\") as f:\n",
        "    intent_classes = pickle.load(f)\n",
        "\n",
        "# Encode the ground truth labels into numerical format\n",
        "y_true_encoded = label_encoder.transform(y_true)\n",
        "\n",
        "# ==== Step 3: Define enhanced prediction function with confidence scores ====\n",
        "def get_model_predictions_with_confidence(texts, confidence_threshold=0.0):\n",
        "    \"\"\"\n",
        "    Get model predictions with confidence scores.\n",
        "    If max confidence is below threshold, classify as \"unknown\"\n",
        "\n",
        "    Args:\n",
        "        texts: List of text inputs\n",
        "        confidence_threshold: Minimum confidence to make a prediction (0.0-1.0)\n",
        "\n",
        "    Returns:\n",
        "        predictions: List of predicted class indices\n",
        "        confidences: List of confidence scores\n",
        "        pred_labels: List of predicted class labels\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    confidences = []\n",
        "    pred_labels = []\n",
        "\n",
        "    # Add \"unknown\" class if it doesn't exist\n",
        "    unknown_idx = len(intent_classes)\n",
        "    all_classes = intent_classes.tolist()\n",
        "    if \"unknown\" not in all_classes:\n",
        "        all_classes.append(\"unknown\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    for text in texts:\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            probs = outputs.logits.softmax(dim=1)\n",
        "            confidence, pred = torch.max(probs, dim=1)\n",
        "            confidence = confidence.item()\n",
        "            pred = pred.item()\n",
        "\n",
        "            # If confidence is below threshold, classify as unknown\n",
        "            if confidence < confidence_threshold:\n",
        "                predictions.append(unknown_idx)  # Unknown class index\n",
        "                pred_labels.append(\"unknown\")\n",
        "            else:\n",
        "                predictions.append(pred)\n",
        "                pred_labels.append(intent_classes[pred])\n",
        "\n",
        "            confidences.append(confidence)\n",
        "\n",
        "    return predictions, confidences, pred_labels\n",
        "\n",
        "# ==== Step 4: Function to evaluate at different confidence thresholds ====\n",
        "def evaluate_with_thresholds(thresholds=[0.0, 0.3, 0.5, 0.7, 0.9]):\n",
        "    \"\"\"Evaluate model with different confidence thresholds\"\"\"\n",
        "    results = {}\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        print(f\"\\n=== Evaluating with confidence threshold: {threshold} ===\")\n",
        "        y_pred, confidences, y_pred_labels = get_model_predictions_with_confidence(\n",
        "            test_texts, confidence_threshold=threshold\n",
        "        )\n",
        "\n",
        "        # Create extended true labels (for comparison with unknown predictions)\n",
        "        extended_true_labels = y_true.copy()\n",
        "\n",
        "        # Create classification report\n",
        "        print(\"Classification Report:\")\n",
        "        # We need to handle the case where \"unknown\" might not be in predictions\n",
        "        unique_labels = sorted(set(y_pred_labels + [\"unknown\"]))\n",
        "        report = classification_report(extended_true_labels, y_pred_labels, labels=unique_labels)\n",
        "        print(report)\n",
        "\n",
        "        # Store results\n",
        "        results[threshold] = {\n",
        "            'y_pred': y_pred,\n",
        "            'confidences': confidences,\n",
        "            'y_pred_labels': y_pred_labels,\n",
        "            'report': report\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "# ==== Step 5: Run standard evaluation (without threshold) ====\n",
        "y_pred_standard, confidences_standard, y_pred_labels_standard = get_model_predictions_with_confidence(\n",
        "    test_texts, confidence_threshold=0.0\n",
        ")\n",
        "\n",
        "print(\"=== Standard Classification Report (No Threshold) ===\")\n",
        "print(classification_report(y_true, y_pred_labels_standard, target_names=intent_classes))\n",
        "\n",
        "# ==== Step 6: Create and plot standard confusion matrix ====\n",
        "cm = confusion_matrix(y_true_encoded, y_pred_standard)\n",
        "plt.figure(figsize=(12, 12))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=intent_classes)\n",
        "disp.plot(xticks_rotation='vertical', cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ==== Step 7: Plot normalized confusion matrix ====\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm_normalized, annot=True, cmap=\"Blues\", xticklabels=intent_classes, yticklabels=intent_classes, fmt=\".2f\")\n",
        "plt.title(\"Normalized Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ==== Step 8: Plot confidence distribution ====\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(confidences_standard, bins=20, alpha=0.7)\n",
        "plt.axvline(x=0.5, color='r', linestyle='--', label='Example threshold (0.5)')\n",
        "plt.title(\"Confidence Score Distribution\")\n",
        "plt.xlabel(\"Confidence Score\")\n",
        "plt.ylabel(\"Number of Predictions\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ==== Step 9: Plot confidence by class ====\n",
        "confidence_by_class = {}\n",
        "for true_label, pred_label, conf in zip(y_true, y_pred_labels_standard, confidences_standard):\n",
        "    if true_label not in confidence_by_class:\n",
        "        confidence_by_class[true_label] = []\n",
        "    confidence_by_class[true_label].append(conf)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "boxplot_data = [confidence_by_class[label] for label in intent_classes if label in confidence_by_class]\n",
        "plt.boxplot(boxplot_data, labels=[label for label in intent_classes if label in confidence_by_class])\n",
        "plt.title(\"Confidence Distribution by True Class\")\n",
        "plt.xlabel(\"True Class\")\n",
        "plt.ylabel(\"Confidence Score\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.axhline(y=0.5, color='r', linestyle='--', label='Example threshold (0.5)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ==== Step 10: Save misclassified examples ====\n",
        "misclassified_examples = []\n",
        "for i, (true, pred_idx, text, confidence) in enumerate(zip(y_true, y_pred_labels_standard, test_texts, confidences_standard)):\n",
        "    if true != pred_idx:\n",
        "        misclassified_examples.append({\n",
        "            'text': text,\n",
        "            'true_label': true,\n",
        "            'predicted_label': pred_idx,\n",
        "            'confidence': confidence\n",
        "        })\n",
        "\n",
        "# Save misclassified examples to CSV\n",
        "if misclassified_examples:\n",
        "    misclassified_df = pd.DataFrame(misclassified_examples)\n",
        "    misclassified_df = misclassified_df.sort_values('confidence', ascending=True)  # Sort by confidence\n",
        "    misclassified_csv_path = f\"{MODEL_SAVE_PATH}/misclassified_examples.csv\"\n",
        "    misclassified_df.to_csv(misclassified_csv_path, index=False)\n",
        "    print(f\"Saved {len(misclassified_examples)} misclassified examples to {misclassified_csv_path}\")\n",
        "\n",
        "    # Display some examples with lowest confidence\n",
        "    print(\"\\n=== Examples with Lowest Confidence (Most Likely Errors) ===\")\n",
        "    print(misclassified_df.head(10))\n",
        "else:\n",
        "    print(\"No misclassified examples found.\")\n",
        "\n",
        "# ==== Step 11: Try different confidence thresholds ====\n",
        "print(\"\\n=== Evaluating Model with Different Confidence Thresholds ===\")\n",
        "threshold_results = evaluate_with_thresholds([0.0, 0.3, 0.5, 0.7, 0.9])\n",
        "\n",
        "# ==== Step 12: Plot impact of threshold on accuracy and unknown rate ====\n",
        "thresholds = list(threshold_results.keys())\n",
        "unknown_rates = []\n",
        "accuracies = []\n",
        "\n",
        "for threshold in thresholds:\n",
        "    # Calculate unknown rate\n",
        "    unknown_count = y_pred_labels_standard.count(\"unknown\") if \"unknown\" in y_pred_labels_standard else 0\n",
        "    unknown_rates.append(unknown_count / len(test_texts))\n",
        "\n",
        "    # Extract accuracy from classification report\n",
        "    report_lines = threshold_results[threshold]['report'].split('\\n')\n",
        "    accuracy_line = [line for line in report_lines if 'accuracy' in line][0]\n",
        "    accuracy = float(accuracy_line.split()[-1])\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# Plot threshold impact\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(thresholds, accuracies, 'b-', marker='o', label='Accuracy')\n",
        "plt.plot(thresholds, unknown_rates, 'r-', marker='x', label='Unknown Rate')\n",
        "plt.title(\"Impact of Confidence Threshold\")\n",
        "plt.xlabel(\"Confidence Threshold\")\n",
        "plt.ylabel(\"Rate\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ==== Step 13: Interactive threshold selection (optional) ====\n",
        "def interactive_threshold_selection():\n",
        "    \"\"\"Allow user to input a custom threshold and see results\"\"\"\n",
        "    while True:\n",
        "        try:\n",
        "            custom_threshold = float(input(\"\\nEnter a custom confidence threshold (0.0-1.0) or -1 to exit: \"))\n",
        "            if custom_threshold == -1:\n",
        "                break\n",
        "            if 0.0 <= custom_threshold <= 1.0:\n",
        "                print(f\"\\n=== Evaluating with custom threshold: {custom_threshold} ===\")\n",
        "                _, _, y_pred_labels_custom = get_model_predictions_with_confidence(\n",
        "                    test_texts, confidence_threshold=custom_threshold\n",
        "                )\n",
        "\n",
        "                # Create classification report\n",
        "                print(\"Classification Report:\")\n",
        "                unique_labels = sorted(set(y_pred_labels_custom + [\"unknown\"]))\n",
        "                report = classification_report(y_true, y_pred_labels_custom, labels=unique_labels)\n",
        "                print(report)\n",
        "            else:\n",
        "                print(\"Threshold must be between 0.0 and 1.0.\")\n",
        "        except ValueError:\n",
        "            print(\"Please enter a valid number.\")\n",
        "\n",
        "# Uncomment to use interactive mode\n",
        "# interactive_threshold_selection()\n",
        "\n",
        "print(\"\\nAnalysis complete! You can now determine the optimal confidence threshold for your application.\")"
      ],
      "metadata": {
        "id": "grI9z6niMVSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Teks\n",
        "test_sentences = [\n",
        "    # GREETING\n",
        "    \"Halo, selamat pagi!\",\n",
        "    \"Apa kabar?\",\n",
        "    \"Hai, bot!\",\n",
        "    \"Permisi, boleh bertanya?\",\n",
        "    \"Yo, ada orang di sana?\",\n",
        "\n",
        "    # GOODBYE\n",
        "    \"Terima kasih, sampai jumpa.\",\n",
        "    \"Ok, saya pergi dulu.\",\n",
        "    \"Sampai nanti!\",\n",
        "    \"Dadah, bot.\",\n",
        "    \"Aku akan kembali nanti.\",\n",
        "\n",
        "    # CONFIRM\n",
        "    \"Iya, benar.\",\n",
        "    \"Betul sekali.\",\n",
        "    \"Ya, saya setuju.\",\n",
        "    \"Tentu saja.\",\n",
        "    \"Itu yang saya maksud.\",\n",
        "\n",
        "    # DENIED\n",
        "    \"Tidak, bukan itu.\",\n",
        "    \"Salah.\",\n",
        "    \"Bukan, maksud saya yang lain.\",\n",
        "    \"Enggak.\",\n",
        "    \"Saya tidak yakin dengan itu.\",\n",
        "\n",
        "    # AMBIGUOUS (bisa mengecoh)\n",
        "    \"Saya rasa tidak perlu, tapi ya juga boleh.\",\n",
        "    \"Mungkin... tapi entahlah.\",\n",
        "    \"Terserah kamu aja deh.\",\n",
        "    \"Boleh iya, boleh juga tidak.\",\n",
        "    \"Ya tapi tidak juga sih...\",\n",
        "    \"p\",\n",
        "    \"test\",\n",
        "    \"y\",\n",
        "    \"g\",\n",
        "    \"N\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "P9gOO73x_oxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oS_b2pfqxA8Z",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title self prediksi inference (bugged)\n",
        "# If you want to load an existing model and run predictions\n",
        "run_prediction_demo_enhanced( #model, tokenizer, intent_classes, label_encoder, method=ood_method\n",
        "    model_path=MODEL_SAVE_PATH,  # Your MODEL_SAVE_PATH\n",
        "    method='combined',  # Which OOD detection method to use\n",
        "    test_texts=test_sentences\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUYgABG0S6lf",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title Dataset Augmentation for Indonesian NLP - Improved Version\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import nltk\n",
        "import torch\n",
        "import time\n",
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import wordnet\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from deep_translator import GoogleTranslator\n",
        "from collections import defaultdict, Counter\n",
        "from tqdm import tqdm\n",
        "import Levenshtein as lev  # For better text difference calculation\n",
        "\n",
        "# Download WordNet data (if not already downloaded)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "# =========[ KONFIGURASI ]=========\n",
        "INPUT_FILE = \"test.csv\" # @param {\"type\":\"string\"}\n",
        "DATA_TYPE = \"test\" # @param [\"train\", \"val\", \"test\"]\n",
        "TARGET_SAMPLES_PER_CLASS = 750    # @param {type:\"integer\"} Target jumlah sampel per kelas\n",
        "NOISE_INTENSITY = 0.7             # @param {type:\"number\"} Control how aggressive augmentations are (0.1-1.0)\n",
        "USE_PARAPHRASE_MODEL = True       # @param {type:\"boolean\"} Aktifkan atau matikan paraphrase\n",
        "USE_BACK_TRANSLATION = True       # @param {type:\"boolean\"} Aktifkan atau matikan back-translation\n",
        "MIN_AUGMENTATIONS_PER_SAMPLE = 2  # @param {type:\"integer\"} Minimum augmentasi per sampel asli\n",
        "MAX_AUGMENTATIONS_PER_SAMPLE = 10  # @param {type:\"integer\"} Maximum augmentasi per sampel asli (reduced from 10)\n",
        "BATCH_SIZE = 16                   # @param {type:\"integer\"} Untuk batch processing\n",
        "\n",
        "def load_from_json(filename):\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def load_all_dictionaries():\n",
        "    all_dicts = load_from_json('/content/ChatbotPerpusBipa/kamus.json')\n",
        "\n",
        "    id_synonyms = all_dicts['id_synonyms']\n",
        "    common_slang = all_dicts['common_slang']\n",
        "    intent_slang = all_dicts['intent_slang']\n",
        "    phonetic_dict = all_dicts['phonetic_dict']\n",
        "    protected_intent_words = all_dicts['protected_intent_words']\n",
        "\n",
        "    return id_synonyms, common_slang, intent_slang, phonetic_dict, protected_intent_words\n",
        "\n",
        "# Option 2:\n",
        "id_synonyms, common_slang, intent_slang, phonetic_dict, protected_intent_words = load_all_dictionaries()\n",
        "\n",
        "# =========[ READ & VALIDATE FILE ]=========\n",
        "def read_dataset(file_path):\n",
        "    \"\"\"Membaca dataset dari file CSV atau XLSX\"\"\"\n",
        "    print(f\"Loading dataset: {file_path}\")\n",
        "\n",
        "    file_ext = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    if file_ext == '.xlsx':\n",
        "        print(f\"Detected Excel file: {file_path}\")\n",
        "        df = pd.read_excel(file_path)\n",
        "        # Konversi ke CSV untuk kompatibilitas\n",
        "        csv_path = file_path.replace('.xlsx', '.csv')\n",
        "        df.to_csv(csv_path, index=False)\n",
        "        print(f\"Converted Excel file to CSV: {csv_path}\")\n",
        "    elif file_ext == '.csv':\n",
        "        print(f\"Detected CSV file: {file_path}\")\n",
        "        df = pd.read_csv(file_path)\n",
        "    else:\n",
        "        raise ValueError(f\"Format file tidak didukung: {file_ext}. Harap gunakan file CSV atau XLSX.\")\n",
        "\n",
        "    df = df.dropna()\n",
        "    print(f\"Dataset dimuat dengan {len(df)} baris\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# =========[ INITIALIZE PARAPHRASE MODEL IF NEEDED ]=========\n",
        "def initialize_paraphrase_model():\n",
        "    \"\"\"Initialize paraphrase model if enabled\"\"\"\n",
        "    if USE_PARAPHRASE_MODEL:\n",
        "        print(\"Loading paraphrase model...\")\n",
        "        start_time = time.time()\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"Wikidepia/IndoT5-base-paraphrase\")\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(\"Wikidepia/IndoT5-base-paraphrase\")\n",
        "\n",
        "        # Move model to GPU if available\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        model = model.to(device)\n",
        "        print(f\"Model loaded in {time.time() - start_time:.2f} seconds. Using device: {device}\")\n",
        "        return model, tokenizer\n",
        "    return None, None\n",
        "\n",
        "# =========[ AUGMENTATION METHODS ]=========\n",
        "def get_better_synonym(word):\n",
        "    \"\"\"Get synonym from custom dictionary or return the original word\"\"\"\n",
        "    word_lower = word.lower()\n",
        "    if word_lower in id_synonyms:\n",
        "        synonyms = id_synonyms[word_lower]\n",
        "        return random.choice(synonyms)\n",
        "    return word\n",
        "\n",
        "def replace_with_synonym(sentence):\n",
        "    \"\"\"Replace words with synonyms while preserving capitalization\"\"\"\n",
        "    words = sentence.split()\n",
        "    new_words = []\n",
        "\n",
        "    # Limit the number of words to replace to avoid excessive changes\n",
        "    num_to_replace = min(2, max(1, int(len(words) * 0.2)))\n",
        "    indices_to_replace = random.sample(range(len(words)), k=min(num_to_replace, len(words)))\n",
        "\n",
        "    for i, word in enumerate(words):\n",
        "        if i in indices_to_replace:\n",
        "            synonym = get_better_synonym(word)\n",
        "            # Preserve capitalization\n",
        "            if word and word[0].isupper() and synonym:\n",
        "                synonym = synonym[0].upper() + synonym[1:]\n",
        "            new_words.append(synonym)\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "def back_translate(sentence):\n",
        "    \"\"\"Translate to English and back to Indonesian with safety checks\"\"\"\n",
        "    if not USE_BACK_TRANSLATION:\n",
        "        return sentence\n",
        "\n",
        "    # Skip very short sentences\n",
        "    if len(sentence.split()) < 3:\n",
        "        return sentence\n",
        "\n",
        "    try:\n",
        "        # First to English\n",
        "        translated = GoogleTranslator(source='id', target='en').translate(sentence)\n",
        "        # Then back to Indonesian\n",
        "        back_translated = GoogleTranslator(source='en', target='id').translate(translated)\n",
        "\n",
        "        # Safety checks\n",
        "        if back_translated and len(back_translated.split()) >= len(sentence.split()) * 0.7:\n",
        "            # Calculate how different the result is\n",
        "            similarity = 1 - (lev.distance(sentence.lower(), back_translated.lower()) / max(len(sentence), len(back_translated)))\n",
        "            # If too different or too similar, return original\n",
        "            if similarity < 0.3 or similarity > 0.9:\n",
        "                return sentence\n",
        "            return back_translated\n",
        "        return sentence\n",
        "    except Exception:\n",
        "        return sentence\n",
        "\n",
        "def add_typo(sentence):\n",
        "    \"\"\"Add a single typo by replacing a character, with reduced probability\"\"\"\n",
        "    # Skip for very short sentences or with low global noise setting\n",
        "    if len(sentence) < 10 or random.random() > NOISE_INTENSITY:\n",
        "        return sentence\n",
        "\n",
        "    chars = list(sentence)\n",
        "    if len(chars) > 3:\n",
        "        # Try to find a good character to modify (not first or last character)\n",
        "        candidates = [i for i in range(1, len(chars)-1) if chars[i].isalpha()]\n",
        "        if candidates:\n",
        "            idx = random.choice(candidates)\n",
        "            # Get neighboring letters on keyboard for more realistic typos\n",
        "            keyboard_neighbors = {\n",
        "                'q': 'wsa', 'w': 'qeasd', 'e': 'wrsdf', 'r': 'etdfg',\n",
        "                't': 'ryfgh', 'y': 'tughj', 'u': 'yihjk', 'i': 'uojkl',\n",
        "                'o': 'ipkl', 'p': 'ol',\n",
        "                'a': 'qwszx', 's': 'awedcxz', 'd': 'serfcvx', 'f': 'drtgvbc',\n",
        "                'g': 'ftyhvbn', 'h': 'gyujbnm', 'j': 'huiknm', 'k': 'jiolm',\n",
        "                'l': 'kop',\n",
        "                'z': 'asx', 'x': 'zsdc', 'c': 'xdfv', 'v': 'cfgb',\n",
        "                'b': 'vghn', 'n': 'bhjm', 'm': 'njk'\n",
        "            }\n",
        "            char = chars[idx].lower()\n",
        "            if char in keyboard_neighbors:\n",
        "                chars[idx] = random.choice(keyboard_neighbors[char])\n",
        "\n",
        "    return \"\".join(chars)\n",
        "\n",
        "def random_deletion(sentence, p=0.1):  # Reduced probability from 0.2\n",
        "    \"\"\"Delete words with probability p\"\"\"\n",
        "    words = sentence.split()\n",
        "\n",
        "    # Don't delete from short sentences\n",
        "    if len(words) <= 4:\n",
        "        return sentence\n",
        "\n",
        "    # Don't delete too many words\n",
        "    max_deletions = max(1, int(len(words) * 0.1))\n",
        "    deletion_count = 0\n",
        "\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if random.uniform(0, 1) > p or deletion_count >= max_deletions:\n",
        "            new_words.append(word)\n",
        "        else:\n",
        "            deletion_count += 1\n",
        "\n",
        "    # Make sure we don't delete everything\n",
        "    if not new_words:\n",
        "        return sentence\n",
        "\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "def random_swap(sentence, n=1):\n",
        "    \"\"\"Swap n pairs of words\"\"\"\n",
        "    words = sentence.split()\n",
        "    if len(words) < 4:  # Don't swap in very short sentences\n",
        "        return sentence\n",
        "\n",
        "    # Limit swaps to just 1 for shorter sentences\n",
        "    if len(words) < 8:\n",
        "        n = 1\n",
        "\n",
        "    for _ in range(min(n, len(words)//3)):  # Reduced number of swaps\n",
        "        idx1, idx2 = random.sample(range(len(words)), 2)\n",
        "        words[idx1], words[idx2] = words[idx2], words[idx1]\n",
        "\n",
        "    return \" \".join(words)\n",
        "\n",
        "def phonetic_augmentation(sentence):\n",
        "    \"\"\"Apply phonetic substitutions common in Indonesian chat\"\"\"\n",
        "    words = sentence.split()\n",
        "    new_words = []\n",
        "\n",
        "    # Limit substitutions to maintain readability\n",
        "    max_substitutions = min(2, max(1, int(len(words) * 0.2)))\n",
        "    substitution_count = 0\n",
        "\n",
        "    for word in words:\n",
        "        word_lower = word.lower()\n",
        "        if word_lower in phonetic_dict and substitution_count < max_substitutions:\n",
        "            new_word = random.choice(phonetic_dict[word_lower])\n",
        "            # Preserve capitalization\n",
        "            if word and word[0].isupper():\n",
        "                new_word = new_word[0].upper() + new_word[1:]\n",
        "            new_words.append(new_word)\n",
        "            substitution_count += 1\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "def apply_slang_typo(text, intent, intensity=1.0):\n",
        "    \"\"\"Apply slang replacements with controllable intensity\"\"\"\n",
        "    # Combine common slang with intent-specific slang\n",
        "    slang_dict = common_slang.copy()\n",
        "    if intent in intent_slang:\n",
        "        slang_dict.update(intent_slang[intent])\n",
        "\n",
        "    # Create regex patterns from the slang dictionary\n",
        "    patterns = {\n",
        "        re.compile(rf'\\b{k}\\b', re.IGNORECASE): v for k, v in slang_dict.items()\n",
        "    }\n",
        "\n",
        "    # Apply only a few patterns based on intensity and text length\n",
        "    max_replacements = min(2, max(1, int(len(text.split()) * 0.2)))\n",
        "    patterns_to_use = random.sample(\n",
        "        list(patterns.items()),\n",
        "        k=min(max_replacements, int(len(patterns) * min(0.3, intensity * 0.5)))\n",
        "    )\n",
        "\n",
        "    for pattern, replacement in patterns_to_use:\n",
        "        text = pattern.sub(replacement, text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def character_noise(text, intensity=1.0):\n",
        "    \"\"\"Add typos like character swaps, insertions, deletions with reduced intensity\"\"\"\n",
        "    # Skip for very short texts\n",
        "    if len(text) < 10 or random.random() > NOISE_INTENSITY:\n",
        "        return text\n",
        "\n",
        "    chars = list(text)\n",
        "    # Significantly reduce swap probability\n",
        "    swap_prob = min(0.03, intensity * 0.01)  # Lower from 0.1 to 0.03\n",
        "\n",
        "    # Limit to just one or two swaps per sentence\n",
        "    max_swaps = min(1, int(len(chars) * 0.05))\n",
        "    swap_count = 0\n",
        "\n",
        "    for i in range(len(chars)-1):\n",
        "        if random.random() < swap_prob and swap_count < max_swaps:\n",
        "            # Don't swap punctuation or spaces\n",
        "            if chars[i].isalpha() and chars[i+1].isalpha():\n",
        "                chars[i], chars[i+1] = chars[i+1], chars[i]\n",
        "                swap_count += 1\n",
        "\n",
        "    return ''.join(chars)\n",
        "\n",
        "def add_common_phrase(sentence):\n",
        "    \"\"\"Add a common Indonesian chat phrase\"\"\"\n",
        "    # Skip for longer sentences\n",
        "    if len(sentence.split()) > 8:\n",
        "        return sentence\n",
        "\n",
        "    common_phrases = [\"sih\", \"ya\", \"dong\", \"cuy\", \"bro\", \"lah\", \"deh\"]\n",
        "    return sentence + \" \" + random.choice(common_phrases)\n",
        "\n",
        "def short_text_augmentation(text, intent):\n",
        "    \"\"\"Special augmentation for very short texts like greetings and goodbyes\"\"\"\n",
        "    # For very short texts, add filler words or expressions\n",
        "    fillers = {\n",
        "        'greeting': ['', ' ya', ' kak', ' min', ' gan', ' bro', ' sis', ' admin', '!'],\n",
        "        'goodbye': ['', ' ya', ' kak', ' min', ' semuanya', '!'],\n",
        "        'confirm': ['', ' kok', ' dong', ' banget', ' sih', ' tentu', ' lah', '!'],\n",
        "        'denied': ['', ' sih', ' kok', ' ah', ' deh', ' lah', '!'],\n",
        "    }\n",
        "\n",
        "    if intent in fillers and len(text.split()) <= 3:\n",
        "        # Add just one filler\n",
        "        if random.random() < 0.7:  # 70% chance to add filler\n",
        "            text += random.choice(fillers[intent])\n",
        "\n",
        "    return text\n",
        "\n",
        "def validate_augmentation(original, augmented):\n",
        "    \"\"\"Validate if augmentation is reasonable with stricter requirements\"\"\"\n",
        "    # Skip if no change\n",
        "    if augmented.lower() == original.lower():\n",
        "        return False\n",
        "\n",
        "    # Calculate word count difference\n",
        "    orig_words = original.split()\n",
        "    aug_words = augmented.split()\n",
        "\n",
        "    # Check if length is reasonable\n",
        "    if len(aug_words) < len(orig_words) * 0.6 or len(aug_words) > len(orig_words) * 1.4:\n",
        "        return False\n",
        "\n",
        "    # Calculate text similarity using Levenshtein distance\n",
        "    normalized_distance = lev.distance(original.lower(), augmented.lower()) / max(len(original), len(augmented))\n",
        "    # If too similar or too different, reject\n",
        "    if normalized_distance < 0.03 or normalized_distance > 0.5:\n",
        "        return False\n",
        "\n",
        "    # Check for excessive non-standard characters\n",
        "    non_indo_pattern = re.compile(r'[^a-zA-Z0-9\\s.,?!\\'\"-:;()[\\]{}]')\n",
        "    if len(non_indo_pattern.findall(augmented)) > 3:\n",
        "        return False\n",
        "\n",
        "    # Check if individual words have been mangled too much\n",
        "    if len(orig_words) == len(aug_words):\n",
        "        word_changes = 0\n",
        "        for i in range(len(orig_words)):\n",
        "            # Check word edit distance\n",
        "            if len(orig_words[i]) > 3 and lev.distance(orig_words[i], aug_words[i]) > len(orig_words[i]) * 0.5:\n",
        "                word_changes += 1\n",
        "\n",
        "        # Reject if too many words changed significantly\n",
        "        if word_changes / len(orig_words) > 0.4:\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "def batch_paraphrase(model, tokenizer, sentences, batch_size=BATCH_SIZE):\n",
        "    \"\"\"Process paraphrasing in batches\"\"\"\n",
        "    if not sentences or model is None or tokenizer is None:\n",
        "        return []\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "    results = []\n",
        "\n",
        "    for i in range(0, len(sentences), batch_size):\n",
        "        batch = sentences[i:i+batch_size]\n",
        "        inputs = tokenizer([\"paraphrase: \" + text + \" </s>\" for text in batch],\n",
        "                         padding='longest', truncation=True, max_length=128,\n",
        "                         return_tensors=\"pt\").to(device)\n",
        "\n",
        "        with torch.no_grad():  # Disable gradient calculation for inference\n",
        "            outputs = model.generate(\n",
        "                input_ids=inputs[\"input_ids\"],\n",
        "                attention_mask=inputs[\"attention_mask\"],\n",
        "                max_length=128,\n",
        "                do_sample=True,\n",
        "                top_k=200,  # Reduce from 200 to 120 for more conservative output\n",
        "                top_p=0.98,\n",
        "                temperature=NOISE_INTENSITY + 0.3, # Added temperature control\n",
        "                early_stopping=False,\n",
        "                num_return_sequences=min(3, BATCH_SIZE // len(batch))\n",
        "            )\n",
        "\n",
        "        decoded = [tokenizer.decode(outputs[j], skip_special_tokens=True)\n",
        "                  for j in range(len(outputs))]\n",
        "        results.extend(decoded)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Combined augmentation strategies\n",
        "def augment_text(text, intent, intensity=1.0):\n",
        "    \"\"\"Apply multiple augmentation techniques based on class needs\"\"\"\n",
        "    # Scale intensity by global noise setting\n",
        "    intensity = intensity * NOISE_INTENSITY\n",
        "\n",
        "    # Protect intent-critical words\n",
        "    protected = []\n",
        "    if intent in protected_intent_words:\n",
        "        for word in protected_intent_words[intent]:\n",
        "            pattern = re.compile(rf'\\b{word}\\b', re.IGNORECASE)\n",
        "            for match in pattern.finditer(text):\n",
        "                # Replace with a temporary placeholder\n",
        "                placeholder = f\"__PROTECTED_{len(protected)}__\"\n",
        "                text = text[:match.start()] + placeholder + text[match.end():]\n",
        "                protected.append((placeholder, match.group(0)))\n",
        "\n",
        "    # Available methods - reorder by safety\n",
        "    methods = {\n",
        "        'synonym': replace_with_synonym,          # Safe\n",
        "        'back_translate': back_translate if USE_BACK_TRANSLATION else None,  # Generally safe\n",
        "        'slang': lambda t: apply_slang_typo(t, intent, intensity), # Safe if using known slang\n",
        "        'common_phrase': add_common_phrase,       # Safe\n",
        "        'short_text': lambda t: short_text_augmentation(t, intent), # Safe\n",
        "        'swap': random_swap,                      # Moderately safe\n",
        "        'deletion': random_deletion,              # Can be problematic\n",
        "        'phonetic': phonetic_augmentation,        # Can be problematic\n",
        "        'char_noise': lambda t: character_noise(t, intensity * 0.5),  # Reduced intensity\n",
        "        'typo': add_typo                          # Most problematic\n",
        "    }\n",
        "\n",
        "    # Remove None methods\n",
        "    methods = {k: v for k, v in methods.items() if v is not None}\n",
        "\n",
        "    # Choose augmentation methods based on text length and intent\n",
        "    text_length = len(text.split())\n",
        "\n",
        "    if text_length <= 3:  # Very short text\n",
        "        # For short texts, focus on safer methods\n",
        "        method_choices = ['slang', 'short_text', 'synonym', 'common_phrase']\n",
        "        num_methods = min(2, int(intensity * 2))  # Reduce number of transformations\n",
        "    else:  # Longer text\n",
        "        # Weight safer methods higher in the selection\n",
        "        method_choices = ['synonym', 'synonym', 'back_translate', 'slang', 'slang',\n",
        "                          'common_phrase', 'swap', 'deletion', 'phonetic', 'char_noise']\n",
        "        # Apply fewer transformations overall\n",
        "        num_methods = min(2, int(intensity * 1.5))\n",
        "\n",
        "    # Filter out any methods that aren't available (like back_translate if disabled)\n",
        "    method_choices = [m for m in method_choices if m in methods]\n",
        "\n",
        "    # Sample methods\n",
        "    if method_choices:\n",
        "        selected_methods = random.sample(method_choices, k=min(num_methods, len(method_choices)))\n",
        "    else:\n",
        "        selected_methods = []\n",
        "\n",
        "    # Apply selected methods in sequence\n",
        "    result = text\n",
        "    for method_name in selected_methods:\n",
        "        if method_name in methods:\n",
        "            method = methods[method_name]\n",
        "            result = method(result)\n",
        "\n",
        "    # Restore protected words\n",
        "    for placeholder, original in protected:\n",
        "        result = result.replace(placeholder, original)\n",
        "\n",
        "    return result\n",
        "\n",
        "def augment_data(text, intent):\n",
        "    \"\"\"Generate multiple augmentations for a text\"\"\"\n",
        "    methods = [\n",
        "        replace_with_synonym,\n",
        "        back_translate if USE_BACK_TRANSLATION else None,\n",
        "        lambda t: augment_text(t, intent, 1.0)  # Use combined approach\n",
        "    ]\n",
        "\n",
        "    # Remove None methods\n",
        "    methods = [m for m in methods if m is not None]\n",
        "\n",
        "    augmented = set()\n",
        "    for method in methods:\n",
        "        try:\n",
        "            result = method(text)\n",
        "            if validate_augmentation(text, result):\n",
        "                augmented.add(result)\n",
        "        except Exception as e:\n",
        "            print(f\"Error applying {method.__name__}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return list(augmented)\n",
        "\n",
        "def balance_samples(results_by_intent, target_samples_per_class, original_counts):\n",
        "    \"\"\"\n",
        "    Balance samples by intent with preference toward reaching TARGET_SAMPLES_PER_CLASS\n",
        "    \"\"\"\n",
        "    balanced_results = defaultdict(list)\n",
        "\n",
        "    for intent, samples in results_by_intent.items():\n",
        "        orig_count = original_counts.get(intent, 0)\n",
        "        current_count = len(samples)\n",
        "\n",
        "        # Keep all original data\n",
        "        original_data = samples[:orig_count]\n",
        "        balanced_results[intent].extend(original_data)\n",
        "\n",
        "        # Get augmented samples (everything after original data)\n",
        "        augmented_data = samples[orig_count:]\n",
        "\n",
        "        # Calculate how many we need\n",
        "        remaining_slots = target_samples_per_class - orig_count\n",
        "\n",
        "        if remaining_slots > 0:\n",
        "            # If we have enough augmented samples\n",
        "            if len(augmented_data) >= remaining_slots:\n",
        "                # Randomize selection\n",
        "                random.shuffle(augmented_data)\n",
        "                # Add what we need\n",
        "                balanced_results[intent].extend(augmented_data[:remaining_slots])\n",
        "            else:\n",
        "                # If we don't have enough, add all augmented samples\n",
        "                balanced_results[intent].extend(augmented_data)\n",
        "                # And duplicate some if needed (to reach closer to target)\n",
        "                shortage = remaining_slots - len(augmented_data)\n",
        "                if shortage > 0 and len(augmented_data) > 0:\n",
        "                    # Add duplicates of existing augmentations to help reach target\n",
        "                    extras = random.choices(augmented_data, k=min(shortage, len(augmented_data) * 2))\n",
        "                    balanced_results[intent].extend(extras)\n",
        "\n",
        "    return balanced_results\n",
        "\n",
        "def plot_distribution(data, title):\n",
        "    \"\"\"Plot distribution of samples by intent\"\"\"\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    data['intent'].value_counts().sort_index().plot(kind='bar', color='skyblue')\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Intent\")\n",
        "    plt.ylabel(\"Jumlah Sampel\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# =========[ MAIN PROCESS ]=========\n",
        "def main():\n",
        "    \"\"\"Main process for dataset augmentation\"\"\"\n",
        "    # Set up file paths based on chosen data type\n",
        "    if DATA_TYPE == \"train\":\n",
        "        OUTPUT_FILE = \"train.csv\"\n",
        "        input_file = INPUT_FILE\n",
        "        output_file = OUTPUT_FILE\n",
        "    elif DATA_TYPE == \"val\":\n",
        "        # If different paths needed for validation\n",
        "        OUTPUT_FILE = \"val.csv\"\n",
        "        input_file = INPUT_FILE.replace(\"train\", \"val\")\n",
        "        output_file = OUTPUT_FILE.replace(\"train\", \"val\")\n",
        "    elif DATA_TYPE == \"test\":\n",
        "        # If different paths needed for test\n",
        "        OUTPUT_FILE = \"test.csv\"\n",
        "        input_file = INPUT_FILE.replace(\"train\", \"test\")\n",
        "        output_file = OUTPUT_FILE.replace(\"train\", \"test\")\n",
        "    else:\n",
        "        input_file = INPUT_FILE\n",
        "        output_file = OUTPUT_FILE\n",
        "\n",
        "    # Read the dataset\n",
        "    df = read_dataset(input_file)\n",
        "\n",
        "    # Initialize paraphrase model if enabled\n",
        "    model, tokenizer = initialize_paraphrase_model()\n",
        "\n",
        "    # Count original samples per intent\n",
        "    intent_counts = Counter(df['intent'])\n",
        "    print(\"Original class distribution:\")\n",
        "    for intent, count in intent_counts.items():\n",
        "        print(f\"  {intent}: {count}\")\n",
        "\n",
        "    # Calculate augmentation factors for balancing\n",
        "    augmentation_factors = {}\n",
        "    for intent, count in intent_counts.items():\n",
        "        if count >= TARGET_SAMPLES_PER_CLASS:\n",
        "            augmentation_factors[intent] = 1  # Minimum factor\n",
        "        else:\n",
        "            factor = max(1, min(10, TARGET_SAMPLES_PER_CLASS / count))  # Reduced max factor from 10 to 5\n",
        "            augmentation_factors[intent] = factor\n",
        "\n",
        "    print(\"\\nAugmentation factors:\")\n",
        "    for intent, factor in augmentation_factors.items():\n",
        "        print(f\"  {intent}: {factor:.2f}x\")\n",
        "\n",
        "    # Start augmentation process\n",
        "    print(\"Starting balanced augmentation...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    augmented_results = defaultdict(list)\n",
        "    paraphrase_candidates = defaultdict(list)\n",
        "\n",
        "    # First, add all original data\n",
        "    for _, row in df.iterrows():\n",
        "        intent = row['intent']\n",
        "        text = row['text']\n",
        "        augmented_results[intent].append(text)\n",
        "\n",
        "    # Then determine augmentation targets for each intent\n",
        "    for intent, factor in augmentation_factors.items():\n",
        "        original_count = intent_counts[intent]\n",
        "        intent_df = df[df['intent'] == intent]\n",
        "\n",
        "        for _, row in tqdm(intent_df.iterrows(), desc=f\"Augmenting '{intent}'\", total=len(intent_df)):\n",
        "            text = row['text']\n",
        "\n",
        "            # Calculate needed augmentations for this sample\n",
        "            num_augmentations = max(\n",
        "                MIN_AUGMENTATIONS_PER_SAMPLE,\n",
        "                min(MAX_AUGMENTATIONS_PER_SAMPLE, int(factor * 2.0))  # Reduced multiplier from 2 to 1.5\n",
        "            )\n",
        "\n",
        "            # Regular augmentations\n",
        "            attempts = 0\n",
        "            augmentations_created = 0\n",
        "\n",
        "            while augmentations_created < num_augmentations and attempts < num_augmentations * 4:\n",
        "                attempts += 1\n",
        "                # More conservative intensity calculation\n",
        "                intensity = min(1.0, NOISE_INTENSITY + (factor - 1) * 0.2)  # Use NOISE_INTENSITY directly\n",
        "                aug_text = augment_text(text, intent, intensity)\n",
        "\n",
        "\n",
        "                if aug_text.lower() != text.lower() and validate_augmentation(text, aug_text):\n",
        "                    augmented_results[intent].append(aug_text)\n",
        "                    augmentations_created += 1\n",
        "\n",
        "            # For paraphrase model processing (batch later)\n",
        "            if USE_PARAPHRASE_MODEL and augmentations_created < num_augmentations:\n",
        "                paraphrase_candidates[intent].append(text)\n",
        "\n",
        "    # Batch paraphrase additional samples if needed\n",
        "    if USE_PARAPHRASE_MODEL and model is not None:\n",
        "        print(\"\\nApplying paraphrase model in batches...\")\n",
        "\n",
        "        for intent, texts in paraphrase_candidates.items():\n",
        "            # Skip intents that already have enough samples\n",
        "            if len(augmented_results[intent]) >= TARGET_SAMPLES_PER_CLASS:\n",
        "                continue\n",
        "\n",
        "            print(f\"  Processing {len(texts)} texts for intent '{intent}'\")\n",
        "\n",
        "            needed = TARGET_SAMPLES_PER_CLASS - len(augmented_results[intent])\n",
        "            # Process only what we need with a small buffer\n",
        "            process_count = min(len(texts), needed * 2)\n",
        "\n",
        "            # Get a random sample if there are many candidates\n",
        "            if len(texts) > process_count:\n",
        "                texts_to_process = random.sample(texts, process_count)\n",
        "            else:\n",
        "                texts_to_process = texts\n",
        "\n",
        "            # Process in batches\n",
        "            paraphrased = batch_paraphrase(model, tokenizer, texts_to_process, BATCH_SIZE)\n",
        "\n",
        "            # Add valid paraphrases\n",
        "            valid_count = 0\n",
        "            for orig, para in zip(texts_to_process, paraphrased):\n",
        "                if validate_augmentation(orig, para):\n",
        "                    augmented_results[intent].append(para)\n",
        "                    valid_count += 1\n",
        "\n",
        "                    # Stop if we have enough\n",
        "                    if len(augmented_results[intent]) >= TARGET_SAMPLES_PER_CLASS:\n",
        "                        break\n",
        "\n",
        "            print(f\"    Added {valid_count} valid paraphrases\")\n",
        "\n",
        "    # Balance the data\n",
        "    print(\"Balancing final dataset...\")\n",
        "    balanced_data = balance_samples(augmented_results, TARGET_SAMPLES_PER_CLASS, intent_counts)\n",
        "    # After all augmentation is done, check if classes are balanced\n",
        "    min_class_size = min(len(samples) for samples in balanced_data.values())\n",
        "\n",
        "    # If classes are not balanced, downsample the larger classes\n",
        "    for intent in balanced_data:\n",
        "        if len(balanced_data[intent]) > min_class_size:\n",
        "            # Keep all original data\n",
        "            orig_count = intent_counts.get(intent, 0)\n",
        "            original_data = balanced_data[intent][:orig_count]\n",
        "            augmented_data = balanced_data[intent][orig_count:]\n",
        "\n",
        "            # Randomly select augmented data to keep\n",
        "            needed = min_class_size - orig_count\n",
        "            if needed > 0 and augmented_data:\n",
        "                random.shuffle(augmented_data)\n",
        "                balanced_data[intent] = original_data + augmented_data[:needed]\n",
        "            else:\n",
        "                balanced_data[intent] = original_data[:min_class_size]\n",
        "    # Create final balanced dataframe\n",
        "    rows = []\n",
        "    for intent, texts in balanced_data.items():\n",
        "        for text in texts:\n",
        "            rows.append({\"intent\": intent, \"text\": text})\n",
        "\n",
        "    final_df = pd.DataFrame(rows)\n",
        "\n",
        "    # Print final statistics\n",
        "    print(\"\\nFinal dataset statistics:\")\n",
        "    final_counts = Counter(final_df['intent'])\n",
        "    for intent, count in final_counts.items():\n",
        "        orig = intent_counts.get(intent, 0)\n",
        "        added = count - orig\n",
        "        print(f\"  {intent}: {count} total ({orig} original + {added} augmented)\")\n",
        "\n",
        "    # Save to file\n",
        "    final_df.to_csv(output_file, index=False)\n",
        "    print(f\"\\nSaved balanced dataset to {output_file}\")\n",
        "\n",
        "    # Plot distributions\n",
        "    try:\n",
        "        print(\"Plotting class distributions...\")\n",
        "        plot_distribution(df, \"Original Distribution\")\n",
        "        plot_distribution(final_df, \"Augmented Distribution\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting distributions: {e}\")\n",
        "\n",
        "    # Calculate statistics\n",
        "    original_total = len(df)\n",
        "    augmented_total = len(final_df)\n",
        "    time_taken = time.time() - start_time\n",
        "\n",
        "    print(f\"\\nSummary:\")\n",
        "    print(f\"  Original samples: {original_total}\")\n",
        "    print(f\"  Final samples: {augmented_total}\")\n",
        "    print(f\"  Added samples: {augmented_total - original_total}\")\n",
        "    print(f\"  Augmentation ratio: {augmented_total / original_total:.2f}x\")\n",
        "    print(f\"  Processing time: {time_taken:.2f} seconds\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title balance_and_reduce_dataset\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def balance_and_reduce_dataset(filepath, output_filepath):\n",
        "    \"\"\"\n",
        "    Balances and reduces a dataset, ensuring equal representation of each class.\n",
        "    Supports both .csv and .xlsx input files.\n",
        "\n",
        "    Args:\n",
        "        filepath: Path to the input CSV or XLSX file.\n",
        "        output_filepath: Path to save the balanced and reduced dataset.\n",
        "    \"\"\"\n",
        "    # Check file extension\n",
        "    file_ext = os.path.splitext(filepath)[-1].lower()\n",
        "\n",
        "    if file_ext == '.xlsx':\n",
        "        try:\n",
        "            df = pd.read_excel(filepath)\n",
        "            # Convert to CSV first\n",
        "            temp_csv_path = filepath.replace('.xlsx', '_converted.csv')\n",
        "            df.to_csv(temp_csv_path, index=False)\n",
        "            print(f\"Converted XLSX to CSV: {temp_csv_path}\")\n",
        "            filepath = temp_csv_path  # Continue using the converted file\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading or converting Excel file: {e}\")\n",
        "            return\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(filepath)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {filepath}\")\n",
        "        return\n",
        "    except pd.errors.ParserError:\n",
        "        print(f\"Error: Unable to parse the CSV file at {filepath}\")\n",
        "        return\n",
        "\n",
        "    # Validate required column\n",
        "    if 'intent' not in df.columns:\n",
        "        print(\"Error: 'intent' column not found in the dataset.\")\n",
        "        return\n",
        "\n",
        "    # Hitung setengah dari jumlah data tiap intent (min 1)\n",
        "    min_samples_per_class = max(1, df['intent'].value_counts().min() // 2)\n",
        "\n",
        "    balanced_df = pd.DataFrame(columns=df.columns)\n",
        "\n",
        "    for intent in df['intent'].unique():\n",
        "        intent_data = df[df['intent'] == intent]\n",
        "        if len(intent_data) < min_samples_per_class:\n",
        "            print(f\"Skipping intent '{intent}' due to insufficient data.\")\n",
        "            continue\n",
        "\n",
        "        sampled_intent_data = intent_data.sample(n=min_samples_per_class, random_state=42)\n",
        "        balanced_df = pd.concat([balanced_df, sampled_intent_data], ignore_index=True)\n",
        "\n",
        "    if balanced_df.empty:\n",
        "        print(\"Resulting dataset is empty. No intents had enough data.\")\n",
        "        return\n",
        "\n",
        "    balanced_df.to_csv(output_filepath, index=False)\n",
        "    print(f\"Balanced and reduced dataset saved to {output_filepath}\")\n",
        "\n",
        "#balance_and_reduce_dataset('/content/ChatbotPerpusBipa/train.xlsx', '/content/test.csv')\n",
        "balance_and_reduce_dataset('/content/train.csv', '/content/test.csv')\n",
        "\n"
      ],
      "metadata": {
        "id": "6__vZAEFwm0m",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test set\n",
        "import pandas as pd\n",
        "\n",
        "data = [\n",
        "    (\"Halo, selamat pagi!\", \"greeting\"),\n",
        "    (\"Hai admin, apa kabar?\", \"greeting\"),\n",
        "    (\"Selamat siang, min!\", \"greeting\"),\n",
        "    (\"Halo, saya mau tanya.\", \"greeting\"),\n",
        "    (\"Permisi, boleh bertanya?\", \"greeting\"),\n",
        "    (\"Assalamualaikum\", \"greeting\"),\n",
        "    (\"Hallo kak, semoga harimu menyenangkan\", \"greeting\"),\n",
        "    (\"Selamat malam semuanya!\", \"greeting\"),\n",
        "    (\"Halo saya ingin tanya\", \"greeting\"),\n",
        "    (\"bantuin\", \"greeting\"),\n",
        "\n",
        "    (\"Oke, saya setuju.\", \"confirm\"),\n",
        "    (\"Benar, itu yang saya maksud.\", \"confirm\"),\n",
        "    (\"Ya, saya menyetujui.\", \"confirm\"),\n",
        "    (\"Baik, lanjutkan saja.\", \"confirm\"),\n",
        "    (\"Saya sepakat dengan hal itu.\", \"confirm\"),\n",
        "    (\"Iya, saya setuju banget.\", \"confirm\"),\n",
        "    (\"Sudah sesuai kok.\", \"confirm\"),\n",
        "    (\"Silakan, itu benar.\", \"confirm\"),\n",
        "    (\"Aku menyetujuinya\", \"confirm\"),\n",
        "    (\"YES\", \"confirm\"),\n",
        "\n",
        "    (\"Tidak, saya tidak setuju.\", \"denied\"),\n",
        "    (\"Bukan, bukan itu maksud saya.\", \"denied\"),\n",
        "    (\"Maaf, saya kurang setuju.\", \"denied\"),\n",
        "    (\"Itu bukan yang saya cari.\", \"denied\"),\n",
        "    (\"Saya rasa itu salah.\", \"denied\"),\n",
        "    (\"Tidak sesuai dengan kebutuhan saya.\", \"denied\"),\n",
        "    (\"Bukan begitu, maaf.\", \"denied\"),\n",
        "    (\"Sepertinya bukan itu.\", \"denied\"),\n",
        "    (\"Saya menolak opsi tersebut.\", \"denied\"),\n",
        "    (\"Nah Nigga.\", \"denied\"),\n",
        "\n",
        "    (\"Terima kasih, sampai jumpa.\", \"goodbye\"),\n",
        "    (\"Oke, makasih ya min.\", \"goodbye\"),\n",
        "    (\"Dadah, sampai nanti.\", \"goodbye\"),\n",
        "    (\"Selamat tinggal.\", \"goodbye\"),\n",
        "    (\"Sampai bertemu kembali.\", \"goodbye\"),\n",
        "    (\"Saya pamit dulu ya.\", \"goodbye\"),\n",
        "    (\"Sekian dari saya, terima kasih.\", \"goodbye\"),\n",
        "    (\"Terima kasih atas bantuannya\", \"goodbye\"),\n",
        "    (\"Sampai ketemu lain waktu\", \"goodbye\"),\n",
        "    (\"Saya keluar dulu ya\", \"goodbye\"),\n",
        "\n",
        "    (\"Jam berapa perpustakaan buka?\", \"jam_layanan\"),\n",
        "    (\"Boleh tahu jam operasionalnya?\", \"jam_layanan\"),\n",
        "    (\"Library buka hari Sabtu nggak?\", \"jam_layanan\"),\n",
        "    (\"Perpustakaan tutup hari Minggu?\", \"jam_layanan\"),\n",
        "    (\"Jadwal buka perpustakaan apa ya?\", \"jam_layanan\"),\n",
        "    (\"Sampai jam berapa buka hari ini?\", \"jam_layanan\"),\n",
        "    (\"Jam layanan offline sampai kapan?\", \"jam_layanan\"),\n",
        "    (\"Ada libur operasional?\", \"jam_layanan\"),\n",
        "    (\"Hari libur nasional tetap buka?\", \"jam_layanan\"),\n",
        "    (\"Bisa info jam buka lengkapnya?\", \"jam_layanan\"),\n",
        "\n",
        "    (\"Ada buku tentang jaringan komputer?\", \"cari_buku\"),\n",
        "    (\"Saya cari buku algoritma dasar.\", \"cari_buku\"),\n",
        "    (\"Punya buku sejarah Indonesia?\", \"cari_buku\"),\n",
        "    (\"Buku tentang pemrograman Python ada?\", \"cari_buku\"),\n",
        "    (\"Saya mau pinjam buku filsafat.\", \"cari_buku\"),\n",
        "    (\"Ada daftar buku terbaru?\", \"cari_buku\"),\n",
        "    (\"Buku manajemen bisnis tersedia?\", \"cari_buku\"),\n",
        "    (\"Apakah ada buku referensi skripsi?\", \"cari_buku\"),\n",
        "    (\"Buku motivasi diri ada?\", \"cari_buku\"),\n",
        "    (\"Saya cari novel fiksi, ada?\", \"cari_buku\"),\n",
        "\n",
        "    (\"Bagaimana cara menjadi anggota perpustakaan?\", \"keanggotaan\"),\n",
        "    (\"Syarat untuk mendaftar keanggotaan apa saja?\", \"keanggotaan\"),\n",
        "    (\"Saya mahasiswa, apakah bisa daftar jadi anggota?\", \"keanggotaan\"),\n",
        "    (\"Apakah perlu kartu mahasiswa untuk jadi anggota?\", \"keanggotaan\"),\n",
        "    (\"Dimana saya bisa mendaftar keanggotaan perpustakaan?\", \"keanggotaan\"),\n",
        "    (\"Berapa lama keanggotaan perpustakaan berlaku?\", \"keanggotaan\"),\n",
        "    (\"Apa keuntungan jadi anggota perpustakaan?\", \"keanggotaan\"),\n",
        "    (\"Apakah bisa pinjam buku kalau belum jadi anggota?\", \"keanggotaan\"),\n",
        "    (\"Saya ingin akses fasilitas, apakah perlu jadi anggota?\", \"keanggotaan\"),\n",
        "    (\"Apakah mahasiswa dari kampus lain bisa jadi anggota?\", \"keanggotaan\"),\n",
        "    # Greeting (20 tambahan)\n",
        "    (\"Halo min, apa kabar?\", \"greeting\"),\n",
        "    (\"Hai semuanya!\", \"greeting\"),\n",
        "    (\"Selamat sore!\", \"greeting\"),\n",
        "    (\"Assalamualaikum wr wb\", \"greeting\"),\n",
        "    (\"Hallo teman-teman!\", \"greeting\"),\n",
        "    (\"Pagi min!\", \"greeting\"),\n",
        "    (\"Hi kak\", \"greeting\"),\n",
        "    (\"Salam sejahtera!\", \"greeting\"),\n",
        "    (\"Selamat beraktivitas!\", \"greeting\"),\n",
        "    (\"Semangat pagi semuanya!\", \"greeting\"),\n",
        "    (\"Permisi, mau nanya nih.\", \"greeting\"),\n",
        "    (\"Helo, min!\", \"greeting\"),\n",
        "    (\"Good morning!\", \"greeting\"),\n",
        "    (\"Halo, saya butuh bantuan.\", \"greeting\"),\n",
        "    (\"Sore min, ada yang mau saya tanyain.\", \"greeting\"),\n",
        "    (\"Haii, anyone here?\", \"greeting\"),\n",
        "    (\"Woi min!\", \"greeting\"),\n",
        "    (\"Met pagi\", \"greeting\"),\n",
        "    (\"Hi min, salam kenal!\", \"greeting\"),\n",
        "    (\"Assalamu'alaikum min\", \"greeting\"),\n",
        "\n",
        "    # Confirm (20 tambahan)\n",
        "    (\"Yes, betul sekali.\", \"confirm\"),\n",
        "    (\"Yap, itu maksud saya.\", \"confirm\"),\n",
        "    (\"Iya kak, benar.\", \"confirm\"),\n",
        "    (\"Sip, lanjutkan.\", \"confirm\"),\n",
        "    (\"Sudah sesuai min.\", \"confirm\"),\n",
        "    (\"Tepat sekali!\", \"confirm\"),\n",
        "    (\"Aku oke dengan itu.\", \"confirm\"),\n",
        "    (\"Betul begitu min.\", \"confirm\"),\n",
        "    (\"Sangat setuju!\", \"confirm\"),\n",
        "    (\"Ya sudah, saya setuju.\", \"confirm\"),\n",
        "    (\"Mantap, gaskeun!\", \"confirm\"),\n",
        "    (\"Oke, silakan dilanjut.\", \"confirm\"),\n",
        "    (\"Benar min, lanjut.\", \"confirm\"),\n",
        "    (\"Setuju aja deh.\", \"confirm\"),\n",
        "    (\"Okelah.\", \"confirm\"),\n",
        "    (\"Sepakat saya.\", \"confirm\"),\n",
        "    (\"Udah cocok kok.\", \"confirm\"),\n",
        "    (\"Bener bgt min.\", \"confirm\"),\n",
        "    (\"Yes please.\", \"confirm\"),\n",
        "    (\"Cocok tuh.\", \"confirm\"),\n",
        "\n",
        "    # Denied (20 tambahan)\n",
        "    (\"Gak, bukan itu.\", \"denied\"),\n",
        "    (\"Maaf, salah.\", \"denied\"),\n",
        "    (\"Sepertinya tidak.\", \"denied\"),\n",
        "    (\"Aku kurang setuju deh.\", \"denied\"),\n",
        "    (\"Saya rasa itu tidak tepat.\", \"denied\"),\n",
        "    (\"Bukan pilihan saya.\", \"denied\"),\n",
        "    (\"Kurang cocok sih.\", \"denied\"),\n",
        "    (\"Gak sesuai ekspektasi saya.\", \"denied\"),\n",
        "    (\"Nope, salah.\", \"denied\"),\n",
        "    (\"Sepertinya beda deh.\", \"denied\"),\n",
        "    (\"Maaf ya, saya tidak sependapat.\", \"denied\"),\n",
        "    (\"Enggak, tolong cek lagi.\", \"denied\"),\n",
        "    (\"Menurut saya salah.\", \"denied\"),\n",
        "    (\"Gak cocok min.\", \"denied\"),\n",
        "    (\"Itu kayaknya bukan.\", \"denied\"),\n",
        "    (\"No, thanks.\", \"denied\"),\n",
        "    (\"Saya tolak ya.\", \"denied\"),\n",
        "    (\"Saya gak mau itu.\", \"denied\"),\n",
        "    (\"Kurang sesuai dengan kebutuhan saya.\", \"denied\"),\n",
        "    (\"Sayangnya, tidak.\", \"denied\"),\n",
        "\n",
        "    # Goodbye (20 tambahan)\n",
        "    (\"Bye bye!\", \"goodbye\"),\n",
        "    (\"Makasih banyak ya!\", \"goodbye\"),\n",
        "    (\"Sampai ketemu lagi.\", \"goodbye\"),\n",
        "    (\"Dadaah!\", \"goodbye\"),\n",
        "    (\"Thanks ya!\", \"goodbye\"),\n",
        "    (\"Terima kasih min.\", \"goodbye\"),\n",
        "    (\"Saya logout dulu.\", \"goodbye\"),\n",
        "    (\"Sampai jumpa min.\", \"goodbye\"),\n",
        "    (\"Saya undur diri.\", \"goodbye\"),\n",
        "    (\"Mau pamit dulu.\", \"goodbye\"),\n",
        "    (\"See you again.\", \"goodbye\"),\n",
        "    (\"Oke, sampai lain waktu.\", \"goodbye\"),\n",
        "    (\"Saya off dulu ya.\", \"goodbye\"),\n",
        "    (\"Mau keluar dulu.\", \"goodbye\"),\n",
        "    (\"Terimakasih atas jawabannya.\", \"goodbye\"),\n",
        "    (\"Makasih, sukses selalu!\", \"goodbye\"),\n",
        "    (\"Mau izin pamit.\", \"goodbye\"),\n",
        "    (\"Thanks and goodbye.\", \"goodbye\"),\n",
        "    (\"Pamit ya min.\", \"goodbye\"),\n",
        "    (\"Terima kasih untuk bantuannya.\", \"goodbye\"),\n",
        "\n",
        "    # Jam Layanan (20 tambahan)\n",
        "    (\"Jam buka dari jam berapa?\", \"jam_layanan\"),\n",
        "    (\"Hari ini buka jam berapa?\", \"jam_layanan\"),\n",
        "    (\"Library tutup jam berapa?\", \"jam_layanan\"),\n",
        "    (\"Jam operasional library?\", \"jam_layanan\"),\n",
        "    (\"Buka sampai jam berapa ya?\", \"jam_layanan\"),\n",
        "    (\"Weekend buka ga min?\", \"jam_layanan\"),\n",
        "    (\"Ada perubahan jam buka hari ini?\", \"jam_layanan\"),\n",
        "    (\"Library open jam brp?\", \"jam_layanan\"),\n",
        "    (\"Kapan jam istirahatnya?\", \"jam_layanan\"),\n",
        "    (\"Perpustakaan ada waktu tutup istirahat?\", \"jam_layanan\"),\n",
        "    (\"Apakah tetap buka saat libur?\", \"jam_layanan\"),\n",
        "    (\"Hari Sabtu tutup jam berapa?\", \"jam_layanan\"),\n",
        "    (\"Minggu buka gak?\", \"jam_layanan\"),\n",
        "    (\"Saya mau ke library, buka gak?\", \"jam_layanan\"),\n",
        "    (\"Ada jadwal operasional khusus?\", \"jam_layanan\"),\n",
        "    (\"Perubahan jam buka saat puasa?\", \"jam_layanan\"),\n",
        "    (\"Jam layanan apa aja?\", \"jam_layanan\"),\n",
        "    (\"Kalau sore masih buka?\", \"jam_layanan\"),\n",
        "    (\"Malam buka sampai jam berapa?\", \"jam_layanan\"),\n",
        "    (\"Library tutupnya jam berapa?\", \"jam_layanan\"),\n",
        "\n",
        "    # Cari Buku (20 tambahan)\n",
        "    (\"Ada buku tentang Java?\", \"cari_buku\"),\n",
        "    (\"Saya butuh buku desain grafis.\", \"cari_buku\"),\n",
        "    (\"Mau cari buku psikologi.\", \"cari_buku\"),\n",
        "    (\"Novel cinta terbaru ada?\", \"cari_buku\"),\n",
        "    (\"Buku teknik mesin tersedia?\", \"cari_buku\"),\n",
        "    (\"Ada buku panduan skripsi?\", \"cari_buku\"),\n",
        "    (\"Buku motivasi tersedia?\", \"cari_buku\"),\n",
        "    (\"Ada koleksi buku bisnis?\", \"cari_buku\"),\n",
        "    (\"Perlu buku tentang UI/UX.\", \"cari_buku\"),\n",
        "    (\"Butuh referensi tentang ekonomi.\", \"cari_buku\"),\n",
        "    (\"Punya buku tentang marketing digital?\", \"cari_buku\"),\n",
        "    (\"Mau pinjam buku drama Korea.\", \"cari_buku\"),\n",
        "    (\"Ada koleksi buku klasik?\", \"cari_buku\"),\n",
        "    (\"Butuh buku tentang filsafat Yunani.\", \"cari_buku\"),\n",
        "    (\"Ada buku tentang Arduino?\", \"cari_buku\"),\n",
        "    (\"Mau referensi buku matematika.\", \"cari_buku\"),\n",
        "    (\"Punya buku tentang AI?\", \"cari_buku\"),\n",
        "    (\"Cari buku resep masakan.\", \"cari_buku\"),\n",
        "    (\"Ada novel petualangan?\", \"cari_buku\"),\n",
        "    (\"Saya perlu buku ensiklopedia.\", \"cari_buku\"),\n",
        "\n",
        "    # Keanggotaan (20 tambahan)\n",
        "    (\"Bagaimana daftar jadi member?\", \"keanggotaan\"),\n",
        "    (\"Syarat jadi anggota apa aja?\", \"keanggotaan\"),\n",
        "    (\"Saya ingin daftar keanggotaan.\", \"keanggotaan\"),\n",
        "    (\"Cara registrasi anggota?\", \"keanggotaan\"),\n",
        "    (\"Apakah ada biaya pendaftaran?\", \"keanggotaan\"),\n",
        "    (\"Kalau alumni, bisa daftar keanggotaan?\", \"keanggotaan\"),\n",
        "    (\"Mahasiswa baru bisa jadi member?\", \"keanggotaan\"),\n",
        "    (\"Saya mau daftar jadi member, gimana caranya?\", \"keanggotaan\"),\n",
        "    (\"Apakah butuh foto untuk daftar?\", \"keanggotaan\"),\n",
        "    (\"Boleh mendaftar online?\", \"keanggotaan\"),\n",
        "    (\"Dimana tempat daftar keanggotaan?\", \"keanggotaan\"),\n",
        "    (\"Berapa lama masa berlaku anggota?\", \"keanggotaan\"),\n",
        "    (\"Benefit apa saja jadi member?\", \"keanggotaan\"),\n",
        "    (\"Apa saja fasilitas untuk anggota?\", \"keanggotaan\"),\n",
        "    (\"Syarat perpanjangan keanggotaan?\", \"keanggotaan\"),\n",
        "    (\"Bisa akses e-library kalau sudah anggota?\", \"keanggotaan\"),\n",
        "    (\"Harus bawa KTP untuk daftar?\", \"keanggotaan\"),\n",
        "    (\"Bagaimana prosedur mendaftar anggota?\", \"keanggotaan\"),\n",
        "    (\"Adakah biaya tahunan untuk anggota?\", \"keanggotaan\"),\n",
        "    (\"Kalau hilang kartu anggota, gimana?\", \"keanggotaan\"),\n",
        "    (\"Apa saja fasilitas yang tersedia di perpustakaan?\", \"fasilitas\"),\n",
        "    (\"Perpustakaan ini punya ruang diskusi gak?\", \"fasilitas\"),\n",
        "    (\"Apakah ada ruang baca yang nyaman?\", \"fasilitas\"),\n",
        "    (\"Saya butuh ruang untuk belajar kelompok, ada?\", \"fasilitas\"),\n",
        "    (\"Fasilitas komputer di perpustakaan bisa dipakai umum?\", \"fasilitas\"),\n",
        "    (\"Ada wifi gratis di sana?\", \"fasilitas\"),\n",
        "    (\"Punya fasilitas cetak atau fotokopi gak?\", \"fasilitas\"),\n",
        "    (\"Apa di perpustakaan ini ada area khusus multimedia?\", \"fasilitas\"),\n",
        "    (\"Saya bisa akses e-book dari fasilitas perpustakaan?\", \"fasilitas\"),\n",
        "    (\"Apakah ada tempat duduk dengan colokan listrik?\", \"fasilitas\"),\n",
        "\n",
        "    (\"Boleh tau sarana pendukung yang disediakan?\", \"fasilitas\"),\n",
        "    (\"Ada fasilitas untuk penyandang disabilitas?\", \"fasilitas\"),\n",
        "    (\"Perpustakaan punya ruang istirahat mahasiswa?\", \"fasilitas\"),\n",
        "    (\"Adakah ruang privat untuk baca buku?\", \"fasilitas\"),\n",
        "    (\"Fasilitas scan dokumen tersedia di perpustakaan ini?\", \"fasilitas\"),\n",
        "    (\"Bisa sebutkan apa aja layanan atau fasilitas yang ada?\", \"fasilitas\"),\n",
        "    (\"Saya cari tempat untuk ngetik tugas, ada komputer umum?\", \"fasilitas\"),\n",
        "    (\"Adakah tempat parkir untuk pengunjung?\", \"fasilitas\"),\n",
        "    (\"Apakah ada ruang diskusi ber-AC?\", \"fasilitas\"),\n",
        "    (\"Bisa pinjam laptop atau tablet di perpustakaan?\", \"fasilitas\"),\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(data, columns=[\"text\", \"intent\"])\n",
        "df.to_csv(\"test.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "2bICI1U4QZLy",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title hapus duplikat\n",
        "\n",
        "# prompt: train.csv the dataset has coloumn text and intent, scans the datasets, and check if there is any duplicate row on text in same intent, and clear it so there is only one. and print the results\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv('/content/test.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: train.csv not found. Please upload the file to the Colab environment.\")\n",
        "    exit()\n",
        "\n",
        "# Remove duplicates based on 'text' and 'intent' columns, keeping the first occurrence\n",
        "df_deduplicated = df.drop_duplicates(subset=['text', 'intent'], keep='first')\n",
        "\n",
        "# Print the shape of the original and deduplicated DataFrames\n",
        "print(f\"Original DataFrame shape: {df.shape}\")\n",
        "print(f\"Deduplicated DataFrame shape: {df_deduplicated.shape}\")\n",
        "\n",
        "# Print the deduplicated DataFrame (optional)\n",
        "df_deduplicated\n",
        "\n",
        "# Save the deduplicated DataFrame to a new CSV file (optional)\n",
        "df_deduplicated.to_csv('tests.csv', index=False)\n",
        "print(f\"Number of rows before removing duplicates: {len(df)}\")\n",
        "print(f\"Number of rows after removing duplicates: {len(df_deduplicated)}\")\n",
        "\n",
        "# Display the intent distribution\n",
        "intent_distribution = df_deduplicated['intent'].value_counts()\n",
        "print(\"\\nIntent Distribution:\")\n",
        "intent_distribution\n"
      ],
      "metadata": {
        "id": "klrC0Xom8Vot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title combine dataset\n",
        "# prompt: combine 2 dataset csv becasuse same coloumn\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the two datasets\n",
        "try:\n",
        "    df1 = pd.read_csv('/content/train.csv')\n",
        "    df2 = pd.read_csv('/content/test.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: One or both CSV files not found. Please upload the files to the Colab environment.\")\n",
        "    exit()\n",
        "\n",
        "# Combine the datasets based on the 'intent' column\n",
        "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "# Remove duplicates based on 'text' and 'intent', keeping the first occurrence\n",
        "combined_df_deduplicated = combined_df.drop_duplicates(subset=['text', 'intent'], keep='first')\n",
        "\n",
        "\n",
        "# Print some info\n",
        "print(f\"Shape of df1: {df1.shape}\")\n",
        "print(f\"Shape of df2: {df2.shape}\")\n",
        "print(f\"Shape of combined_df: {combined_df.shape}\")\n",
        "print(f\"Shape of combined_df_deduplicated: {combined_df_deduplicated.shape}\")\n",
        "\n",
        "# Save the combined and deduplicated DataFrame to a new CSV file\n",
        "combined_df_deduplicated.to_csv('combined_dataset.csv', index=False)\n"
      ],
      "metadata": {
        "id": "oiVwLBrzJ0tR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title json\n",
        "import json\n",
        "\n",
        "# Function to save dictionary to JSON file\n",
        "def save_to_json(dictionary, filename):\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(dictionary, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"Saved {filename}\")\n",
        "\n",
        "# =========[ IMPROVED SYNONYM DICTIONARY ]=========\n",
        "# Kamus sinonim Indonesia untuk kata-kata umum\n",
        "id_synonyms = {\n",
        "\t\"nigga\": [\"nigger\", \"hitam\", \"negro\", \"niger\"],\n",
        "\n",
        "    # Kata-kata perpisahan (goodbye)\n",
        "    \"sampai jumpa\": [\"dadah\", \"bye\", \"see you\", \"sampai nanti\", \"sampai bertemu lagi\", \"daa\", \"adios\"],\n",
        "    \"pergi\": [\"cabut\", \"berangkat\", \"angkat kaki\", \"pamit\", \"jalan\"],\n",
        "    \"selamat tinggal\": [\"goodbye\", \"babai\", \"ciao\"],\n",
        "\n",
        "    # Konfirmasi (confirm)\n",
        "    \"iya\": [\"ya\", \"yoi\", \"yup\", \"yo\", \"oke\", \"ok\", \"betul\", \"benar\", \"bener\", \"setuju\"],\n",
        "    \"pasti\": [\"tentu\", \"jelas\", \"pastinya\", \"sudah pasti\"],\n",
        "    \"benar\": [\"beneran\", \"bener\", \"emang\"],\n",
        "\n",
        "    # Penolakan (deny)\n",
        "    \"tidak\": [\"tak\", \"tiada\", \"bukan\", \"enggak\", \"nggak\", \"gak\", \"kagak\"],\n",
        "    \"tidak mau\": [\"nggak mau\", \"gak mau\", \"enggan\", \"tidak ingin\", \"males\"],\n",
        "    \"salah\": [\"keliru\", \"tidak benar\", \"ngaco\"],\n",
        "\n",
        "    # Jam buka (jam_buka)\n",
        "    \"jam\": [\"pukul\", \"waktu\"],\n",
        "    \"buka\": [\"operasional\", \"aktif\", \"mulai buka\", \"tersedia\"],\n",
        "    \"perpustakaan\": [\"perpus\", \"library\", \"tempat buku\"],\n",
        "    \"hari ini\": [\"sekarang\", \"hari ini juga\", \"kini\"],\n",
        "    \"jadwal\": [\"schedule\", \"waktu buka\", \"timetable\"],\n",
        "\n",
        "    # Cari buku (cari_buku)\n",
        "    \"cari\": [\"mencari\", \"nyari\", \"telusuri\", \"nemuin\"],\n",
        "    \"buku\": [\"bacaan\", \"kitab\", \"literatur\"],\n",
        "    \"judul\": [\"title\", \"nama buku\"],\n",
        "    \"pengarang\": [\"penulis\", \"author\", \"penyusun\"],\n",
        "    \"kategori\": [\"genre\", \"jenis\", \"topik\", \"tema\"],\n",
        "    \"dimana\": [\"di mana\", \"lokasi\", \"letak\"],\n",
        "\n",
        "    # Keanggotaan (keanggotaan)\n",
        "    \"anggota\": [\"member\", \"pengguna\", \"user\"],\n",
        "    \"daftar\": [\"registrasi\", \"pendaftaran\", \"gabung\", \"ikut\"],\n",
        "    \"bergabung\": [\"join\", \"ikut serta\", \"masuk\"],\n",
        "    \"biaya\": [\"bayaran\", \"harga\", \"uang\"],\n",
        "    \"syarat\": [\"ketentuan\", \"aturan\", \"persyaratan\"],\n",
        "\n",
        "    # Fasilitas (fasilitas)\n",
        "    \"fasilitas\": [\"sarana\", \"layanan\", \"fitur\", \"kemudahan\"],\n",
        "    \"apa saja\": [\"apa aja\", \"apa yang tersedia\", \"ada apa aja\"],\n",
        "    \"tersedia\": [\"ada\", \"disediakan\", \"dapat digunakan\"],\n",
        "    \"ruangan\": [\"tempat\", \"area\", \"kamar\", \"space\"],\n",
        "    \"komputer\": [\"PC\", \"laptop\", \"mesin\", \"alat\"],\n",
        "    \"wifi\": [\"internet\", \"akses wifi\", \"jaringan\", \"hotspot\"],\n",
        "    \"pinjam\": [\"pinjem\", \"minjem\", \"meminjam\", \"meminjem\"],\n",
        "\t  \"cara\": [\"ketentuan\", \"prosedur\", \"aturan\", \"caranya\"]\n",
        "}\n",
        "\n",
        "# =========[ INTENT-SPECIFIC SLANG DICTIONARIES ]=========\n",
        "# Kata gaul umum untuk semua intent\n",
        "common_slang = {\n",
        "    'tidak': 'gak',\n",
        "    'iya': 'yoi',\n",
        "    'terima kasih': 'makasih',\n",
        "    'saya': 'gw',\n",
        "    'kamu': 'lo',\n",
        "    'sedang': 'lagi',\n",
        "    'bagaimana': 'gimana',\n",
        "    'begitu': 'gitu',\n",
        "    'bisa': 'bsa',\n",
        "    'akan': 'bakal',\n",
        "    'untuk': 'buat',\n",
        "    'tahu': 'tau',\n",
        "    'apakah': 'apa',\n",
        "    'mengapa': 'kenapa',\n",
        "    'selamat': 'selamet',\n",
        "    'dengan': 'dgn',\n",
        "    'sangat': 'banget'\n",
        "}\n",
        "\n",
        "# Kamus kata gaul khusus intent\n",
        "intent_slang = {\n",
        "    'jam_layanan': {\n",
        "        'perpustakaan': 'perpus',\n",
        "        'buka': 'open',\n",
        "        'tutup': 'close',\n",
        "        'sampai': 'sampe',\n",
        "        'jam': 'jm',\n",
        "        'informasi': 'info',\n",
        "        'hari ini': 'hr ini',\n",
        "        'kapan': 'kpn',\n",
        "        'jadwal': 'jdwl',\n",
        "        'operasional': 'ops',\n",
        "        'layanan': 'lyn',\n",
        "        'masih': 'msih',\n",
        "        'minggu': 'mg',\n",
        "        'hari': 'hri',\n",
        "        'pukul': 'pkl',\n",
        "    },\n",
        "    'cari_buku': {\n",
        "        'mencari': 'nyari',\n",
        "        'mau mencari': 'mau nyari',\n",
        "        'ingin mencari': 'pengen nyari',\n",
        "        'tolong carikan': 'cariin',\n",
        "        'tolong bantu cari': 'bantuin cari',\n",
        "        'mencarikan': 'cariin',\n",
        "        'buku': 'book',\n",
        "        'butuh': 'need',\n",
        "        'melihat': 'liat',\n",
        "        'daftar': 'list',\n",
        "        'akses': 'akses',\n",
        "        'temukan': 'nemu',\n",
        "        'mencoba': 'nyoba',\n",
        "        'mengakses': 'akses',\n",
        "        'gunakan': 'make use',\n",
        "        'fitur': 'fitr',\n",
        "        'pencarian': 'search',\n",
        "        'referensi': 'ref',\n",
        "        'bantuan': 'bntuan',\n",
        "        'dimana': 'dmn',\n",
        "        'cek': 'check',\n",
        "        'lihat-lihat': 'liat2',\n",
        "        'cari': 'search',\n",
        "        'mencari buku': 'nyari book',\n",
        "    },\n",
        "    'greeting': {\n",
        "        'halo': 'haloo',\n",
        "        'hai': 'hay',\n",
        "        'hello': 'helo',\n",
        "        'selamat pagi': 'slmt pagi',\n",
        "        'selamat siang': 'slmt siang',\n",
        "        'selamat sore': 'slmt sore',\n",
        "        'selamat malam': 'slmt malam',\n",
        "        'apa kabar': 'apa kbr',\n",
        "        'assalamualaikum': 'asswrwb',\n",
        "        'permisi': 'permizz',\n",
        "        'hai bot': 'hey bot',\n",
        "        'bot': 'bt',\n",
        "        'selamat datang': 'slmt dtg',\n",
        "        # Additional variations for greeting\n",
        "        'halo selamat pagi': 'hai morning',\n",
        "        'hai selamat siang': 'helo siang',\n",
        "        'met pagi': 'morning',\n",
        "        'pagi': 'pgi',\n",
        "        'siang': 'siang boss',\n",
        "        'nigga': 'nigger',\n",
        "    },\n",
        "    'goodbye': {\n",
        "        'terima kasih': 'makasih',\n",
        "        'goodbye': 'gudbai',\n",
        "        'makasih': 'mksh',\n",
        "        'makasih ya': 'thx ya',\n",
        "        'sampai jumpa': 'sampe jmpa',\n",
        "        'dadah': 'daah',\n",
        "        'bye': 'byee',\n",
        "        'sampai nanti': 'sampe ntar',\n",
        "        'see you': 'cu',\n",
        "        'thanks': 'thx',\n",
        "        'thank you': 'tq',\n",
        "        'sekian': 'skian',\n",
        "        'itu saja': 'itu aj',\n",
        "        # Additional variations for goodbye\n",
        "        'ok makasih': 'ok thx',\n",
        "        'terima kasih banyak': 'thanks banget',\n",
        "        'makasih atas bantuannya': 'thx for helping',\n",
        "        'sampai bertemu lagi': 'see u later',\n",
        "        'selamat tinggal': 'bye bye',\n",
        "    },\n",
        "    'confirm': {\n",
        "        'betul': 'btl',\n",
        "        'setuju': 'stju',\n",
        "        'bener': 'bnr',\n",
        "        'iya benar': 'ya bnr',\n",
        "        'okey': 'okeyy',\n",
        "        'ok deh': 'okedeh',\n",
        "        # Additional variations for confirm\n",
        "        'tentu saja': 'tentu',\n",
        "        'saya setuju': 'aku setuju',\n",
        "        'benar sekali': 'bener banget',\n",
        "        'ya betul': 'yup betul',\n",
        "        'tentu boleh': 'boleh dong',\n",
        "        'setuju sekali': 'sangat setuju',\n",
        "    },\n",
        "    'denied': {\n",
        "        'tidak mau': 'gak mau',\n",
        "        'ga mau': 'gk mw',\n",
        "        'tidak setuju': 'gak setuju',\n",
        "        'saya tidak': 'aku ga',\n",
        "        'nggak perlu': 'ga perlu',\n",
        "        'ga usah': 'rasah',\n",
        "        'tidak perlu': 'gak usah',\n",
        "        'no': 'nope',\n",
        "        # Additional variations for denied\n",
        "        'saya tidak setuju': 'aku gak setuju',\n",
        "        'tidak bisa': 'ga bisa',\n",
        "        'tidak boleh': 'gak boleh',\n",
        "        'jangan': 'jgn',\n",
        "        'maaf tidak': 'sorry no',\n",
        "        'tidak begitu': 'gak gitu',\n",
        "    },\n",
        "    'keanggotaan': {\n",
        "        'anggota': 'anggta',\n",
        "        'keanggotaan': 'member',\n",
        "        'daftar': 'dftr',\n",
        "        'pendaftaran': 'daftarin',\n",
        "        'syarat': 'req',\n",
        "        'jadi anggota': 'jd member',\n",
        "        'mendaftar': 'daftar',\n",
        "        'mahasiswa': 'mhs',\n",
        "        'kampus': 'uni',\n",
        "        'kartu mahasiswa': 'ktm',\n",
        "        'aktif': 'aktif',\n",
        "        'foto': 'poto',\n",
        "        'pas foto': 'pasfoto',\n",
        "        'perpustakaan': 'perpus',\n",
        "        'bisa jadi anggota': 'bs jd member',\n",
        "        'boleh daftar': 'boleh gabung',\n",
        "        'bawa apa': 'bwa apa',\n",
        "        'tunjukin apa': 'tunjuk apa',\n",
        "        'masuk perpustakaan': 'akses perpus',\n",
        "        'pakai fasilitas': 'gunain fasilitas',\n",
        "        'kampus lain': 'dari luar',\n",
        "        'dari luar': 'bukan bina patria',\n",
        "        'akses fasilitas': 'pakai fasilitas',\n",
        "        'pakai perpus': 'akses perpus',\n",
        "        'baca di tempat': 'bca di tmpt',\n",
        "        'bisa baca': 'bs bca',\n",
        "        'pinjam buku': 'minjem book',\n",
        "        'mahasiswa luar': 'mhs luar',\n",
        "        'harus bawa': 'hrus bwa',\n",
        "    },\n",
        "    # Intent baru untuk fasilitas perpustakaan\n",
        "    'fasilitas': {\n",
        "        'perpustakaan': 'perpus',\n",
        "        'penelusuran': 'cari',\n",
        "        'literatur': 'lit',\n",
        "        'skripsi': 'skrip',\n",
        "        'jurnal': 'jrnl',\n",
        "        'laporan': 'lprn',\n",
        "        'pkl': 'magang',\n",
        "        'fotokopi': 'fotcopy',\n",
        "        'fotocopy': 'fc',\n",
        "        'koleksi': 'koleksi',\n",
        "        'ruang': 'room',\n",
        "        'baca': 'read',\n",
        "        'ruang baca': 'reading room',\n",
        "        'locker': 'loker',\n",
        "        'tas': 'bag',\n",
        "        'jaket': 'jkt',\n",
        "        'penitipan': 'nitip',\n",
        "        'internet': 'inet',\n",
        "        'wifi': 'wi-fi',\n",
        "        'komputer': 'pc',\n",
        "        'printer': 'print',\n",
        "        'scanning': 'scan',\n",
        "        'fasilitas apa saja': 'fasil apa aja',\n",
        "        'apa saja fasilitas': 'apa aja fasil',\n",
        "        'fasilitas yang ada': 'fasil yg ada',\n",
        "        'fasilitas tersedia': 'fasil tersedia',\n",
        "        'ada fasilitas': 'ada fasil',\n",
        "        'punya fasilitas': 'punya fasil',\n",
        "        'bisa pakai': 'bs pake',\n",
        "        'tersedia': 'ada',\n",
        "        'ruangan': 'room',\n",
        "        'diskusi': 'diskus',\n",
        "        'ruang diskusi': 'ruang diskus',\n",
        "        'layanan apa': 'lyn apa',\n",
        "        'apa saja layanan': 'apa aja lyn',\n",
        "        'referensi': 'ref',\n",
        "        'dimana': 'dmn',\n",
        "        'lokasi': 'lokasi',\n",
        "        'tempat': 'tmpt',\n",
        "        'lantai': 'lt',\n",
        "        'multimedia': 'mm',\n",
        "        'penelitian': 'riset',\n",
        "        'riset': 'research',\n",
        "    },\n",
        "     'cara_pinjam': {\n",
        "        'pinjam': 'minjem',\n",
        "        'meminjam': 'minjem',\n",
        "        'buku': 'bku',\n",
        "        'perpustakaan': 'perpus',\n",
        "        'cara': 'gmn',\n",
        "        'bagaimana': 'gmna',\n",
        "        'peminjaman': 'minjem',\n",
        "        'mengambil': 'ngambil',\n",
        "        'pengambilan': 'ngambil',\n",
        "        'prosedur': 'prosedurnya',\n",
        "        'aturan': 'rule',\n",
        "        'sistem': 'sistim',\n",
        "        'ngembaliin': 'balikin',\n",
        "        'kembali': 'balikin',\n",
        "        'pengembalian': 'balikin'\n",
        "    }\n",
        "}\n",
        "\n",
        "# =========[ PHONETIC AUGMENTATION DICTIONARY ]=========\n",
        "phonetic_dict = {\n",
        "        # Greeting related\n",
        "        \"saya\": [\"sy\", \"saia\", \"ane\", \"ana\", \"w\", \"gw\", \"q\", \"aq\"],\n",
        "        \"kamu\": [\"km\", \"kamyu\", \"u\", \"lo\", \"lu\", \"l\", \"ngana\", \"sampeyan\", \"antum\", \"ente\"],\n",
        "        \"halo\": [\"hlo\", \"hallo\", \"helo\", \"haloo\", \"hellow\", \"hy\", \"hyy\", \"p\", \"ping\"],\n",
        "        \"selamat\": [\"slmt\", \"slamat\", \"met\", \"slam\"],\n",
        "        \"pagi\": [\"pgi\", \"morning\", \"pg\", \"subuh\"],\n",
        "        \"siang\": [\"siang\", \"afternoon\", \"siank\", \"siyang\"],\n",
        "        \"malam\": [\"mlm\", \"malem\", \"mlem\", \"night\", \"evening\", \"mlem\", \"mlm\"],\n",
        "        \"apa\": [\"ap\", \"ape\", \"apah\", \"pa\"],\n",
        "        \"kabar\": [\"kbr\", \"kabare\", \"kbar\", \"kabbar\"],\n",
        "        \"gimana\": [\"gmn\", \"bgmn\", \"gmana\", \"gimane\", \"gmn\"],\n",
        "\n",
        "        # Goodbye related\n",
        "        \"sampai\": [\"smp\", \"sampe\", \"smpe\", \"smpei\", \"sampeyan\"],\n",
        "        \"jumpa\": [\"jpa\", \"jmpa\", \"ktmu\", \"jumpe\"],\n",
        "        \"dadah\": [\"byebye\", \"bye\", \"bay\", \"byee\", \"bbye\", \"bye2\", \"dadah\"],\n",
        "        \"pamit\": [\"pmt\", \"pamitt\", \"off\", \"out\", \"cabut\", \"cbut\"],\n",
        "        \"tinggal\": [\"tgl\", \"tnggal\", \"tinggel\", \"tggal\", \"tinggelin\"],\n",
        "        \"duluan\": [\"dlu\", \"duluan\", \"dluan\", \"dluan ya\", \"ahead\"],\n",
        "        \"pergi\": [\"pgi\", \"pegi\", \"prgi\", \"out\", \"keluar\"],\n",
        "        \"pulang\": [\"plg\", \"plng\", \"balik\", \"blk\", \"mudik\"],\n",
        "\n",
        "        # Confirm related\n",
        "        \"ya\": [\"y\", \"yah\", \"iye\", \"yoi\", \"yups\", \"yes\", \"yess\", \"yesss\", \"okey\", \"okeh\", \"oks\"],\n",
        "        \"setuju\": [\"stju\", \"acc\", \"accept\", \"approved\", \"approve\", \"deal\", \"oke\", \"ok\", \"sip\"],\n",
        "        \"benar\": [\"bnr\", \"bner\", \"bener\", \"bnr\", \"yoi\", \"correct\"],\n",
        "        \"sudah\": [\"sdh\", \"dah\", \"udh\", \"done\", \"wes\", \"uwes\", \"udah\", \"sdah\"],\n",
        "        \"bisa\": [\"bs\", \"bsa\", \"biza\", \"bsa\", \"biza\", \"ok\"],\n",
        "        \"pasti\": [\"pst\", \"psti\", \"pastii\", \"sure\", \"certain\"],\n",
        "        \"siap\": [\"sp\", \"ready\", \"sip\", \"sp\", \"roger\", \"on\", \"online\"],\n",
        "        \"jadi\": [\"jd\", \"jdi\", \"jdnya\", \"jdiin\", \"proceed\"],\n",
        "        \"lanjut\": [\"lnjt\", \"lanjt\", \"next\", \"go\"],\n",
        "        \"mantap\": [\"mntap\", \"mantab\", \"mntb\", \"top\", \"mantul\", \"josss\", \"kerennn\"],\n",
        "        \"bagus\": [\"bgs\", \"bgus\", \"nice\", \"naiss\", \"keren\", \"top\"],\n",
        "\n",
        "        # Denied related\n",
        "        \"tidak\": [\"tdk\", \"gak\", \"ga\", \"g\", \"nggak\", \"ngga\", \"nope\", \"no\", \"kagak\", \"kaga\", \"kgk\"],\n",
        "        \"jangan\": [\"jgn\", \"jngn\", \"don't\", \"dont\", \"jgn\", \"ga usah\", \"tdk usah\", \"gausa\", \"gausah\"],\n",
        "        \"belum\": [\"blm\", \"blom\", \"belom\", \"not yet\", \"durung\", \"durong\", \"belm\"],\n",
        "        \"batal\": [\"btl\", \"cancel\", \"cansel\", \"urungkan\", \"batalin\", \"gajadi\"],\n",
        "        \"maaf\": [\"sorry\", \"sori\", \"maf\", \"maap\", \"maaaaf\", \"mrff\", \"sry\", \"srry\"],\n",
        "        \"menolak\": [\"tlk\", \"reject\", \"decline\", \"dtolak\", \"nolak\", \"gak mau\", \"gamau\"],\n",
        "        \"mustahil\": [\"impossible\", \"ga mungkin\", \"g mungkin\", \"tdk mungkin\", \"gak bs\"],\n",
        "        \"salah\": [\"slh\", \"wrong\", \"error\", \"eror\", \"salh\", \"fail\"],\n",
        "        \"gagal\": [\"ggl\", \"fail\", \"failed\", \"error\", \"gagak\", \"failll\"],\n",
        "\n",
        "        # Common words\n",
        "        \"terima\": [\"trma\", \"thanks\", \"thx\", \"trims\", \"tq\", \"tyvm\", \"makasih\", \"mksih\"],\n",
        "        \"kasih\": [\"ksh\", \"ksih\", \"thx\", \"makasih\", \"mksih\", \"thanks\"],\n",
        "        \"tolong\": [\"tlng\", \"help\", \"tlg\", \"tulung\", \"bantu\", \"bantuin\"],\n",
        "        \"please\": [\"plz\", \"plis\", \"pliss\", \"plisss\", \"pliissss\", \"tolong\"],\n",
        "        \"besok\": [\"bsk\", \"bsok\", \"besok\", \"tmrw\", \"esok\", \"besuk\"],\n",
        "        \"waktu\": [\"wkt\", \"waktu\", \"time\", \"tm\", \"jam\"],\n",
        "        \"cukup\": [\"ckp\", \"enough\", \"cukuppp\", \"cukups\", \"ckup\"],\n",
        "        \"melihat\": [\"lihat\", \"liat\", \"look\", \"see\", \"watching\", \"ngeliat\"],\n",
        "        \"alasan\": [\"alsan\", \"reason\", \"why\", \"alesan\", \"alsn\"],\n",
        "        \"untuk\": [\"utk\", \"buat\", \"bwt\", \"4\", \"tuk\", \"2\", \"to\"],\n",
        "        \"melakukan\": [\"lakukan\", \"do\", \"lakuin\", \"melakuin\", \"ngerjain\"],\n",
        "        \"ini\": [\"ni\", \"this\", \"these\", \"iki\", \"nih\", \"ne\"],\n",
        "        \"kita\": [\"kta\", \"we\", \"us\", \"w\", \"kt\"],\n",
        "        \"hari\": [\"hr\", \"day\", \"hri\", \"days\", \"dayy\", \"harii\"],\n",
        "        \"bro\": [\"broh\", \"brow\", \"brother\", \"mas\", \"bang\", \"bor\", \"omm\"],\n",
        "        \"sis\": [\"sist\", \"sister\", \"mbak\", \"mba\", \"nte\", \"ceu\", \"teh\"],\n",
        "\n",
        "        # Tambahan kata untuk intent fasilitas\n",
        "        \"fasilitas\": [\"fasil\", \"fasilit\", \"fasl\", \"fasility\", \"facility\", \"facilities\"],\n",
        "        \"penelusuran\": [\"cari\", \"telusur\", \"search\", \"browsing\", \"browse\"],\n",
        "        \"literatur\": [\"lit\", \"literature\", \"bacaan\", \"bahan\", \"referensi\", \"ref\"],\n",
        "        \"skripsi\": [\"skrip\", \"thesis\", \"tugas akhir\", \"ta\", \"karya ilmiah\"],\n",
        "        \"jurnal\": [\"jrnl\", \"journal\", \"paper\", \"artikel ilmiah\", \"artikel\"],\n",
        "        \"laporan\": [\"lprn\", \"report\", \"repot\", \"lapor\", \"dokumen\"],\n",
        "        \"fotokopi\": [\"fc\", \"fotcopy\", \"fotocopy\", \"copy\", \"foto copy\", \"duplikat\"],\n",
        "        \"koleksi\": [\"koleksi\", \"collection\", \"bahan\", \"buku\", \"dokumen\"],\n",
        "        \"ruang\": [\"ruangan\", \"room\", \"space\", \"tempat\", \"area\"],\n",
        "        \"baca\": [\"reading\", \"bca\", \"membaca\", \"belajar\", \"studi\"],\n",
        "        \"locker\": [\"loker\", \"lemari\", \"cabinet\", \"penyimpanan\", \"storage\"],\n",
        "        \"internet\": [\"inet\", \"net\", \"web\", \"online\", \"jaringan\", \"wifi\", \"wi-fi\"],\n",
        "\n",
        "        #pinjam buku\n",
        "        \"pinjam\": [\"minjem\", \"mimjem\", \"mnjm\", \"pinjam\", \"meminjam\", \"nyewa\"],\n",
        "        \"buku\": [\"bku\", \"buk\", \"book\", \"books\", \"bukuu\", \"bkoo\"],\n",
        "        \"perpustakaan\": [\"perpus\", \"library\", \"lib\", \"pustaka\", \"perpust\"],\n",
        "        \"pengembalian\": [\"balikin\", \"ngembaliin\", \"ngembalikan\", \"kembaliin\", \"return\"],\n",
        "        \"mengembalikan\": [\"balikin\", \"ngembaliin\", \"kembali\", \"ngembali\"],\n",
        "        \"mengambil\": [\"ngambil\", \"ngmbl\", \"ambil\", \"mengmbil\"],\n",
        "        \"pengambilan\": [\"ngambil\", \"pengmbilan\", \"ambil\", \"pick up\"],\n",
        "        \"aturan\": [\"rule\", \"peraturan\", \"aturan2\", \"aturan-aturan\"],\n",
        "        \"prosedur\": [\"prosedurnya\", \"langkah\", \"step\", \"alur\", \"tahapan\"],\n",
        "        \"cara\": [\"gmn\", \"gmna\", \"gimana\", \"cmna\", \"how\", \"caranya\"],\n",
        "        \"sistem\": [\"sistim\", \"system\", \"sisem\", \"systm\"]\n",
        "    }\n",
        "\n",
        "# =========[ PROTECTED WORDS BY INTENT ]=========\n",
        "# Words that should not be altered for each intent to preserve meaning\n",
        "protected_intent_words = {\n",
        "    'jam_layanan': ['bipa', 'bina patria'],\n",
        "    'cari_buku': ['judul', 'penulis', 'isbn', 'kategori', 'bipa', 'bina patria'],\n",
        "    'greeting': ['anjing'],\n",
        "    'goodbye': ['anjing'],\n",
        "    'confirm': ['anjing'],\n",
        "    'denied': ['anjing'],\n",
        "    'keanggotaan': [\n",
        "         'kartu mahasiswa', 'bipa', 'bina patria'\n",
        "    ],\n",
        "    # Kata yang dilindungi untuk intent fasilitas\n",
        "    'fasilitas': [\n",
        "        'skripsi', 'laporan', 'pkl', 'locker', 'ruang diskusi', 'referensi', 'bipa', 'bina patria'\n",
        "    ],\n",
        "    'cara_pinjam': ['bipa', 'bina patria']\n",
        "}\n",
        "\n",
        "# You can also save all dictionaries in a single JSON file if preferred\n",
        "all_dictionaries = {\n",
        "    'id_synonyms': id_synonyms,\n",
        "    'common_slang': common_slang,\n",
        "    'intent_slang': intent_slang,\n",
        "    'phonetic_dict': phonetic_dict,\n",
        "    'protected_intent_words': protected_intent_words\n",
        "}\n",
        "save_to_json(all_dictionaries, 'kamus.json')"
      ],
      "metadata": {
        "id": "vkHp5oDSV9eg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}