{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HaqTetsuya/rusdi-prototype-1/blob/main/rusdi_prototype_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baq_vbCGocRh",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title download dependancy\n",
        "# Cell 1: Instalasi library yang diperlukan\n",
        "!pip install transformers torch pandas scikit-learn matplotlib seaborn tqdm deep-translator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "-8eolkt8hn4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20a8317f-42c7-4bae-8923-4fd5e90c6c8a",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'rusdi-prototype-1' already exists and is not an empty directory.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# @title import dependency, load drive, and github {\"form-width\":\"20%\"}\n",
        "!git clone https://github.com/HaqTetsuya/rusdi-prototype-1.git\n",
        "\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import seaborn as sns\n",
        "from google.colab import drive, files\n",
        "from plotly.subplots import make_subplots\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
        "#from tqdm import tqdm\n",
        "from tqdm.auto import tqdm  # If you need both tqdm and tqdm.auto\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "FName = \"indobert_intent_model2\" #@param {type:\"string\"}\n",
        "\n",
        "# Update MODEL_SAVE_PATH with user input\n",
        "MODEL_SAVE_PATH = f\"/content/drive/MyDrive/{FName}\"\n",
        "\n",
        "\n",
        "# Cell 4: Kelas Dataset untuk IndoBERT\n",
        "class IntentDataset(Dataset):\n",
        "    \"\"\"Dataset untuk klasifikasi intent dengan IndoBERT\"\"\"\n",
        "\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Convert dict of tensors to flat tensors\n",
        "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
        "        item['labels'] = torch.tensor(label)\n",
        "\n",
        "        return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "qqQ8ahtVzC6r",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title load data\n",
        "\n",
        "def load_csv_data(csv_path, label_encoder=None, show_distribution=False):\n",
        "    \"\"\"Memuat data intent dari file CSV. Bisa untuk train/test tanpa split.\"\"\"\n",
        "    print(f\"\\nMemuat data dari: {csv_path}\")\n",
        "\n",
        "    if not os.path.exists(csv_path):\n",
        "        raise FileNotFoundError(f\"File tidak ditemukan: {csv_path}\")\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    if 'text' not in df.columns or 'intent' not in df.columns:\n",
        "        raise ValueError(\"Kolom 'text' dan 'intent' harus ada di CSV\")\n",
        "\n",
        "    if label_encoder is None:\n",
        "        label_encoder = LabelEncoder()\n",
        "        df['intent_encoded'] = label_encoder.fit_transform(df['intent'])\n",
        "        intent_classes = label_encoder.classes_\n",
        "        print(f\"Label encoder baru dibuat dari data {csv_path}\")\n",
        "    else:\n",
        "        df['intent_encoded'] = label_encoder.transform(df['intent'])\n",
        "        intent_classes = label_encoder.classes_\n",
        "        print(f\"Menggunakan label encoder yang sudah ada\")\n",
        "\n",
        "    if show_distribution:\n",
        "        intent_counts = df['intent'].value_counts()\n",
        "        print(\"\\nDistribusi intent:\")\n",
        "        for intent, count in intent_counts.items():\n",
        "            print(f\"  {intent}: {count}\")\n",
        "\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        sns.barplot(x=intent_counts.index, y=intent_counts.values, palette=\"viridis\")\n",
        "        plt.xlabel(\"Intent\")\n",
        "        plt.ylabel(\"Jumlah Sampel\")\n",
        "        plt.title(\"Distribusi Intent\")\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.pie(intent_counts, labels=intent_counts.index, autopct='%1.1f%%', colors=sns.color_palette(\"viridis\", len(intent_counts)))\n",
        "        plt.title(\"Proporsi Intent\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    texts = df['text'].values\n",
        "    labels = df['intent_encoded'].values\n",
        "\n",
        "    return texts, labels, intent_classes, label_encoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "cellView": "form",
        "id": "VMP8y1EczkWo"
      },
      "outputs": [],
      "source": [
        "# @title  setup model IndoBERT\n",
        "def setup_indobert_for_intent(num_labels):\n",
        "    \"\"\"Load model IndoBERT untuk klasifikasi intent\"\"\"\n",
        "\n",
        "    print(\"Memuat model IndoBERT...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        \"indobenchmark/indobert-base-p1\",\n",
        "        num_labels=num_labels\n",
        "    )\n",
        "    print(\"Model berhasil dimuat\")\n",
        "\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title kalibrasi OOD\n",
        "def calibrate_ood_detection(model, tokenizer, dataloader, temperature=1.0, percentile=85):\n",
        "    \"\"\"\n",
        "    Kalibrasi threshold untuk OOD detection menggunakan data in-distribution\n",
        "    dengan metode Energy-based dan MSP (Maximum Softmax Probability)\n",
        "    \"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "\n",
        "    # Untuk Energy method dan MSP method\n",
        "    energy_scores = []\n",
        "    msp_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Kalibrasi OOD detection\"):\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            # Energy score (nilai lebih tinggi untuk OOD)\n",
        "            energy = -temperature * torch.logsumexp(logits / temperature, dim=1)\n",
        "            energy_scores.extend(energy.cpu().numpy())\n",
        "\n",
        "            # MSP score (nilai lebih rendah untuk OOD)\n",
        "            softmax_probs = F.softmax(logits, dim=1)\n",
        "            max_probs, _ = torch.max(softmax_probs, dim=1)\n",
        "            msp_scores.extend(max_probs.cpu().numpy())\n",
        "\n",
        "    # Hitung threshold untuk Energy (maksimal energy untuk in-distribution)\n",
        "    energy_threshold = np.percentile(energy_scores, percentile)\n",
        "\n",
        "    # Hitung threshold untuk MSP (minimal probability untuk in-distribution)\n",
        "    msp_threshold = np.percentile(msp_scores, 100 - percentile)  # Inverse percentile karena higher = better\n",
        "\n",
        "    return {\n",
        "        \"energy_threshold\": float(energy_threshold),\n",
        "        \"msp_threshold\": float(msp_threshold)\n",
        "    }\n",
        "def save_ood_thresholds(thresholds, save_path):\n",
        "    \"\"\"\n",
        "    Menyimpan threshold OOD ke file JSON\n",
        "    \"\"\"\n",
        "    threshold_file = os.path.join(save_path, \"ood_thresholds.json\")\n",
        "    with open(threshold_file, 'w') as f:\n",
        "        json.dump(thresholds, f, indent=4)\n",
        "    print(f\"OOD thresholds disimpan di {threshold_file}\")\n",
        "    return threshold_file\n",
        "\n",
        "def load_ood_thresholds(model_path):\n",
        "    \"\"\"\n",
        "    Memuat threshold OOD dari file JSON\n",
        "    \"\"\"\n",
        "    # Coba load thresholds.json terlebih dahulu (format baru)\n",
        "    try:\n",
        "        with open(os.path.join(model_path, \"ood_thresholds.json\"), 'r') as f:\n",
        "            thresholds = json.load(f)\n",
        "            return thresholds\n",
        "    except FileNotFoundError:\n",
        "        # Jika tidak ditemukan, coba load format lama\n",
        "        try:\n",
        "            with open(os.path.join(model_path, \"ood_threshold.json\"), 'r') as f:\n",
        "                threshold_data = json.load(f)\n",
        "                return {\n",
        "                    \"energy_threshold\": threshold_data[\"energy_threshold\"],\n",
        "                    \"msp_threshold\": None  # Tidak ada dalam format lama\n",
        "                }\n",
        "        except FileNotFoundError:\n",
        "            print(\"Warning: OOD threshold files not found. Using default thresholds.\")\n",
        "            return {\n",
        "                \"energy_threshold\": 0.0,  # Default fallback value\n",
        "                \"msp_threshold\": 0.5  # Default fallback value\n",
        "            }"
      ],
      "metadata": {
        "id": "APTL6Ht96u9Q",
        "cellView": "form"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "fNCPJ3uFzp2V",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Training function\n",
        "\n",
        "def train_intent_classifier(model, tokenizer, train_texts, train_labels, val_texts, val_labels,\n",
        "                           batch_size=16, epochs=10, learning_rate=2e-5, weight_decay=0.01,\n",
        "                           save_path=MODEL_SAVE_PATH, use_class_weights=True, patience=3, class_names=None):\n",
        "    \"\"\"\n",
        "    Melatih model IndoBERT untuk klasifikasi intent dengan perbaikan:\n",
        "    - Enhanced visualization (interactive and static)\n",
        "    - Per-class metric tracking\n",
        "    - Confusion matrix generation\n",
        "    - Batch-level metric tracking\n",
        "    - Learning rate visualization\n",
        "    \"\"\"\n",
        "\n",
        "    # Persiapkan dataset\n",
        "    print(\"Menyiapkan dataset...\")\n",
        "    train_dataset = IntentDataset(train_texts, train_labels, tokenizer)\n",
        "    val_dataset = IntentDataset(val_texts, val_labels, tokenizer)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Compute class weights if needed\n",
        "    if use_class_weights:\n",
        "        # Get unique classes\n",
        "        unique_classes = np.unique(train_labels)\n",
        "        # Compute weights\n",
        "        weights = compute_class_weight(\n",
        "            class_weight='balanced',\n",
        "            classes=unique_classes,\n",
        "            y=train_labels\n",
        "        )\n",
        "        # Convert to tensor\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        class_weights = torch.FloatTensor(weights).to(device)\n",
        "        print(f\"Menggunakan class weights: {weights}\")\n",
        "    else:\n",
        "        class_weights = None\n",
        "\n",
        "    # Optimizer dengan weight decay untuk regularisasi\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    # Scheduler dengan warmup\n",
        "    num_training_steps = len(train_dataloader) * epochs\n",
        "    num_warmup_steps = int(0.1 * num_training_steps)  # 10% warmup\n",
        "    scheduler = get_scheduler(\"cosine\", optimizer=optimizer,\n",
        "                             num_warmup_steps=num_warmup_steps,\n",
        "                             num_training_steps=num_training_steps)\n",
        "\n",
        "    # Cek untuk GPU\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Menggunakan device: {device}\")\n",
        "    model.to(device)\n",
        "    print(f\"Mulai pelatihan model...\")\n",
        "    print(f\"Total epoch: {epochs}, batch size: {batch_size}, learning rate: {learning_rate}, weight decay: {weight_decay}\")\n",
        "\n",
        "    # Create loss function with class weights if needed\n",
        "    if class_weights is not None:\n",
        "        loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "    else:\n",
        "        loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Enhanced logging training with batch-level tracking\n",
        "    best_val_loss = float('inf')\n",
        "    counter = 0  # Counter for early stopping\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'val_accuracy': [],\n",
        "        'val_f1': [],\n",
        "        'val_precision': [],\n",
        "        'val_recall': [],\n",
        "        'batch_metrics': {\n",
        "            'iteration': [],\n",
        "            'loss': [],\n",
        "            'epoch': [],\n",
        "            'progress': [],\n",
        "            'learning_rates': []\n",
        "        },\n",
        "        'class_f1': [],  # Per-class F1 scores\n",
        "        'class_precision': [],  # Per-class precision\n",
        "        'class_recall': []  # Per-class recall\n",
        "    }\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        tqdm.write(f\"\\nEpoch {epoch+1}/{epochs} - Training dimulai...\")\n",
        "        progress_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader),\n",
        "                         desc=f\"Epoch {epoch+1}/{epochs} [Training]\", leave=False)\n",
        "\n",
        "        for batch_idx, batch in progress_bar:\n",
        "            try:\n",
        "                # Pindahkan batch ke device\n",
        "                inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "                # Forward pass with custom loss function\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Modified to use our loss function instead of the model's default\n",
        "                outputs = model(**inputs)\n",
        "                logits = outputs.logits\n",
        "                loss = loss_fn(logits, labels)\n",
        "\n",
        "                # Backward pass\n",
        "                loss.backward()\n",
        "\n",
        "                # Gradient clipping to prevent exploding gradients\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "                # Track batch-level metrics\n",
        "                iterations_per_epoch = len(train_dataloader)\n",
        "                global_iteration = epoch * iterations_per_epoch + batch_idx\n",
        "                progress = (epoch + (batch_idx / iterations_per_epoch)) * 100\n",
        "\n",
        "                history['batch_metrics']['iteration'].append(global_iteration)\n",
        "                history['batch_metrics']['loss'].append(loss.item())\n",
        "                history['batch_metrics']['epoch'].append(epoch)\n",
        "                history['batch_metrics']['progress'].append(progress)\n",
        "                history['batch_metrics']['learning_rates'].append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "                train_loss += loss.item()\n",
        "                progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                if \"out of memory\" in str(e):\n",
        "                    tqdm.write(\"Peringatan: Kehabisan memori! Membersihkan cache...\")\n",
        "                    torch.cuda.empty_cache()\n",
        "                    continue\n",
        "                else:\n",
        "                    raise e\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_dataloader)\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        tqdm.write(f\"Epoch {epoch+1}/{epochs} - Validasi dimulai...\")\n",
        "        progress_bar = tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Validation]\", leave=False)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in progress_bar:\n",
        "                inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "                outputs = model(**inputs)\n",
        "                logits = outputs.logits\n",
        "                loss = loss_fn(logits, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Hitung akurasi\n",
        "                predictions = torch.argmax(logits, dim=1)\n",
        "                correct += (predictions == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "                # Collect predictions and labels for metrics\n",
        "                all_preds.extend(predictions.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "                progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
        "\n",
        "        # Calculate overall metrics\n",
        "        avg_val_loss = val_loss / len(val_dataloader)\n",
        "        accuracy = correct / total\n",
        "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "        precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "        recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "\n",
        "        # Calculate per-class metrics\n",
        "        class_f1 = f1_score(all_labels, all_preds, average=None, zero_division=0)\n",
        "        class_precision = precision_score(all_labels, all_preds, average=None, zero_division=0)\n",
        "        class_recall = recall_score(all_labels, all_preds, average=None, zero_division=0)\n",
        "\n",
        "        # Save metrics to history\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "        history['val_accuracy'].append(accuracy)\n",
        "        history['val_f1'].append(f1)\n",
        "        history['val_precision'].append(precision)\n",
        "        history['val_recall'].append(recall)\n",
        "\n",
        "        # Save per-class metrics\n",
        "        history['class_f1'].append(class_f1.tolist())\n",
        "        history['class_precision'].append(class_precision.tolist())\n",
        "        history['class_recall'].append(class_recall.tolist())\n",
        "\n",
        "        # Generate confusion matrix for this epoch\n",
        "        if class_names is not None:\n",
        "            plot_confusion_matrix(all_labels, all_preds, class_names, epoch, save_path)\n",
        "\n",
        "        # Print detailed metrics\n",
        "        tqdm.write(f\"Epoch {epoch+1}/{epochs}:\")\n",
        "        tqdm.write(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
        "        tqdm.write(f\"  Val Loss: {avg_val_loss:.4f}, Val Accuracy: {accuracy*100:.2f}%\")\n",
        "        tqdm.write(f\"  Val F1: {f1:.4f}, Val Precision: {precision:.4f}, Val Recall: {recall:.4f}\")\n",
        "\n",
        "        # Early stopping and model saving logic\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            counter = 0  # Reset early stopping counter\n",
        "\n",
        "            if not os.path.exists(save_path):\n",
        "                os.makedirs(save_path)\n",
        "\n",
        "            tqdm.write(f\"Menyimpan model terbaik ke {save_path}\")\n",
        "            model.save_pretrained(save_path)\n",
        "            tokenizer.save_pretrained(save_path)\n",
        "\n",
        "            # Save classification report for best model\n",
        "            report = classification_report(all_labels, all_preds, output_dict=True)\n",
        "            with open(os.path.join(save_path, \"classification_report.json\"), 'w') as f:\n",
        "                json.dump(report, f, indent=4)\n",
        "        else:\n",
        "            counter += 1\n",
        "            tqdm.write(f\"Validation loss tidak membaik. Early stopping counter: {counter}/{patience}\")\n",
        "\n",
        "            if counter >= patience:\n",
        "                tqdm.write(f\"Early stopping triggered setelah {epoch+1} epochs\")\n",
        "                break\n",
        "\n",
        "    tqdm.write(f\"Pelatihan selesai! Model terbaik disimpan di {save_path}\")\n",
        "\n",
        "    # Generate enhanced visualizations\n",
        "    enhanced_plot_training_results(history, save_path, class_names=class_names)\n",
        "\n",
        "    # Simpan history ke file JSON\n",
        "    save_enhanced_history(history, save_path)\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def enhanced_plot_training_results(history, save_path, class_names=None):\n",
        "    \"\"\"\n",
        "    Enhanced function to plot training results with more detailed visualizations\n",
        "\n",
        "    Args:\n",
        "        history: Dictionary containing training history metrics\n",
        "        save_path: Path to save visualization files\n",
        "        class_names: Optional list of class names for confusion matrix\n",
        "    \"\"\"\n",
        "    # Create the static plots (same as before for compatibility)\n",
        "    static_plot_training_results(history, save_path)\n",
        "\n",
        "    # Create interactive plotly visualizations\n",
        "    interactive_plot_training_results(history, save_path)\n",
        "\n",
        "    # If we have class metrics in our history, plot those too\n",
        "    if 'class_f1' in history and class_names is not None:\n",
        "        plot_class_metrics(history, save_path, class_names)\n",
        "\n",
        "    # If we tracked learning rates, plot those\n",
        "    if 'learning_rates' in history:\n",
        "        plot_learning_rate(history, save_path)\n",
        "\n",
        "def static_plot_training_results(history, save_path):\n",
        "    \"\"\"Plot and save training metrics using matplotlib (static)\"\"\"\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Plot 1: Loss\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(history['train_loss'], label='Training Loss', marker='o')\n",
        "    plt.plot(history['val_loss'], label='Validation Loss', marker='o')\n",
        "    plt.title('Loss selama Training')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot 2: Accuracy\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(history['val_accuracy'], label='Validation Accuracy', marker='o', color='green')\n",
        "    plt.title('Akurasi selama Training')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot 3: F1 Score\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(history['val_f1'], label='Validation F1', marker='o', color='purple')\n",
        "    plt.title('F1 Score selama Training')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('F1 Score')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot 4: Precision & Recall\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(history['val_precision'], label='Validation Precision', marker='o', color='orange')\n",
        "    plt.plot(history['val_recall'], label='Validation Recall', marker='o', color='brown')\n",
        "    plt.title('Precision & Recall selama Training')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the plot\n",
        "    plt.savefig(os.path.join(save_path, \"training_metrics.png\"))\n",
        "    plt.close()\n",
        "\n",
        "def interactive_plot_training_results(history, save_path):\n",
        "    \"\"\"Create interactive plotly visualizations of training metrics\"\"\"\n",
        "    # Create epochs list for x-axis\n",
        "    epochs = list(range(1, len(history['train_loss']) + 1))\n",
        "\n",
        "    # Create a DataFrame for easier plotting\n",
        "    df = pd.DataFrame({\n",
        "        'Epoch': epochs,\n",
        "        'Training Loss': history['train_loss'],\n",
        "        'Validation Loss': history['val_loss'],\n",
        "        'Validation Accuracy': history['val_accuracy'],\n",
        "        'Validation F1': history['val_f1'],\n",
        "        'Validation Precision': history['val_precision'],\n",
        "        'Validation Recall': history['val_recall']\n",
        "    })\n",
        "\n",
        "    # Create subplot figure\n",
        "    fig = make_subplots(\n",
        "        rows=2, cols=2,\n",
        "        subplot_titles=('Loss', 'Accuracy', 'F1 Score', 'Precision & Recall'),\n",
        "        vertical_spacing=0.15,\n",
        "        horizontal_spacing=0.1\n",
        "    )\n",
        "\n",
        "    # Add traces for each metric\n",
        "    # Loss plot\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=epochs, y=history['train_loss'], mode='lines+markers', name='Training Loss'),\n",
        "        row=1, col=1\n",
        "    )\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=epochs, y=history['val_loss'], mode='lines+markers', name='Validation Loss'),\n",
        "        row=1, col=1\n",
        "    )\n",
        "\n",
        "    # Accuracy plot\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=epochs, y=history['val_accuracy'], mode='lines+markers', name='Validation Accuracy', line=dict(color='green')),\n",
        "        row=1, col=2\n",
        "    )\n",
        "\n",
        "    # F1 Score plot\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=epochs, y=history['val_f1'], mode='lines+markers', name='Validation F1', line=dict(color='purple')),\n",
        "        row=2, col=1\n",
        "    )\n",
        "\n",
        "    # Precision & Recall plot\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=epochs, y=history['val_precision'], mode='lines+markers', name='Validation Precision', line=dict(color='orange')),\n",
        "        row=2, col=2\n",
        "    )\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=epochs, y=history['val_recall'], mode='lines+markers', name='Validation Recall', line=dict(color='brown')),\n",
        "        row=2, col=2\n",
        "    )\n",
        "\n",
        "    # Update layout\n",
        "    fig.update_layout(\n",
        "        height=800,\n",
        "        width=1200,\n",
        "        title_text=\"Training Metrics (Interactive)\",\n",
        "        hovermode=\"x unified\"\n",
        "    )\n",
        "\n",
        "    # Save interactive plot as HTML\n",
        "    fig.write_html(os.path.join(save_path, \"interactive_training_metrics.html\"))\n",
        "\n",
        "    # Create a combined metrics plot for better trend comparison\n",
        "    fig_combined = px.line(\n",
        "        df,\n",
        "        x='Epoch',\n",
        "        y=['Training Loss', 'Validation Loss', 'Validation Accuracy', 'Validation F1', 'Validation Precision', 'Validation Recall'],\n",
        "        title='All Training Metrics',\n",
        "        labels={'value': 'Metric Value', 'variable': 'Metric'}\n",
        "    )\n",
        "\n",
        "    fig_combined.update_layout(height=600, width=1000, hovermode=\"x unified\")\n",
        "    fig_combined.write_html(os.path.join(save_path, \"combined_metrics.html\"))\n",
        "\n",
        "def plot_confusion_matrix(all_labels, all_preds, class_names, epoch, save_path):\n",
        "    \"\"\"Plot and save confusion matrix for the epoch\"\"\"\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt='d',\n",
        "        cmap='Blues',\n",
        "        xticklabels=class_names,\n",
        "        yticklabels=class_names\n",
        "    )\n",
        "    plt.title(f'Confusion Matrix - Epoch {epoch+1}')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the confusion matrix\n",
        "    cm_dir = os.path.join(save_path, \"confusion_matrices\")\n",
        "    os.makedirs(cm_dir, exist_ok=True)\n",
        "    plt.savefig(os.path.join(cm_dir, f\"cm_epoch_{epoch+1}.png\"))\n",
        "    plt.close()\n",
        "\n",
        "def plot_class_metrics(history, save_path, class_names):\n",
        "    \"\"\"Plot per-class performance metrics\"\"\"\n",
        "    # Create directory for class metrics\n",
        "    os.makedirs(os.path.join(save_path, \"class_metrics\"), exist_ok=True)\n",
        "\n",
        "    # Plot F1 per class if available\n",
        "    if 'class_f1' in history:\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        # Convert dictionary structure to usable format\n",
        "        epochs = len(history['class_f1'])\n",
        "        x_epochs = list(range(1, epochs + 1))\n",
        "\n",
        "        for i, class_name in enumerate(class_names):\n",
        "            class_f1 = [history['class_f1'][epoch][i] for epoch in range(epochs)]\n",
        "            plt.plot(x_epochs, class_f1, marker='o', label=f'{class_name}')\n",
        "\n",
        "        plt.title('F1 Score per Class')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('F1 Score')\n",
        "        plt.grid(True)\n",
        "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(save_path, \"class_metrics\", \"f1_per_class.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        # Interactive version with plotly\n",
        "        fig = go.Figure()\n",
        "        for i, class_name in enumerate(class_names):\n",
        "            class_f1 = [history['class_f1'][epoch][i] for epoch in range(epochs)]\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=x_epochs,\n",
        "                y=class_f1,\n",
        "                mode='lines+markers',\n",
        "                name=class_name\n",
        "            ))\n",
        "\n",
        "        fig.update_layout(\n",
        "            title='F1 Score per Class (Interactive)',\n",
        "            xaxis_title='Epoch',\n",
        "            yaxis_title='F1 Score',\n",
        "            height=600,\n",
        "            width=1000,\n",
        "            hovermode=\"x unified\"\n",
        "        )\n",
        "        fig.write_html(os.path.join(save_path, \"class_metrics\", \"f1_per_class.html\"))\n",
        "\n",
        "def plot_learning_rate(history, save_path):\n",
        "    \"\"\"Plot learning rate changes over training\"\"\"\n",
        "    epochs = len(history['learning_rates'][0])\n",
        "    steps_per_epoch = len(history['learning_rates'])\n",
        "\n",
        "    # Flatten the learning rates\n",
        "    all_steps = []\n",
        "    all_lrs = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for step in range(steps_per_epoch):\n",
        "            all_steps.append(epoch + step/steps_per_epoch)\n",
        "            all_lrs.append(history['learning_rates'][step][epoch])\n",
        "\n",
        "    # Create the plot\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(all_steps, all_lrs)\n",
        "    plt.title('Learning Rate Schedule')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_path, \"learning_rate_schedule.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    # Interactive version\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=all_steps,\n",
        "        y=all_lrs,\n",
        "        mode='lines',\n",
        "        name='Learning Rate'\n",
        "    ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title='Learning Rate Schedule (Interactive)',\n",
        "        xaxis_title='Epoch',\n",
        "        yaxis_title='Learning Rate',\n",
        "        height=500,\n",
        "        width=900\n",
        "    )\n",
        "    fig.write_html(os.path.join(save_path, \"learning_rate_schedule.html\"))\n",
        "\n",
        "def save_enhanced_history(history, save_path):\n",
        "    \"\"\"Save enhanced training history with additional visualizations\"\"\"\n",
        "    # Convert numpy arrays to lists for JSON serialization\n",
        "    for key in history:\n",
        "        if isinstance(history[key], np.ndarray):\n",
        "            history[key] = history[key].tolist()\n",
        "        elif isinstance(history[key], list):\n",
        "            # Handle nested numpy arrays\n",
        "            if history[key] and isinstance(history[key][0], np.ndarray):\n",
        "                history[key] = [item.tolist() if isinstance(item, np.ndarray) else item for item in history[key]]\n",
        "\n",
        "    # Save the enhanced history\n",
        "    with open(os.path.join(save_path, \"enhanced_training_history.json\"), 'w') as f:\n",
        "        json.dump(history, f, indent=4)\n",
        "\n",
        "    print(f\"Enhanced training history saved to {os.path.join(save_path, 'enhanced_training_history.json')}\")\n",
        "\n",
        "def enhanced_calibrate_ood(model, tokenizer, val_dataloader, save_path, temperature=1.0, percentile=85):\n",
        "    \"\"\"\n",
        "    Kalibrasi dan simpan threshold OOD yang ditingkatkan\n",
        "    \"\"\"\n",
        "    print(\"Kalibrasi threshold untuk OOD detection...\")\n",
        "    thresholds = calibrate_ood_detection(model, tokenizer, val_dataloader,\n",
        "                                        temperature=temperature,\n",
        "                                        percentile=percentile)\n",
        "\n",
        "    print(f\"Energy threshold: {thresholds['energy_threshold']:.4f}\")\n",
        "    print(f\"MSP threshold: {thresholds['msp_threshold']:.4f}\")\n",
        "\n",
        "    # Save thresholds\n",
        "    save_ood_thresholds(thresholds, save_path)\n",
        "\n",
        "    return thresholds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "pWFt6WSazwgT",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Evaluasi model\n",
        "def evaluate_model_enhanced(model, tokenizer, val_texts, val_labels, intent_classes, save_path):\n",
        "    \"\"\"Enhanced model evaluation with better visualizations\"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    # Prepare dataset and dataloader\n",
        "    val_dataset = IntentDataset(val_texts, val_labels, tokenizer)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Evaluation loop\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_dataloader, desc=\"Evaluasi Model\"):\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            predictions = torch.argmax(logits, dim=1)\n",
        "\n",
        "            all_preds.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Generate and save confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "               xticklabels=intent_classes, yticklabels=intent_classes)\n",
        "    plt.title('Confusion Matrix - Final Model')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_path, \"final_confusion_matrix.png\"))\n",
        "\n",
        "    # Interactive confusion matrix with plotly\n",
        "    fig = px.imshow(cm,\n",
        "                   labels=dict(x=\"Predicted Label\", y=\"True Label\", color=\"Count\"),\n",
        "                   x=intent_classes, y=intent_classes,\n",
        "                   text_auto=True,\n",
        "                   color_continuous_scale='Blues')\n",
        "\n",
        "    fig.update_layout(\n",
        "        title='Confusion Matrix (Interactive)',\n",
        "        width=900,\n",
        "        height=800\n",
        "    )\n",
        "\n",
        "    fig.write_html(os.path.join(save_path, \"interactive_confusion_matrix.html\"))\n",
        "\n",
        "    # Generate classification report\n",
        "    report = classification_report(all_labels, all_preds,\n",
        "                                 target_names=intent_classes,\n",
        "                                 output_dict=True)\n",
        "\n",
        "    # Create a visual representation of the classification report\n",
        "    report_df = pd.DataFrame(report).transpose()\n",
        "    report_df = report_df.round(3)\n",
        "\n",
        "    # Save classification metrics as CSV\n",
        "    report_df.to_csv(os.path.join(save_path, \"classification_report.csv\"))\n",
        "\n",
        "    # Create visualization of the classification report\n",
        "    plt.figure(figsize=(12, len(intent_classes)*0.5 + 3))\n",
        "    metrics = ['precision', 'recall', 'f1-score']\n",
        "\n",
        "    # Filter out summary rows and keep only per-class metrics\n",
        "    class_df = report_df.loc[intent_classes]\n",
        "\n",
        "    sns.heatmap(class_df[metrics], annot=True, cmap='YlGnBu', fmt='.3f',\n",
        "               yticklabels=intent_classes, cbar=True)\n",
        "    plt.title('Performance Metrics by Intent Class')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_path, \"class_performance_metrics.png\"))\n",
        "\n",
        "    # Interactive performance visualization\n",
        "    fig = px.imshow(class_df[metrics],\n",
        "                   labels=dict(x=\"Metric\", y=\"Intent Class\", color=\"Score\"),\n",
        "                   x=metrics, y=intent_classes,\n",
        "                   text_auto=True,\n",
        "                   color_continuous_scale='YlGnBu')\n",
        "\n",
        "    fig.update_layout(\n",
        "        title='Performance Metrics by Intent Class (Interactive)',\n",
        "        width=800,\n",
        "        height=max(400, len(intent_classes)*30)\n",
        "    )\n",
        "\n",
        "    fig.write_html(os.path.join(save_path, \"interactive_class_performance.html\"))\n",
        "\n",
        "    # Print report summary\n",
        "    print(\"\\nModel Evaluation Report:\")\n",
        "    print(f\"Overall Accuracy: {report['accuracy']:.4f}\")\n",
        "    print(f\"Macro F1-score: {report['macro avg']['f1-score']:.4f}\")\n",
        "    print(f\"Weighted F1-score: {report['weighted avg']['f1-score']:.4f}\")\n",
        "\n",
        "    return report, cm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Teks judul default\n",
        "def evaluate_model_simple(model, tokenizer, val_texts, val_labels, intent_classes, save_path):\n",
        "    \"\"\"Evaluate model and show key visualizations without saving files\"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    # Dataset dan loader\n",
        "    val_dataset = IntentDataset(val_texts, val_labels, tokenizer)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_dataloader, desc=\"Evaluasi Model\"):\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "            predictions = torch.argmax(logits, dim=1)\n",
        "\n",
        "            all_preds.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Classification report\n",
        "    report = classification_report(all_labels, all_preds,\n",
        "                                    target_names=intent_classes,\n",
        "                                    output_dict=True)\n",
        "    report_df = pd.DataFrame(report).transpose().round(3)\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\n📊 Model Evaluation Report:\")\n",
        "    print(f\"Accuracy        : {report['accuracy']:.4f}\")\n",
        "    print(f\"Macro F1-score  : {report['macro avg']['f1-score']:.4f}\")\n",
        "    print(f\"Weighted F1     : {report['weighted avg']['f1-score']:.4f}\")\n",
        "\n",
        "    # --- Visualisasi confusion matrix ---\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=intent_classes, yticklabels=intent_classes)\n",
        "    plt.title(\"🧩 Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # --- Visualisasi metrik per kelas (precision, recall, f1) ---\n",
        "    class_df = report_df.loc[intent_classes]\n",
        "    metrics = ['precision', 'recall', 'f1-score']\n",
        "\n",
        "    plt.figure(figsize=(10, len(intent_classes)*0.5 + 2))\n",
        "    sns.heatmap(class_df[metrics], annot=True, fmt=\".3f\", cmap=\"YlGnBu\",\n",
        "                yticklabels=intent_classes, cbar=True)\n",
        "    plt.title(\"📈 Performance Metrics per Intent Class\")\n",
        "    plt.xlabel(\"Metric\")\n",
        "    plt.ylabel(\"Intent Class\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return report, cm"
      ],
      "metadata": {
        "cellView": "form",
        "id": "zc0sMTEGuE5H"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Upp9G0xn0fN7",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title PredictionIntent\n",
        "def predict_intent_with_enhanced_ood(text, model, tokenizer, intent_classes,\n",
        "                                     energy_threshold, msp_threshold=None,\n",
        "                                     temperature=1.0, method='combined',\n",
        "                                     label_encoder=None, device=None,\n",
        "                                     return_logits=False):\n",
        "    \"\"\"\n",
        "    Memprediksi intent dari teks input dengan deteksi Out-of-Distribution yang ditingkatkan\n",
        "\n",
        "    Args:\n",
        "        text: Teks input untuk diprediksi\n",
        "        model: Model yang sudah dilatih\n",
        "        tokenizer: Tokenizer untuk model\n",
        "        intent_classes: List nama intent\n",
        "        energy_threshold: Threshold untuk energy-based OOD detection\n",
        "        msp_threshold: Threshold untuk MSP-based OOD detection\n",
        "        temperature: Parameter temperature untuk energy\n",
        "        method: Metode deteksi OOD - 'energy', 'msp', atau 'combined'\n",
        "        label_encoder: Label encoder untuk intent classes\n",
        "        device: Device untuk inference\n",
        "        return_logits: Jika True, mengembalikan logits asli\n",
        "\n",
        "    Returns:\n",
        "        dict: Hasil prediksi dengan detail OOD detection\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    if isinstance(text, str):\n",
        "        text = [text]  # Convert single text to list\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenisasi input\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # List untuk menyimpan hasil setiap input\n",
        "    results = []\n",
        "\n",
        "    # Prediksi\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Hitung energy score: -T*log(sum(exp(logits/T)))\n",
        "        energy = -temperature * torch.logsumexp(logits / temperature, dim=1)\n",
        "\n",
        "        # Hitung confidence dengan softmax\n",
        "        probabilities = torch.softmax(logits, dim=1)\n",
        "        predictions = torch.argmax(probabilities, dim=1)\n",
        "        max_probs = torch.max(probabilities, dim=1)[0]\n",
        "\n",
        "        for i in range(len(text)):\n",
        "            prediction = predictions[i].item()\n",
        "            energy_score = energy[i].item()\n",
        "            confidence = max_probs[i].item()\n",
        "\n",
        "            # Deteksi OOD berdasarkan metode yang dipilih\n",
        "            is_ood_energy = energy_score > energy_threshold if energy_threshold is not None else False\n",
        "            is_ood_msp = confidence < msp_threshold if msp_threshold is not None else False\n",
        "\n",
        "            if method == 'energy':\n",
        "                is_ood = is_ood_energy\n",
        "            elif method == 'msp':\n",
        "                is_ood = is_ood_msp\n",
        "            else:  # 'combined'\n",
        "                is_ood = is_ood_energy and is_ood_msp\n",
        "\n",
        "            # Tentukan intent berdasarkan hasil OOD detection\n",
        "            if is_ood:\n",
        "                predicted_intent = \"unknown\"\n",
        "                topk_intents = [(\"unknown\", 1.0)]  # Unknown intent dengan confidence 100%\n",
        "            else:\n",
        "                predicted_intent = intent_classes[prediction]\n",
        "\n",
        "                # Dapatkan top 3 intent dengan confidence tertinggi\n",
        "                top_k = min(3, len(intent_classes))\n",
        "                topk_values, topk_indices = torch.topk(probabilities[i], top_k)\n",
        "                topk_intents = [(intent_classes[idx.item()], val.item())\n",
        "                                for idx, val in zip(topk_indices, topk_values)]\n",
        "\n",
        "            # Buat hasil untuk input ini\n",
        "            result = {\n",
        "                \"text\": text[i],\n",
        "                \"intent\": predicted_intent,\n",
        "                \"confidence\": confidence,\n",
        "                \"energy_score\": energy_score,\n",
        "                \"is_ood\": is_ood,\n",
        "                \"is_ood_energy\": is_ood_energy,\n",
        "                \"is_ood_msp\": is_ood_msp if msp_threshold is not None else None,\n",
        "                \"top_intents\": topk_intents\n",
        "            }\n",
        "\n",
        "            if return_logits:\n",
        "                result[\"logits\"] = logits[i].cpu().numpy()\n",
        "\n",
        "            results.append(result)\n",
        "\n",
        "    # Jika hanya satu input, kembalikan hasil langsung tanpa list\n",
        "    if len(text) == 1:\n",
        "        return results[0]\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title semantic_fallback.py\n",
        "\n",
        "# ==========================================\n",
        "# SEMANTIC FALLBACK - satu blok kode lengkap\n",
        "# ==========================================\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "# Fungsi ini hanya dijalankan sekali untuk menghitung & menyimpan intent embeddings\n",
        "def build_intent_embeddings(intent_classes, tokenizer, model_name='indobenchmark/indobert-base-p1'):\n",
        "    model = AutoModel.from_pretrained(model_name)\n",
        "    model.eval()\n",
        "\n",
        "    intent_embeddings = {}\n",
        "    with torch.no_grad():\n",
        "        for label in intent_classes:\n",
        "            inputs = tokenizer(label, return_tensors=\"pt\")\n",
        "            outputs = model(**inputs)\n",
        "            embedding = outputs.last_hidden_state[:, 0, :]  # Ambil [CLS]\n",
        "            embedding = F.normalize(embedding, dim=-1)\n",
        "            intent_embeddings[label] = embedding.squeeze()\n",
        "\n",
        "    torch.save(intent_embeddings, \"intent_embeddings.pt\")\n",
        "    print(\"✅ Intent embeddings berhasil disimpan ke intent_embeddings.pt\")\n",
        "    return intent_embeddings\n",
        "\n",
        "# Fungsi pemanggilan: cek apakah file sudah ada atau hitung baru\n",
        "def load_or_build_intent_embeddings(intent_classes):\n",
        "    if os.path.exists(\"intent_embeddings.pt\"):\n",
        "        print(\"📦 Memuat intent_embeddings.pt dari disk...\")\n",
        "        return torch.load(\"intent_embeddings.pt\")\n",
        "    else:\n",
        "        print(\"🔁 Membuat intent embeddings dari awal...\")\n",
        "        return build_intent_embeddings(intent_classes, AutoTokenizer.from_pretrained('indobenchmark/indobert-base-p1'))\n",
        "\n",
        "# Fallback function\n",
        "def semantic_fallback(text, intent_embeddings, threshold=0.75, model_name='indobenchmark/indobert-base-p1'):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModel.from_pretrained(model_name)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "        outputs = model(**inputs)\n",
        "        input_emb = outputs.last_hidden_state[:, 0, :]\n",
        "        input_emb = F.normalize(input_emb, dim=-1)\n",
        "\n",
        "        max_sim = 0.0\n",
        "        best_intent = \"unknown\"\n",
        "\n",
        "        for label, label_emb in intent_embeddings.items():\n",
        "            sim = F.cosine_similarity(input_emb, label_emb.unsqueeze(0)).item()\n",
        "            if sim > max_sim:\n",
        "                max_sim = sim\n",
        "                best_intent = label\n",
        "\n",
        "        if max_sim >= threshold:\n",
        "            return best_intent, max_sim\n",
        "        else:\n",
        "            return \"unknown\", max_sim\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "iKMyON1-JbzC"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "fDr3k0Ir0rOU",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Run pipeline\n",
        "def run_full_pipeline_enhanced(use_drive=True, percentile=95, ood_method='combined', split_dataset=\"no\", val_split=0.2):\n",
        "    \"\"\"Jalankan pipeline lengkap dengan enhanced OOD detection dan visualisasi yang ditingkatkan\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    use_drive : bool\n",
        "        Apakah menggunakan Google Drive untuk penyimpanan\n",
        "    percentile : int\n",
        "        Persentil untuk threshold OOD detection\n",
        "    ood_method : str\n",
        "        Metode OOD detection ('msp', 'energy', 'combined')\n",
        "    split_dataset : str\n",
        "        Mode pemisahan dataset (\"yes\" untuk split dari train.csv, \"no\" untuk file terpisah)\n",
        "    val_split : float\n",
        "        Proporsi data validasi jika split_dataset=\"yes\" (default: 0.2)\n",
        "    \"\"\"\n",
        "\n",
        "    # Buat folder untuk simpan model\n",
        "    os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
        "\n",
        "    # 1. Load data dan siapkan train/validation set berdasarkan mode yang dipilih\n",
        "    train_csv = \"train.csv\"\n",
        "    val_csv = \"val.csv\"\n",
        "\n",
        "    if split_dataset.lower() == \"yes\":\n",
        "        # Load semua data dari file train.csv\n",
        "        all_texts, all_labels, intent_classes, label_encoder = load_csv_data(train_csv, show_distribution=True)\n",
        "\n",
        "        # Split data menjadi train dan validation\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "            all_texts, all_labels, test_size=val_split, random_state=42, stratify=all_labels\n",
        "        )\n",
        "\n",
        "        print(f\"\\n✅ Dataset telah dibagi: {len(train_texts)} data training dan {len(val_texts)} data validasi\")\n",
        "    else:\n",
        "        # Mode file terpisah (original)\n",
        "        # 2. Muat data training (buat label encoder)\n",
        "        train_texts, train_labels, intent_classes, label_encoder = load_csv_data(train_csv, show_distribution=True)\n",
        "\n",
        "        # 3. Muat data validasi (pakai label encoder dari training)\n",
        "        val_texts, val_labels, _, _ = load_csv_data(val_csv, label_encoder=label_encoder, show_distribution=True)\n",
        "\n",
        "    num_labels = len(intent_classes)\n",
        "\n",
        "    # 4. Setup model\n",
        "    model, tokenizer = setup_indobert_for_intent(num_labels)\n",
        "\n",
        "    # 5. Latih model dengan enhanced visualization\n",
        "    model, history = train_intent_classifier(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        train_texts,\n",
        "        train_labels,\n",
        "        val_texts,\n",
        "        val_labels,\n",
        "        class_names=intent_classes,  # Pass class names for per-class metrics\n",
        "        batch_size=16,\n",
        "        epochs=10,\n",
        "        learning_rate=2e-5,\n",
        "        weight_decay=0.01,\n",
        "        patience=3\n",
        "    )\n",
        "\n",
        "    # 6. OOD calibration (enhanced)\n",
        "    val_dataset = IntentDataset(val_texts, val_labels, tokenizer)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
        "    thresholds = enhanced_calibrate_ood(model, tokenizer, val_dataloader, MODEL_SAVE_PATH, percentile=percentile)\n",
        "\n",
        "    # 7. Evaluasi model dan visualisasikan hasil\n",
        "    report, cm = evaluate_model_enhanced(model, tokenizer, val_texts, val_labels, intent_classes, MODEL_SAVE_PATH)\n",
        "\n",
        "    # 8. Generate enhanced visualizations for the final model\n",
        "    enhanced_plot_training_results(history, MODEL_SAVE_PATH, class_names=intent_classes)\n",
        "    save_enhanced_history(history, MODEL_SAVE_PATH)\n",
        "    tqdm.write(f\"Pelatihan selesai! Model terbaik disimpan di {MODEL_SAVE_PATH}\")\n",
        "\n",
        "    # 9. Simpan intent classes & label encoder\n",
        "    with open(f\"{MODEL_SAVE_PATH}/intent_classes.pkl\", \"wb\") as f:\n",
        "        pickle.dump(intent_classes, f)\n",
        "\n",
        "    with open(f\"{MODEL_SAVE_PATH}/label_encoder.pkl\", \"wb\") as f:\n",
        "        pickle.dump(label_encoder, f)\n",
        "\n",
        "    print(f\"\\n✅ Model telah berhasil dilatih dan disimpan di {MODEL_SAVE_PATH}\")\n",
        "    print(f\"Jumlah intent: {num_labels}\")\n",
        "    print(f\"Intent yang didukung: {', '.join(intent_classes)}\")\n",
        "    print(f\"OOD detection thresholds: Energy={thresholds['energy_threshold']:.4f}, MSP={thresholds['msp_threshold']:.4f}\")\n",
        "    print(f\"Visualisasi training telah disimpan di {MODEL_SAVE_PATH}\")\n",
        "    print(f\"- Interactive plots dapat dibuka pada file HTML di folder tersebut\")\n",
        "    print(f\"- Static plots tersedia dalam format PNG\")\n",
        "\n",
        "    # 10. Demo prediksi\n",
        "    run_prediction_demo_enhanced(model, tokenizer, intent_classes, label_encoder, method=ood_method)\n",
        "\n",
        "    return model, tokenizer, intent_classes, label_encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Dq-WV98s0zgH"
      },
      "outputs": [],
      "source": [
        "# @title Run Prediksi\n",
        "def run_prediction_demo_enhanced(model=None, tokenizer=None, intent_classes=None, label_encoder=None, model_path=None, method='combined', test_texts=None):\n",
        "    \"\"\"Jalankan demo prediksi intent dengan model yang telah dilatih dan enhanced OOD detection\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : Model object, optional\n",
        "        Model yang sudah dilatih\n",
        "    tokenizer : Tokenizer object, optional\n",
        "        Tokenizer yang sesuai dengan model\n",
        "    intent_classes : list, optional\n",
        "        Daftar kelas intent\n",
        "    label_encoder : LabelEncoder, optional\n",
        "        Label encoder yang digunakan saat training\n",
        "    model_path : str, optional\n",
        "        Path ke model tersimpan (digunakan jika model=None)\n",
        "    method : str, optional\n",
        "        Metode OOD detection ('msp', 'energy', 'combined')\n",
        "    test_texts : list, optional\n",
        "        Daftar teks untuk diprediksi secara batch. Setelah batch, akan lanjut ke mode interaktif.\n",
        "    \"\"\"\n",
        "\n",
        "    if model_path is None:\n",
        "        model_path = MODEL_SAVE_PATH\n",
        "\n",
        "    # Jika model tidak diberikan, muat dari path penyimpanan\n",
        "    if model is None or tokenizer is None or intent_classes is None:\n",
        "        if not os.path.exists(model_path):\n",
        "            print(f\"Error: Model tidak ditemukan di {model_path}\")\n",
        "            print(\"Jalankan run_full_pipeline() terlebih dahulu untuk melatih model\")\n",
        "            return\n",
        "\n",
        "        # Muat model dan tokenizer\n",
        "        print(f\"Memuat model dari {model_path}...\")\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "        # Muat intent classes\n",
        "        import pickle\n",
        "        with open(f\"{model_path}/intent_classes.pkl\", \"rb\") as f:\n",
        "            intent_classes = pickle.load(f)\n",
        "            print(f\"Intent yang didukung: {', '.join(intent_classes)}\")\n",
        "            intent_embeddings = load_or_build_intent_embeddings(intent_classes)\n",
        "\n",
        "    # Load OOD thresholds\n",
        "    thresholds = load_ood_thresholds(model_path)\n",
        "    energy_threshold = thresholds[\"energy_threshold\"]\n",
        "    msp_threshold = thresholds.get(\"msp_threshold\")\n",
        "\n",
        "    if msp_threshold is not None:\n",
        "        print(f\"OOD thresholds loaded: Energy={energy_threshold:.4f}, MSP={msp_threshold:.4f}\")\n",
        "    else:\n",
        "        print(f\"OOD thresholds loaded: Energy={energy_threshold:.4f}, MSP=None\")\n",
        "\n",
        "    print(f\"Menggunakan metode deteksi OOD: {method}\")\n",
        "\n",
        "    print(\"\\nDemo Prediksi Intent dengan Enhanced OOD Detection:\")\n",
        "    print(\"====================================================\")\n",
        "\n",
        "    # Helper function untuk memprediksi dan menampilkan hasil\n",
        "    def predict_and_display(text):\n",
        "        result = predict_intent_with_enhanced_ood(\n",
        "            text,\n",
        "            model,\n",
        "            tokenizer,\n",
        "            intent_classes,\n",
        "            energy_threshold,\n",
        "            msp_threshold,\n",
        "            method=method\n",
        "        )\n",
        "\n",
        "        if result[\"is_ood\"]:\n",
        "            print(f\"⚠️ Intent terdeteksi: unknown\")\n",
        "            print(f\"   Energy score: {result['energy_score']:.4f} (threshold: {energy_threshold:.4f})\")\n",
        "            if msp_threshold:\n",
        "                print(f\"   Confidence score: {result['confidence']:.4f} (threshold: {msp_threshold:.4f})\")\n",
        "        else:\n",
        "            print(f\"✓ Intent terdeteksi: {result['intent']} (confidence: {result['confidence']:.4f})\")\n",
        "\n",
        "        print(\"\\nTop 3 intent:\")\n",
        "        for i, (intent_name, score) in enumerate(result[\"top_intents\"]):\n",
        "            print(f\"  {i+1}. {intent_name}: {score:.4f}\")\n",
        "\n",
        "        print(\"\\nDetail OOD detection:\")\n",
        "        print(f\"  Energy-based: {'OOD' if result['is_ood_energy'] else 'In-Distribution'} ({result['energy_score']:.4f})\")\n",
        "        if msp_threshold:\n",
        "            print(f\"  MSP-based: {'OOD' if result['is_ood_msp'] else 'In-Distribution'} ({result['confidence']:.4f})\")\n",
        "        print(f\"  Final decision: {'OOD' if result['is_ood'] else 'In-Distribution'}\")\n",
        "\n",
        "    # Jika test_texts diberikan, lakukan prediksi batch\n",
        "    if test_texts is not None and isinstance(test_texts, list) and len(test_texts) > 0:\n",
        "        print(f\"\\nMemprediksi {len(test_texts)} contoh teks:\")\n",
        "        print(\"----------------------------\")\n",
        "\n",
        "        for i, text in enumerate(test_texts):\n",
        "            print(f\"\\nContoh #{i+1}: \\\"{text}\\\"\")\n",
        "            predict_and_display(text)\n",
        "\n",
        "        print(\"\\n----------------------------\")\n",
        "        print(\"Selesai memprediksi contoh teks. Beralih ke mode interaktif.\")\n",
        "\n",
        "    # Mode interaktif\n",
        "    print(\"\\nMode Interaktif - Masukkan teks untuk prediksi intent\")\n",
        "    print(\"Ketik 'exit' untuk keluar\")\n",
        "    print(\"----------------------------\")\n",
        "\n",
        "    # Prediksi input pengguna\n",
        "    while True:\n",
        "        user_input = input(\"\\nMasukkan teks: \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        predict_and_display(user_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SpUFrU9D-qPt"
      },
      "outputs": [],
      "source": [
        "# Cell 13: Jalankan pipeline (uncomment untuk menjalankan)\n",
        "\n",
        "model, tokenizer, intent_classes, label_encoder = run_full_pipeline_enhanced(\n",
        "    percentile=85,  # Adjust OOD sensitivity (higher = more strict)\n",
        "    ood_method='combined',  # Use 'energy', 'msp', or 'combined'\n",
        "    split_dataset=\"yes\"\n",
        ")\n",
        "\n",
        "\n",
        "# Cell 14: Jalankan demo prediksi (uncomment untuk menjalankan)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Teks judul default\n",
        "test_sentences = [\n",
        "    # GREETING\n",
        "    \"Halo, selamat pagi!\",\n",
        "    \"Apa kabar?\",\n",
        "    \"Hai, bot!\",\n",
        "    \"Permisi, boleh bertanya?\",\n",
        "    \"Yo, ada orang di sana?\",\n",
        "\n",
        "    # GOODBYE\n",
        "    \"Terima kasih, sampai jumpa.\",\n",
        "    \"Ok, saya pergi dulu.\",\n",
        "    \"Sampai nanti!\",\n",
        "    \"Dadah, bot.\",\n",
        "    \"Aku akan kembali nanti.\",\n",
        "\n",
        "    # CONFIRM\n",
        "    \"Iya, benar.\",\n",
        "    \"Betul sekali.\",\n",
        "    \"Ya, saya setuju.\",\n",
        "    \"Tentu saja.\",\n",
        "    \"Itu yang saya maksud.\",\n",
        "\n",
        "    # DENIED\n",
        "    \"Tidak, bukan itu.\",\n",
        "    \"Salah.\",\n",
        "    \"Bukan, maksud saya yang lain.\",\n",
        "    \"Enggak.\",\n",
        "    \"Saya tidak yakin dengan itu.\",\n",
        "\n",
        "    # AMBIGUOUS (bisa mengecoh)\n",
        "    \"Saya rasa tidak perlu, tapi ya juga boleh.\",\n",
        "    \"Mungkin... tapi entahlah.\",\n",
        "    \"Terserah kamu aja deh.\",\n",
        "    \"Boleh iya, boleh juga tidak.\",\n",
        "    \"Ya tapi tidak juga sih...\",\n",
        "    \"p\",\n",
        "    \"test\",\n",
        "    \"y\",\n",
        "    \"g\",\n",
        "    \"N\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "P9gOO73x_oxo",
        "cellView": "form"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "oS_b2pfqxA8Z",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28d00065-5bf9-437d-b076-de0367838658"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memuat model dari /content/drive/MyDrive/indobert_intent_model...\n",
            "Intent yang didukung: confirm, denied, goodbye, greeting\n",
            "OOD thresholds loaded: Energy=-6.2725, MSP=0.9988\n",
            "Menggunakan metode deteksi OOD: combined\n",
            "\n",
            "Demo Prediksi Intent dengan Enhanced OOD Detection:\n",
            "====================================================\n",
            "\n",
            "Memprediksi 30 contoh teks:\n",
            "----------------------------\n",
            "\n",
            "Contoh #1: \"Halo, selamat pagi!\"\n",
            "⚠️ Intent terdeteksi: unknown\n",
            "   Energy score: -6.0979 (threshold: -6.2725)\n",
            "   Confidence score: 0.9982 (threshold: 0.9988)\n",
            "\n",
            "Top 3 intent:\n",
            "  1. unknown: 1.0000\n",
            "\n",
            "Detail OOD detection:\n",
            "  Energy-based: OOD (-6.0979)\n",
            "  MSP-based: OOD (0.9982)\n",
            "  Final decision: OOD\n",
            "\n",
            "Contoh #2: \"Apa kabar?\"\n",
            "✓ Intent terdeteksi: greeting (confidence: 0.9990)\n",
            "\n",
            "Top 3 intent:\n",
            "  1. greeting: 0.9990\n",
            "  2. goodbye: 0.0007\n",
            "  3. denied: 0.0003\n",
            "\n",
            "Detail OOD detection:\n",
            "  Energy-based: In-Distribution (-6.3907)\n",
            "  MSP-based: In-Distribution (0.9990)\n",
            "  Final decision: In-Distribution\n",
            "\n",
            "Contoh #3: \"Hai, bot!\"\n",
            "✓ Intent terdeteksi: greeting (confidence: 0.9989)\n",
            "\n",
            "Top 3 intent:\n",
            "  1. greeting: 0.9989\n",
            "  2. goodbye: 0.0008\n",
            "  3. confirm: 0.0002\n",
            "\n",
            "Detail OOD detection:\n",
            "  Energy-based: In-Distribution (-6.2948)\n",
            "  MSP-based: In-Distribution (0.9989)\n",
            "  Final decision: In-Distribution\n",
            "\n",
            "Contoh #4: \"Permisi, boleh bertanya?\"\n",
            "✓ Intent terdeteksi: greeting (confidence: 0.9992)\n",
            "\n",
            "Top 3 intent:\n",
            "  1. greeting: 0.9992\n",
            "  2. goodbye: 0.0005\n",
            "  3. denied: 0.0003\n",
            "\n",
            "Detail OOD detection:\n",
            "  Energy-based: In-Distribution (-6.3851)\n",
            "  MSP-based: In-Distribution (0.9992)\n",
            "  Final decision: In-Distribution\n",
            "\n",
            "Contoh #5: \"Yo, ada orang di sana?\"\n",
            "⚠️ Intent terdeteksi: unknown\n",
            "   Energy score: -4.9663 (threshold: -6.2725)\n",
            "   Confidence score: 0.9906 (threshold: 0.9988)\n",
            "\n",
            "Top 3 intent:\n",
            "  1. unknown: 1.0000\n",
            "\n",
            "Detail OOD detection:\n",
            "  Energy-based: OOD (-4.9663)\n",
            "  MSP-based: OOD (0.9906)\n",
            "  Final decision: OOD\n",
            "\n",
            "Contoh #6: \"Terima kasih, sampai jumpa.\"\n",
            "✓ Intent terdeteksi: goodbye (confidence: 0.9996)\n",
            "\n",
            "Top 3 intent:\n",
            "  1. goodbye: 0.9996\n",
            "  2. greeting: 0.0002\n",
            "  3. denied: 0.0001\n",
            "\n",
            "Detail OOD detection:\n",
            "  Energy-based: In-Distribution (-6.6676)\n",
            "  MSP-based: In-Distribution (0.9996)\n",
            "  Final decision: In-Distribution\n",
            "\n",
            "Contoh #7: \"Ok, saya pergi dulu.\"\n",
            "⚠️ Intent terdeteksi: unknown\n",
            "   Energy score: -5.8914 (threshold: -6.2725)\n",
            "   Confidence score: 0.9983 (threshold: 0.9988)\n",
            "\n",
            "Top 3 intent:\n",
            "  1. unknown: 1.0000\n",
            "\n",
            "Detail OOD detection:\n",
            "  Energy-based: OOD (-5.8914)\n",
            "  MSP-based: OOD (0.9983)\n",
            "  Final decision: OOD\n",
            "\n",
            "Contoh #8: \"Sampai nanti!\"\n",
            "✓ Intent terdeteksi: goodbye (confidence: 0.9992)\n",
            "\n",
            "Top 3 intent:\n",
            "  1. goodbye: 0.9992\n",
            "  2. denied: 0.0006\n",
            "  3. confirm: 0.0002\n",
            "\n",
            "Detail OOD detection:\n",
            "  Energy-based: In-Distribution (-6.2964)\n",
            "  MSP-based: In-Distribution (0.9992)\n",
            "  Final decision: In-Distribution\n",
            "\n",
            "Contoh #9: \"Dadah, bot.\"\n",
            "✓ Intent terdeteksi: goodbye (confidence: 0.9994)\n",
            "\n",
            "Top 3 intent:\n",
            "  1. goodbye: 0.9994\n",
            "  2. greeting: 0.0002\n",
            "  3. confirm: 0.0002\n",
            "\n",
            "Detail OOD detection:\n",
            "  Energy-based: In-Distribution (-6.4655)\n",
            "  MSP-based: In-Distribution (0.9994)\n",
            "  Final decision: In-Distribution\n",
            "\n",
            "Contoh #10: \"Aku akan kembali nanti.\"\n",
            "⚠️ Intent terdeteksi: unknown\n",
            "   Energy score: -5.7344 (threshold: -6.2725)\n",
            "   Confidence score: 0.9976 (threshold: 0.9988)\n",
            "\n",
            "Top 3 intent:\n",
            "  1. unknown: 1.0000\n",
            "\n",
            "Detail OOD detection:\n",
            "  Energy-based: OOD (-5.7344)\n",
            "  MSP-based: OOD (0.9976)\n",
            "  Final decision: OOD\n",
            "\n",
            "Contoh #11: \"Iya, benar.\"\n",
            "✓ Intent terdeteksi: confirm (confidence: 0.9993)\n",
            "\n",
            "Top 3 intent:\n",
            "  1. confirm: 0.9993\n",
            "  2. denied: 0.0004\n",
            "  3. goodbye: 0.0001\n",
            "\n",
            "Detail OOD detection:\n",
            "  Energy-based: OOD (-6.2704)\n",
            "  MSP-based: In-Distribution (0.9993)\n",
            "  Final decision: In-Distribution\n",
            "\n",
            "Contoh #12: \"Betul sekali.\"\n",
            "✓ Intent terdeteksi: confirm (confidence: 0.9994)\n",
            "\n",
            "Top 3 intent:\n",
            "  1. confirm: 0.9994\n",
            "  2. denied: 0.0004\n",
            "  3. goodbye: 0.0002\n",
            "\n",
            "Detail OOD detection:\n",
            "  Energy-based: In-Distribution (-6.3036)\n",
            "  MSP-based: In-Distribution (0.9994)\n",
            "  Final decision: In-Distribution\n",
            "\n",
            "Contoh #13: \"Ya, saya setuju.\"\n",
            "✓ Intent terdeteksi: confirm (confidence: 0.9994)\n",
            "\n",
            "Top 3 intent:\n",
            "  1. confirm: 0.9994\n",
            "  2. denied: 0.0004\n",
            "  3. goodbye: 0.0001\n",
            "\n",
            "Detail OOD detection:\n",
            "  Energy-based: In-Distribution (-6.3236)\n",
            "  MSP-based: In-Distribution (0.9994)\n",
            "  Final decision: In-Distribution\n",
            "\n",
            "Contoh #14: \"Tentu saja.\"\n",
            "✓ Intent terdeteksi: confirm (confidence: 0.9994)\n",
            "\n",
            "Top 3 intent:\n",
            "  1. confirm: 0.9994\n",
            "  2. denied: 0.0003\n",
            "  3. goodbye: 0.0002\n",
            "\n",
            "Detail OOD detection:\n",
            "  Energy-based: In-Distribution (-6.2842)\n",
            "  MSP-based: In-Distribution (0.9994)\n",
            "  Final decision: In-Distribution\n",
            "\n",
            "Contoh #15: \"Itu yang saya maksud.\"\n",
            "✓ Intent terdeteksi: confirm (confidence: 0.9988)\n",
            "\n",
            "Top 3 intent:\n",
            "  1. confirm: 0.9988\n",
            "  2. denied: 0.0009\n",
            "  3. greeting: 0.0001\n",
            "\n",
            "Detail OOD detection:\n",
            "  Energy-based: OOD (-6.1046)\n",
            "  MSP-based: In-Distribution (0.9988)\n",
            "  Final decision: In-Distribution\n",
            "\n",
            "Contoh #16: \"Tidak, bukan itu.\"\n",
            "✓ Intent terdeteksi: denied (confidence: 0.9990)\n",
            "\n",
            "Top 3 intent:\n",
            "  1. denied: 0.9990\n",
            "  2. confirm: 0.0006\n",
            "  3. goodbye: 0.0003\n",
            "\n",
            "Detail OOD detection:\n",
            "  Energy-based: OOD (-6.1749)\n",
            "  MSP-based: In-Distribution (0.9990)\n",
            "  Final decision: In-Distribution\n",
            "\n",
            "Contoh #17: \"Salah.\"\n",
            "✓ Intent terdeteksi: denied (confidence: 0.9992)\n",
            "\n",
            "Top 3 intent:\n",
            "  1. denied: 0.9992\n",
            "  2. greeting: 0.0003\n",
            "  3. confirm: 0.0003\n",
            "\n",
            "Detail OOD detection:\n",
            "  Energy-based: In-Distribution (-6.3078)\n",
            "  MSP-based: In-Distribution (0.9992)\n",
            "  Final decision: In-Distribution\n",
            "\n",
            "Contoh #18: \"Bukan, maksud saya yang lain.\"\n",
            "⚠️ Intent terdeteksi: unknown\n",
            "   Energy score: -6.0852 (threshold: -6.2725)\n",
            "   Confidence score: 0.9987 (threshold: 0.9988)\n",
            "\n",
            "Top 3 intent:\n",
            "  1. unknown: 1.0000\n",
            "\n",
            "Detail OOD detection:\n",
            "  Energy-based: OOD (-6.0852)\n",
            "  MSP-based: OOD (0.9987)\n",
            "  Final decision: OOD\n",
            "\n",
            "Contoh #19: \"Enggak.\"\n",
            "✓ Intent terdeteksi: denied (confidence: 0.9992)\n",
            "\n",
            "Top 3 intent:\n",
            "  1. denied: 0.9992\n",
            "  2. confirm: 0.0003\n",
            "  3. greeting: 0.0003\n",
            "\n",
            "Detail OOD detection:\n",
            "  Energy-based: In-Distribution (-6.3207)\n",
            "  MSP-based: In-Distribution (0.9992)\n",
            "  Final decision: In-Distribution\n",
            "\n",
            "Contoh #20: \"Saya tidak yakin dengan itu.\"\n",
            "✓ Intent terdeteksi: denied (confidence: 0.9990)\n",
            "\n",
            "Top 3 intent:\n",
            "  1. denied: 0.9990\n",
            "  2. confirm: 0.0006\n",
            "  3. goodbye: 0.0002\n",
            "\n",
            "Detail OOD detection:\n",
            "  Energy-based: OOD (-6.1721)\n",
            "  MSP-based: In-Distribution (0.9990)\n",
            "  Final decision: In-Distribution\n",
            "\n",
            "Contoh #21: \"Saya rasa tidak perlu, tapi ya juga boleh.\"\n",
            "⚠️ Intent terdeteksi: unknown\n",
            "   Energy score: -5.2789 (threshold: -6.2725)\n",
            "   Confidence score: 0.9888 (threshold: 0.9988)\n",
            "\n",
            "Top 3 intent:\n",
            "  1. unknown: 1.0000\n",
            "\n",
            "Detail OOD detection:\n",
            "  Energy-based: OOD (-5.2789)\n",
            "  MSP-based: OOD (0.9888)\n",
            "  Final decision: OOD\n",
            "\n",
            "Contoh #22: \"Mungkin... tapi entahlah.\"\n",
            "⚠️ Intent terdeteksi: unknown\n",
            "   Energy score: -5.8089 (threshold: -6.2725)\n",
            "   Confidence score: 0.9979 (threshold: 0.9988)\n",
            "\n",
            "Top 3 intent:\n",
            "  1. unknown: 1.0000\n",
            "\n",
            "Detail OOD detection:\n",
            "  Energy-based: OOD (-5.8089)\n",
            "  MSP-based: OOD (0.9979)\n",
            "  Final decision: OOD\n",
            "\n",
            "Contoh #23: \"Terserah kamu aja deh.\"\n",
            "✓ Intent terdeteksi: confirm (confidence: 0.9994)\n",
            "\n",
            "Top 3 intent:\n",
            "  1. confirm: 0.9994\n",
            "  2. denied: 0.0003\n",
            "  3. goodbye: 0.0002\n",
            "\n",
            "Detail OOD detection:\n",
            "  Energy-based: In-Distribution (-6.3180)\n",
            "  MSP-based: In-Distribution (0.9994)\n",
            "  Final decision: In-Distribution\n",
            "\n",
            "Contoh #24: \"Boleh iya, boleh juga tidak.\"\n",
            "⚠️ Intent terdeteksi: unknown\n",
            "   Energy score: -5.9581 (threshold: -6.2725)\n",
            "   Confidence score: 0.9977 (threshold: 0.9988)\n",
            "\n",
            "Top 3 intent:\n",
            "  1. unknown: 1.0000\n",
            "\n",
            "Detail OOD detection:\n",
            "  Energy-based: OOD (-5.9581)\n",
            "  MSP-based: OOD (0.9977)\n",
            "  Final decision: OOD\n",
            "\n",
            "Contoh #25: \"Ya tapi tidak juga sih...\"\n",
            "⚠️ Intent terdeteksi: unknown\n",
            "   Energy score: -4.5296 (threshold: -6.2725)\n",
            "   Confidence score: 0.9023 (threshold: 0.9988)\n",
            "\n",
            "Top 3 intent:\n",
            "  1. unknown: 1.0000\n",
            "\n",
            "Detail OOD detection:\n",
            "  Energy-based: OOD (-4.5296)\n",
            "  MSP-based: OOD (0.9023)\n",
            "  Final decision: OOD\n",
            "\n",
            "Contoh #26: \"p\"\n",
            "⚠️ Intent terdeteksi: unknown\n",
            "   Energy score: -6.2689 (threshold: -6.2725)\n",
            "   Confidence score: 0.9986 (threshold: 0.9988)\n",
            "\n",
            "Top 3 intent:\n",
            "  1. unknown: 1.0000\n",
            "\n",
            "Detail OOD detection:\n",
            "  Energy-based: OOD (-6.2689)\n",
            "  MSP-based: OOD (0.9986)\n",
            "  Final decision: OOD\n",
            "\n",
            "Contoh #27: \"test\"\n",
            "⚠️ Intent terdeteksi: unknown\n",
            "   Energy score: -6.0574 (threshold: -6.2725)\n",
            "   Confidence score: 0.9978 (threshold: 0.9988)\n",
            "\n",
            "Top 3 intent:\n",
            "  1. unknown: 1.0000\n",
            "\n",
            "Detail OOD detection:\n",
            "  Energy-based: OOD (-6.0574)\n",
            "  MSP-based: OOD (0.9978)\n",
            "  Final decision: OOD\n",
            "\n",
            "Contoh #28: \"y\"\n",
            "✓ Intent terdeteksi: confirm (confidence: 0.9994)\n",
            "\n",
            "Top 3 intent:\n",
            "  1. confirm: 0.9994\n",
            "  2. denied: 0.0003\n",
            "  3. goodbye: 0.0002\n",
            "\n",
            "Detail OOD detection:\n",
            "  Energy-based: OOD (-6.2082)\n",
            "  MSP-based: In-Distribution (0.9994)\n",
            "  Final decision: In-Distribution\n",
            "\n",
            "Contoh #29: \"g\"\n",
            "✓ Intent terdeteksi: denied (confidence: 0.9991)\n",
            "\n",
            "Top 3 intent:\n",
            "  1. denied: 0.9991\n",
            "  2. greeting: 0.0006\n",
            "  3. goodbye: 0.0002\n",
            "\n",
            "Detail OOD detection:\n",
            "  Energy-based: In-Distribution (-6.3085)\n",
            "  MSP-based: In-Distribution (0.9991)\n",
            "  Final decision: In-Distribution\n",
            "\n",
            "Contoh #30: \"N\"\n",
            "✓ Intent terdeteksi: denied (confidence: 0.9991)\n",
            "\n",
            "Top 3 intent:\n",
            "  1. denied: 0.9991\n",
            "  2. greeting: 0.0005\n",
            "  3. goodbye: 0.0002\n",
            "\n",
            "Detail OOD detection:\n",
            "  Energy-based: In-Distribution (-6.2965)\n",
            "  MSP-based: In-Distribution (0.9991)\n",
            "  Final decision: In-Distribution\n",
            "\n",
            "----------------------------\n",
            "Selesai memprediksi contoh teks. Beralih ke mode interaktif.\n",
            "\n",
            "Mode Interaktif - Masukkan teks untuk prediksi intent\n",
            "Ketik 'exit' untuk keluar\n",
            "----------------------------\n",
            "\n",
            "Masukkan teks: exit\n"
          ]
        }
      ],
      "source": [
        "# If you want to load an existing model and run predictions\n",
        "run_prediction_demo_enhanced(\n",
        "    model_path=MODEL_SAVE_PATH,  # Your MODEL_SAVE_PATH\n",
        "    method='combined',  # Which OOD detection method to use\n",
        "    test_texts=test_sentences\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUYgABG0S6lf",
        "outputId": "42fc80c0-68c6-497d-9baa-5e27fa97abb4",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ File dikonversi ke CSV: /content/val.csv\n",
            "🔄 Augmenting class 'greeting'...\n"
          ]
        }
      ],
      "source": [
        "# @title Teks judul default {\"form-width\":\"30%\"}\n",
        "\n",
        "import pandas as pd\n",
        "import random\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "from deep_translator import GoogleTranslator\n",
        "from nltk.corpus import wordnet\n",
        "from collections import defaultdict\n",
        "import os\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# =========[ FILE PATHS ]=========\n",
        "PilihData = \"val\" # @param [\"train\",\"val\"]\n",
        "if PilihData == \"train\":\n",
        "    file_path = \"/content/rusdi-prototype-1/val.xlsx\"\n",
        "    csv_path = \"/content/train.csv\"\n",
        "    output_path_balanced = \"/content/train.csv\"\n",
        "elif PilihData == \"val\":\n",
        "    file_path = \"/content/val.xlsx\"\n",
        "    csv_path = \"/content/val.csv\"\n",
        "    output_path_balanced = \"/content/val.csv\"\n",
        "\n",
        "# =========[ READ & CONVERT XLSX ]=========\n",
        "if file_path.endswith(\".xlsx\"):\n",
        "    df = pd.read_excel(file_path)\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"✅ File dikonversi ke CSV: {csv_path}\")\n",
        "else:\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "# =========[ AUGMENTATION METHODS ]=========\n",
        "\n",
        "def get_synonym(word):\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word, lang='ind'):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.add(lemma.name())\n",
        "    return list(synonyms)\n",
        "\n",
        "def replace_with_synonym(sentence):\n",
        "    words = sentence.split()\n",
        "    new_words = [random.choice(get_synonym(word)) if get_synonym(word) else word for word in words]\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "def back_translate(sentence):\n",
        "    try:\n",
        "        translated = GoogleTranslator(source='id', target='en').translate(sentence)\n",
        "        return GoogleTranslator(source='en', target='id').translate(translated)\n",
        "    except Exception:\n",
        "        return sentence\n",
        "\n",
        "def add_typo(sentence):\n",
        "    chars = list(sentence)\n",
        "    if len(chars) > 3:\n",
        "        idx = random.randint(0, len(chars) - 1)\n",
        "        chars[idx] = random.choice(\"abcdefghijklmnopqrstuvwxyz\")\n",
        "    return \"\".join(chars)\n",
        "\n",
        "def random_deletion(sentence, p=0.3):\n",
        "    words = sentence.split()\n",
        "    if len(words) == 1:\n",
        "        return sentence\n",
        "    new_words = [word for word in words if random.uniform(0, 1) > p]\n",
        "    return \" \".join(new_words) if new_words else random.choice(words)\n",
        "\n",
        "def random_swap(sentence, n=1):\n",
        "    words = sentence.split()\n",
        "    if len(words) < 2:\n",
        "        return sentence\n",
        "    for _ in range(n):\n",
        "        idx1, idx2 = random.sample(range(len(words)), 2)\n",
        "        words[idx1], words[idx2] = words[idx2], words[idx1]\n",
        "    return \" \".join(words)\n",
        "\n",
        "def phonetic_augmentation(sentence):\n",
        "    phonetic_dict = {\n",
        "        \"saya\": \"sy\", \"kamu\": \"km\", \"oke\": \"ok\", \"ga\": \"g\", \"ya\": \"y\",\n",
        "        \"dong\": \"dongz\", \"bro\": \"brow\", \"please\": \"plis\", \"bisa\": \"bs\", \"selamat\": \"slmt\"\n",
        "    }\n",
        "    words = sentence.split()\n",
        "    new_words = [phonetic_dict[word] if word in phonetic_dict else word for word in words]\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "def add_common_phrase(sentence):\n",
        "    common_phrases = [\"ya\", \"anjay\", \"dong\", \"cuy\", \"bro\", \"lah\", \"plis\", \"eh\"]\n",
        "    return sentence + \" \" + random.choice(common_phrases)\n",
        "\n",
        "# Combined augmentations\n",
        "def augment_data(text):\n",
        "    methods = [\n",
        "        replace_with_synonym,\n",
        "        back_translate,\n",
        "        add_typo,\n",
        "        random_deletion,\n",
        "        random_swap,\n",
        "        phonetic_augmentation,\n",
        "        add_common_phrase\n",
        "    ]\n",
        "    augmented = set()\n",
        "    for method in methods:\n",
        "        try:\n",
        "            result = method(text)\n",
        "            if result != text:\n",
        "                augmented.add(result)\n",
        "        except:\n",
        "            continue\n",
        "    return list(augmented)\n",
        "\n",
        "# =========[ BALANCED AUGMENTATION ]=========\n",
        "target_aug_per_class = 600  # Can be changed (e.g., 100 augmented per class)\n",
        "intent_buckets = defaultdict(list)\n",
        "\n",
        "# Group original data by intent\n",
        "for _, row in df.iterrows():\n",
        "    intent_buckets[row['intent']].append(row['text'])\n",
        "\n",
        "df_augmented_balanced = []\n",
        "\n",
        "# Augment per class until target reached\n",
        "for intent, texts in intent_buckets.items():\n",
        "    current_aug_count = 0\n",
        "    index = 0\n",
        "    existing_aug_set = set()\n",
        "    print(f\"🔄 Augmenting class '{intent}'...\")\n",
        "\n",
        "    while current_aug_count < target_aug_per_class:\n",
        "        original_text = texts[index % len(texts)]\n",
        "        augmented_variants = augment_data(original_text)\n",
        "\n",
        "        for aug in augmented_variants:\n",
        "            if aug != original_text and aug not in existing_aug_set:\n",
        "                df_augmented_balanced.append([aug, intent])\n",
        "                existing_aug_set.add(aug)\n",
        "                current_aug_count += 1\n",
        "                if current_aug_count >= target_aug_per_class:\n",
        "                    break\n",
        "        index += 1\n",
        "\n",
        "# Combine original + augmented\n",
        "df_original = df[['text', 'intent']]\n",
        "df_augmented_balanced = pd.DataFrame(df_augmented_balanced, columns=[\"text\", \"intent\"])\n",
        "df_final = pd.concat([df_original, df_augmented_balanced], ignore_index=True)\n",
        "\n",
        "# Save to CSV\n",
        "df_final.to_csv(output_path_balanced, index=False)\n",
        "print(f\"\\n✅ Augmentasi selesai dan dataset seimbang disimpan di: {output_path_balanced}\")\n",
        "\n",
        "# =========[ PLOTTING DISTRIBUTIONS ]=========\n",
        "def plot_distribution(data, title):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    data['intent'].value_counts().sort_index().plot(kind='bar', color='skyblue')\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Intent\")\n",
        "    plt.ylabel(\"Jumlah Sampel\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Show before/after plots\n",
        "print(\"\\n📊 Distribusi Sebelum Augmentasi:\")\n",
        "plot_distribution(df_original, \"Distribusi Intent Sebelum Augmentasi\")\n",
        "\n",
        "print(\"\\n📊 Distribusi Setelah Augmentasi:\")\n",
        "plot_distribution(df_final, \"Distribusi Intent Setelah Augmentasi (Seimbang)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "e6RbBAsCTxZD",
        "outputId": "db4185b0-8394-4597-9855-f5ab6007f284",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File copied from '/content/rusdi-prototype-1/val.xlsx' to '/content/val.xlsx'\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "source_path = '/content/rusdi-prototype-1/val.xlsx'\n",
        "destination_path = '/content/val.xlsx'\n",
        "\n",
        "shutil.copy(source_path, destination_path)\n",
        "print(f\"File copied from '{source_path}' to '{destination_path}'\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: xlsx to csv convert\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Replace 'input.xlsx' with the actual path to your Excel fle\n",
        "excel_file = '/content/rusdi-prototype-1/val.xlsx'\n",
        "\n",
        "# Replace 'output.csv' with the desired path and name for your CSV file\n",
        "csv_file = 'val.csv'\n",
        "\n",
        "try:\n",
        "  # Read the Excel file into a Pandas DataFrame\n",
        "  df = pd.read_excel(excel_file)\n",
        "\n",
        "  # Convert the DataFrame to a CSV file\n",
        "  df.to_csv(csv_file, index=False)\n",
        "\n",
        "  print(f\"Successfully converted '{excel_file}' to '{csv_file}'\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "  print(f\"Error: File '{excel_file}' not found.\")\n",
        "except Exception as e:\n",
        "  print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "y-i6zTOdym6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20d4329b-a11d-4585-9f21-ff0af823440b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully converted '/content/rusdi-prototype-1/val.xlsx' to 'val.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7cRi5Nhv_9S",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Teks judul default\n",
        "# prompt: the model already trained and saved to drive, but can i test the model without running thee training.\n",
        "\n",
        "# Path ke folder model di Google Drive\n",
        "model_path = \"/content/drive/MyDrive/indobert_intent_model\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "# Load intent classes\n",
        "with open(f\"{model_path}/intent_classes.pkl\", \"rb\") as f:\n",
        "    intent_classes = pickle.load(f)\n",
        "\n",
        "# Load label encoder (jika diperlukan)\n",
        "with open(f\"{model_path}/label_encoder.pkl\", \"rb\") as f:\n",
        "    label_encoder = pickle.load(f)\n",
        "\n",
        "\n",
        "def predict_intent(text, model, tokenizer, intent_classes, device=None):\n",
        "    \"\"\"Memprediksi intent dari teks input\"\"\"\n",
        "\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenisasi input\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Prediksi\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probabilities = torch.softmax(logits, dim=1)\n",
        "        prediction = torch.argmax(probabilities, dim=1).item()\n",
        "        confidence = probabilities[0][prediction].item()\n",
        "\n",
        "    predicted_intent = intent_classes[prediction]\n",
        "\n",
        "    # Dapatkan top 3 intent dengan confidence tertinggi\n",
        "    top_k = 3\n",
        "    if len(intent_classes) < top_k:\n",
        "        top_k = len(intent_classes)\n",
        "\n",
        "    topk_values, topk_indices = torch.topk(probabilities, top_k, dim=1)\n",
        "    topk_intents = [(intent_classes[idx.item()], val.item()) for idx, val in zip(topk_indices[0], topk_values[0])]\n",
        "\n",
        "    return predicted_intent, confidence, topk_intents\n",
        "\n",
        "\n",
        "# Contoh penggunaan\n",
        "    while True:\n",
        "        user_input = input(\"\\nMasukkan teks untuk prediksi intent (ketik 'exit' untuk keluar): \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        intent, confidence, topk = predict_intent(user_input, model, tokenizer, intent_classes)\n",
        "        print(f\"Intent terdeteksi: {intent} (confidence: {confidence:.4f})\")\n",
        "        print(\"Top 3 intent:\")\n",
        "        for i, (intent_name, score) in enumerate(topk):\n",
        "            print(f\"  {i+1}. {intent_name}: {score:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2bICI1U4QZLy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}