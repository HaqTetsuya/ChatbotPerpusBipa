{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1swfnTHxWRr7plPX-4HMAHoKeGHUVq33r",
      "authorship_tag": "ABX9TyMdwYxGIxU8wO4IuZTlcICc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HaqTetsuya/ChatbotPerpusBipa/blob/main/bookrecomend2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers nltk pandas scikit-learn\n",
        "!git clone https://github.com/HaqTetsuya/ChatbotPerpusBipa.git\n",
        "!wget https://raw.githubusercontent.com/HaqTetsuya/rusdi-prototype-1/main/py/BooksDatasetCleanFiltered.csv\n",
        "!wget https://raw.githubusercontent.com/HaqTetsuya/ChatbotPerpusBipa/main/BookDatasetSample.csv"
      ],
      "metadata": {
        "id": "iJIAMbRkvtVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81AjnzIqnspP"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Book Recommender System - Google Colab Pipeline\n",
        "# This notebook demonstrates how to use the BookRecommender class to build\n",
        "# a recommendation system for books based on semantic similarity.\n",
        "\"\"\"\n",
        "\n",
        "# Install required packages\n",
        "!pip install -q sentence-transformers nltk pandas scikit-learn\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "import pickle\n",
        "\n",
        "# Save the book recommender code to a file\n",
        "with open('book_recommender.py', 'w') as f:\n",
        "    # Copy the entire code from the original file here\n",
        "    f.write('''import os\n",
        "import re\n",
        "import pickle\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Union, Optional, Tuple\n",
        "from dataclasses import dataclass\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger('book_recommender')\n",
        "\n",
        "@dataclass\n",
        "class BookRecommendation:\n",
        "    \"\"\"Data class for book recommendations with standardized attributes.\"\"\"\n",
        "    title: str\n",
        "    author: str\n",
        "    category: str = \"\"\n",
        "    year: Union[int, str] = \"\"\n",
        "    description: str = \"\"\n",
        "    relevance_score: float = 0.0\n",
        "    rank: int = 0\n",
        "\n",
        "    def to_dict(self) -> Dict:\n",
        "        \"\"\"Convert recommendation to dictionary.\"\"\"\n",
        "        return {\n",
        "            'title': self.title,\n",
        "            'author': self.author,\n",
        "            'category': self.category,\n",
        "            'year': self.year,\n",
        "            'description': self.description,\n",
        "            'relevance_score': self.relevance_score,\n",
        "            'rank': self.rank\n",
        "        }\n",
        "\n",
        "class BookRecommender:\n",
        "    \"\"\"A semantic-based book recommendation system with enhanced features.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = 'all-mpnet-base-v2'):\n",
        "        \"\"\"Initialize the book recommender with specified model.\n",
        "\n",
        "        Args:\n",
        "            model_name: Name of the sentence transformer model to use\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.model = None\n",
        "        self.book_embeddings = None\n",
        "        self.df = None\n",
        "\n",
        "        # Initialize NLP components with lazy loading\n",
        "        self._stop_words = None\n",
        "        self._lemmatizer = None\n",
        "        self._nlp_initialized = False\n",
        "\n",
        "        # Cache for query embeddings to avoid recomputing\n",
        "        self.query_cache = {}\n",
        "\n",
        "        # Maximum cache size\n",
        "        self.max_cache_size = 100\n",
        "\n",
        "        logger.info(f\"BookRecommender initialized with model: {model_name}\")\n",
        "\n",
        "    @property\n",
        "    def stop_words(self):\n",
        "        \"\"\"Lazy loading of stopwords.\"\"\"\n",
        "        if self._stop_words is None:\n",
        "            try:\n",
        "                nltk.data.find('corpora/stopwords')\n",
        "            except LookupError:\n",
        "                logger.info(\"Downloading NLTK stopwords\")\n",
        "                nltk.download('stopwords', quiet=True)\n",
        "            self._stop_words = set(stopwords.words('english'))\n",
        "        return self._stop_words\n",
        "\n",
        "    @property\n",
        "    def lemmatizer(self):\n",
        "        \"\"\"Lazy loading of lemmatizer.\"\"\"\n",
        "        if self._lemmatizer is None:\n",
        "            try:\n",
        "                nltk.data.find('corpora/wordnet')\n",
        "            except LookupError:\n",
        "                logger.info(\"Downloading NLTK wordnet\")\n",
        "                nltk.download('wordnet', quiet=True)\n",
        "                nltk.download('punkt', quiet=True)\n",
        "            self._lemmatizer = WordNetLemmatizer()\n",
        "        return self._lemmatizer\n",
        "\n",
        "    def preprocess_text(self, text: str) -> str:\n",
        "        \"\"\"Advanced text preprocessing with stopword removal and lemmatization.\n",
        "\n",
        "        Args:\n",
        "            text: Text to preprocess\n",
        "\n",
        "        Returns:\n",
        "            Preprocessed text string\n",
        "        \"\"\"\n",
        "        if not isinstance(text, str) or not text.strip():\n",
        "            return \"\"\n",
        "\n",
        "        # Convert to lowercase and remove special characters\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "\n",
        "        # Ensure punkt tokenizer is downloaded\n",
        "        try:\n",
        "            nltk.data.find('tokenizers/punkt')\n",
        "        except LookupError:\n",
        "            logger.info(\"Downloading NLTK punkt tokenizer\")\n",
        "            nltk.download('punkt', quiet=True)\n",
        "\n",
        "        # Tokenize, remove stopwords, and lemmatize\n",
        "        tokens = word_tokenize(text)\n",
        "        tokens = [self.lemmatizer.lemmatize(word) for word in tokens\n",
        "                if word not in self.stop_words and len(word) > 1]\n",
        "\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "\n",
        "    def load_model(self, folder_path: str = \"recommender_model\") -> bool:\n",
        "        \"\"\"Load a previously saved model and embeddings for inference.\n",
        "\n",
        "        Args:\n",
        "            folder_path: Path to the folder containing saved model components\n",
        "\n",
        "        Returns:\n",
        "            Boolean indicating if loading was successful\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Check if folder exists\n",
        "            if not os.path.exists(folder_path):\n",
        "                logger.error(f\"Model folder {folder_path} does not exist.\")\n",
        "                return False\n",
        "\n",
        "            # Load configuration\n",
        "            config_path = os.path.join(folder_path, \"config.pkl\")\n",
        "            if not os.path.exists(config_path):\n",
        "                logger.error(f\"Configuration file not found at {config_path}\")\n",
        "                return False\n",
        "\n",
        "            with open(config_path, 'rb') as f:\n",
        "                config = pickle.load(f)\n",
        "            self.model_name = config.get('model_name', self.model_name)\n",
        "            logger.info(f\"Loaded configuration: model_name={self.model_name}\")\n",
        "\n",
        "            # Load the sentence transformer model\n",
        "            model_path = os.path.join(folder_path, \"sentence_transformer\")\n",
        "            if os.path.exists(model_path):\n",
        "                self.model = SentenceTransformer(model_path)\n",
        "                logger.info(f\"Model loaded from {model_path}\")\n",
        "            else:\n",
        "                # Fall back to loading from HuggingFace if local model not found\n",
        "                logger.info(f\"Local model not found, loading {self.model_name} from HuggingFace\")\n",
        "                self.model = SentenceTransformer(self.model_name)\n",
        "\n",
        "            # Load book embeddings\n",
        "            embeddings_path = os.path.join(folder_path, \"book_embeddings.pkl\")\n",
        "            if not os.path.exists(embeddings_path):\n",
        "                logger.error(f\"Embeddings file not found at {embeddings_path}\")\n",
        "                return False\n",
        "\n",
        "            with open(embeddings_path, 'rb') as f:\n",
        "                self.book_embeddings = pickle.load(f)\n",
        "            logger.info(f\"Embeddings loaded: {len(self.book_embeddings)} book vectors\")\n",
        "\n",
        "            # Load the DataFrame\n",
        "            df_path = os.path.join(folder_path, \"books_data.pkl\")\n",
        "            if not os.path.exists(df_path):\n",
        "                logger.error(f\"Books data file not found at {df_path}\")\n",
        "                return False\n",
        "\n",
        "            with open(df_path, 'rb') as f:\n",
        "                self.df = pickle.load(f)\n",
        "            logger.info(f\"DataFrame loaded: {len(self.df)} books\")\n",
        "\n",
        "            # Clear cache on model reload\n",
        "            self.query_cache = {}\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading model: {str(e)}\", exc_info=True)\n",
        "            return False\n",
        "\n",
        "    def get_query_embedding(self, query: str) -> np.ndarray:\n",
        "        \"\"\"Get embedding for a query with caching.\n",
        "\n",
        "        Args:\n",
        "            query: The user's query text\n",
        "\n",
        "        Returns:\n",
        "            Embedding vector for the query\n",
        "        \"\"\"\n",
        "        # Check if query is in cache\n",
        "        if query in self.query_cache:\n",
        "            return self.query_cache[query]\n",
        "\n",
        "        # Process query and generate embedding\n",
        "        processed_query = self.preprocess_text(query)\n",
        "        query_embedding = self.model.encode([processed_query])\n",
        "\n",
        "        # Add to cache, managing cache size\n",
        "        if len(self.query_cache) >= self.max_cache_size:\n",
        "            # Remove oldest item (FIFO)\n",
        "            self.query_cache.pop(next(iter(self.query_cache)))\n",
        "\n",
        "        self.query_cache[query] = query_embedding\n",
        "        return query_embedding\n",
        "\n",
        "    def recommend_books(self, user_query: str, top_n: int = 5,\n",
        "                        include_description: bool = True,\n",
        "                        min_score: float = 0.0,\n",
        "                        filter_category: Optional[str] = None) -> List[Dict]:\n",
        "        \"\"\"Recommend books based on user query.\n",
        "\n",
        "        Args:\n",
        "            user_query: Text query from user\n",
        "            top_n: Number of recommendations to return\n",
        "            include_description: Whether to include book descriptions\n",
        "            min_score: Minimum similarity score (0-1) for recommendations\n",
        "            filter_category: Optional category to filter results\n",
        "\n",
        "        Returns:\n",
        "            List of book recommendation dictionaries\n",
        "        \"\"\"\n",
        "        if self.model is None or self.book_embeddings is None or self.df is None:\n",
        "            logger.error(\"Model not initialized. Cannot make recommendations.\")\n",
        "            return []\n",
        "\n",
        "        if not user_query.strip():\n",
        "            logger.warning(\"Empty query received\")\n",
        "            return []\n",
        "\n",
        "        logger.info(f\"Finding books similar to: '{user_query}'\")\n",
        "\n",
        "        try:\n",
        "            # Get query embedding (cached if available)\n",
        "            user_embedding = self.get_query_embedding(user_query)\n",
        "\n",
        "            # Compute similarity between query and books\n",
        "            similarities = cosine_similarity(user_embedding, self.book_embeddings)[0]\n",
        "\n",
        "            # Filter by minimum score if specified\n",
        "            valid_indices = np.where(similarities >= min_score)[0]\n",
        "            if len(valid_indices) == 0:\n",
        "                logger.info(f\"No books met the minimum similarity threshold of {min_score}\")\n",
        "                return []\n",
        "\n",
        "            # Create DataFrame with indices and scores for easier filtering\n",
        "            results_df = pd.DataFrame({\n",
        "                'index': range(len(similarities)),\n",
        "                'score': similarities\n",
        "            })\n",
        "\n",
        "            # Filter by category if specified\n",
        "            if filter_category and 'Category' in self.df.columns:\n",
        "                category_mask = self.df['Category'].str.contains(filter_category, case=False, na=False)\n",
        "                valid_indices = [i for i in valid_indices if category_mask.iloc[i]]\n",
        "                if len(valid_indices) == 0:\n",
        "                    logger.info(f\"No books found in category '{filter_category}'\")\n",
        "                    return []\n",
        "\n",
        "                # Update results DataFrame\n",
        "                results_df = results_df[results_df['index'].isin(valid_indices)]\n",
        "\n",
        "            # Sort and get top N\n",
        "            results_df = results_df.sort_values('score', ascending=False).head(top_n)\n",
        "            similar_books_idx = results_df['index'].tolist()\n",
        "\n",
        "            # Generate recommendations\n",
        "            recommendations = []\n",
        "\n",
        "            for i, idx in enumerate(similar_books_idx):\n",
        "                book_row = self.df.iloc[idx]\n",
        "\n",
        "                # Create recommendation using the dataclass\n",
        "                book_data = BookRecommendation(\n",
        "                    title=book_row.get('Title', 'Unknown Title'),\n",
        "                    author=book_row.get('Authors', 'Unknown Author'),\n",
        "                    category=book_row.get('Category', ''),\n",
        "                    year=book_row.get('Publish Date (Year)', ''),\n",
        "                    relevance_score=float(similarities[idx]),\n",
        "                    rank=i + 1\n",
        "                )\n",
        "\n",
        "                # Add description if requested\n",
        "                if include_description and 'Description' in self.df.columns:\n",
        "                    # Truncate long descriptions\n",
        "                    description = book_row['Description']\n",
        "                    if isinstance(description, str) and len(description) > 200:\n",
        "                        description = description[:197] + \"...\"\n",
        "                    book_data.description = description\n",
        "\n",
        "                recommendations.append(book_data.to_dict())\n",
        "\n",
        "            logger.info(f\"Successfully generated {len(recommendations)} recommendations\")\n",
        "            return recommendations\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating recommendations: {str(e)}\", exc_info=True)\n",
        "            return []\n",
        "\n",
        "    def get_similar_books(self, book_id: int, top_n: int = 5) -> List[Dict]:\n",
        "        \"\"\"Find books similar to a specific book in the dataset.\n",
        "\n",
        "        Args:\n",
        "            book_id: Index of the book in the dataset\n",
        "            top_n: Number of similar books to return\n",
        "\n",
        "        Returns:\n",
        "            List of similar book recommendations\n",
        "        \"\"\"\n",
        "        if self.book_embeddings is None or self.df is None:\n",
        "            logger.error(\"Model not initialized. Cannot find similar books.\")\n",
        "            return []\n",
        "\n",
        "        if book_id < 0 or book_id >= len(self.df):\n",
        "            logger.error(f\"Book ID {book_id} out of range (0-{len(self.df)-1})\")\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            # Get the book's embedding\n",
        "            book_embedding = self.book_embeddings[book_id].reshape(1, -1)\n",
        "\n",
        "            # Compute similarities between this book and all others\n",
        "            similarities = cosine_similarity(book_embedding, self.book_embeddings)[0]\n",
        "\n",
        "            # Get top N+1 (including the book itself)\n",
        "            similar_books_idx = np.argsort(similarities)[-top_n-1:][::-1]\n",
        "\n",
        "            # Remove the original book from results\n",
        "            similar_books_idx = [idx for idx in similar_books_idx if idx != book_id][:top_n]\n",
        "\n",
        "            # Format results\n",
        "            recommendations = []\n",
        "            for i, idx in enumerate(similar_books_idx):\n",
        "                book_row = self.df.iloc[idx]\n",
        "\n",
        "                book_data = BookRecommendation(\n",
        "                    title=book_row.get('Title', 'Unknown Title'),\n",
        "                    author=book_row.get('Authors', 'Unknown Author'),\n",
        "                    category=book_row.get('Category', ''),\n",
        "                    year=book_row.get('Publish Date (Year)', ''),\n",
        "                    description=book_row.get('Description', '')[:197] + \"...\" if isinstance(book_row.get('Description', ''), str) and len(book_row.get('Description', '')) > 200 else book_row.get('Description', ''),\n",
        "                    relevance_score=float(similarities[idx]),\n",
        "                    rank=i + 1\n",
        "                )\n",
        "\n",
        "                recommendations.append(book_data.to_dict())\n",
        "\n",
        "            return recommendations\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error finding similar books: {str(e)}\", exc_info=True)\n",
        "            return []\n",
        "\n",
        "    def search_by_keywords(self, keywords: Union[str, List[str]],\n",
        "                           fields: List[str] = ['Title', 'Description'],\n",
        "                           top_n: int = 10) -> List[Dict]:\n",
        "        \"\"\"Search for books by keywords in specific fields.\n",
        "\n",
        "        Args:\n",
        "            keywords: Search keywords as string or list\n",
        "            fields: DataFrame columns to search in\n",
        "            top_n: Maximum number of results to return\n",
        "\n",
        "        Returns:\n",
        "            List of matching book dictionaries\n",
        "        \"\"\"\n",
        "        if self.df is None:\n",
        "            logger.error(\"Book data not loaded. Cannot search.\")\n",
        "            return []\n",
        "\n",
        "        if not keywords:\n",
        "            return []\n",
        "\n",
        "        # Handle different keyword input formats\n",
        "        if isinstance(keywords, str):\n",
        "            keyword_list = [kw.strip().lower() for kw in keywords.split() if kw.strip()]\n",
        "        elif isinstance(keywords, list):\n",
        "            keyword_list = [kw.strip().lower() for kw in keywords if isinstance(kw, str) and kw.strip()]\n",
        "        else:\n",
        "            logger.error(f\"Invalid keywords format: {type(keywords)}\")\n",
        "            return []\n",
        "\n",
        "        if not keyword_list:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            results = []\n",
        "\n",
        "            # Create a copy of the DataFrame for search\n",
        "            search_df = self.df.copy()\n",
        "\n",
        "            # Initialize match score column\n",
        "            search_df['match_score'] = 0\n",
        "\n",
        "            # Search in each specified field\n",
        "            for field in fields:\n",
        "                if field not in search_df.columns:\n",
        "                    logger.warning(f\"Field '{field}' not found in data\")\n",
        "                    continue\n",
        "\n",
        "                # Convert field to string (if not already)\n",
        "                search_df[field] = search_df[field].astype(str)\n",
        "\n",
        "                # Calculate match scores based on keyword presence\n",
        "                for keyword in keyword_list:\n",
        "                    # Add points for each keyword match\n",
        "                    search_df['match_score'] += search_df[field].str.lower().str.contains(keyword, regex=False).astype(int)\n",
        "\n",
        "            # Sort by match score and get top results\n",
        "            matches = search_df[search_df['match_score'] > 0].sort_values('match_score', ascending=False).head(top_n)\n",
        "\n",
        "            # Format results\n",
        "            for i, (_, book) in enumerate(matches.iterrows()):\n",
        "                result = {\n",
        "                    'title': book.get('Title', 'Unknown Title'),\n",
        "                    'author': book.get('Authors', 'Unknown Author'),\n",
        "                    'category': book.get('Category', ''),\n",
        "                    'year': book.get('Publish Date (Year)', ''),\n",
        "                    'keyword_match_score': int(book['match_score']),\n",
        "                    'rank': i + 1\n",
        "                }\n",
        "\n",
        "                if 'Description' in book:\n",
        "                    description = book['Description']\n",
        "                    if isinstance(description, str) and len(description) > 200:\n",
        "                        description = description[:197] + \"...\"\n",
        "                    result['description'] = description\n",
        "\n",
        "                results.append(result)\n",
        "\n",
        "            logger.info(f\"Found {len(results)} books matching keywords: {keywords}\")\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error searching by keywords: {str(e)}\", exc_info=True)\n",
        "            return []\n",
        "\n",
        "    def save_model(self, folder_path: str = \"recommender_model\") -> bool:\n",
        "        \"\"\"Save the current model, embeddings, and data to disk.\n",
        "\n",
        "        Args:\n",
        "            folder_path: Directory to save model components\n",
        "\n",
        "        Returns:\n",
        "            Boolean indicating if saving was successful\n",
        "        \"\"\"\n",
        "        if self.model is None or self.book_embeddings is None or self.df is None:\n",
        "            logger.error(\"Model not fully initialized. Cannot save.\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            # Create directory if it doesn't exist\n",
        "            os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "            # Save configuration\n",
        "            config = {\n",
        "                'model_name': self.model_name,\n",
        "                'embedding_size': self.book_embeddings.shape[1],\n",
        "                'num_books': len(self.df),\n",
        "                'version': '2.0'\n",
        "            }\n",
        "\n",
        "            config_path = os.path.join(folder_path, \"config.pkl\")\n",
        "            with open(config_path, 'wb') as f:\n",
        "                pickle.dump(config, f)\n",
        "\n",
        "            # Save the model\n",
        "            model_path = os.path.join(folder_path, \"sentence_transformer\")\n",
        "            self.model.save(model_path)\n",
        "\n",
        "            # Save embeddings\n",
        "            embeddings_path = os.path.join(folder_path, \"book_embeddings.pkl\")\n",
        "            with open(embeddings_path, 'wb') as f:\n",
        "                pickle.dump(self.book_embeddings, f)\n",
        "\n",
        "            # Save dataframe\n",
        "            df_path = os.path.join(folder_path, \"books_data.pkl\")\n",
        "            with open(df_path, 'wb') as f:\n",
        "                pickle.dump(self.df, f)\n",
        "\n",
        "            logger.info(f\"Model successfully saved to {folder_path}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error saving model: {str(e)}\", exc_info=True)\n",
        "            return False\n",
        "\n",
        "    def train(self, books_df: pd.DataFrame,\n",
        "              text_columns: List[str] = ['Title', 'Description', 'Category' ],\n",
        "              batch_size: int = 32) -> bool:\n",
        "        \"\"\"Train the recommender on a new dataset.\n",
        "\n",
        "        Args:\n",
        "            books_df: DataFrame containing book information\n",
        "            text_columns: Columns to use for generating book embeddings\n",
        "            batch_size: Batch size for embedding generation\n",
        "\n",
        "        Returns:\n",
        "            Boolean indicating if training was successful\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Validate input data\n",
        "            if books_df.empty:\n",
        "                logger.error(\"Empty DataFrame provided for training\")\n",
        "                return False\n",
        "\n",
        "            for col in text_columns:\n",
        "                if col not in books_df.columns:\n",
        "                    logger.error(f\"Required column '{col}' not found in DataFrame\")\n",
        "                    return False\n",
        "\n",
        "            # Store the DataFrame\n",
        "            self.df = books_df.copy()\n",
        "            logger.info(f\"Training on {len(self.df)} books\")\n",
        "\n",
        "            # Initialize model if needed\n",
        "            if self.model is None:\n",
        "                logger.info(f\"Loading model: {self.model_name}\")\n",
        "                self.model = SentenceTransformer(self.model_name)\n",
        "\n",
        "            # Combine text columns for each book\n",
        "            combined_texts = []\n",
        "            for _, book in self.df.iterrows():\n",
        "                text_parts = []\n",
        "                for col in text_columns:\n",
        "                    if isinstance(book[col], str) and book[col].strip():\n",
        "                        text_parts.append(book[col])\n",
        "\n",
        "                # Concatenate with spaces between parts\n",
        "                combined_text = \" \".join(text_parts)\n",
        "                # Preprocess the combined text\n",
        "                processed_text = self.preprocess_text(combined_text)\n",
        "                combined_texts.append(processed_text)\n",
        "\n",
        "            # Generate embeddings in batches\n",
        "            logger.info(\"Generating book embeddings...\")\n",
        "            all_embeddings = []\n",
        "\n",
        "            for i in range(0, len(combined_texts), batch_size):\n",
        "                batch = combined_texts[i:i+batch_size]\n",
        "                batch_embeddings = self.model.encode(batch)\n",
        "                all_embeddings.append(batch_embeddings)\n",
        "                logger.info(f\"Processed batch {i//batch_size + 1}/{(len(combined_texts)-1)//batch_size + 1}\")\n",
        "\n",
        "            # Combine batch results\n",
        "            self.book_embeddings = np.vstack(all_embeddings)\n",
        "\n",
        "            logger.info(f\"Training complete. Generated {len(self.book_embeddings)} embeddings.\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during training: {str(e)}\", exc_info=True)\n",
        "            return False''')\n",
        "\n",
        "# Import the BookRecommender class\n",
        "from book_recommender import BookRecommender\n",
        "\n",
        "# Function to download sample books dataset\n",
        "def download_sample_books_dataset():\n",
        "    \"\"\"Downloads a sample books dataset for training the recommender system.\"\"\"\n",
        "\n",
        "    print(\"Downloading sample books dataset...\")\n",
        "\n",
        "    # Option 1: Kaggle Books Dataset (if you have Kaggle API set up)\n",
        "    try:\n",
        "        # Uncomment these lines if you have Kaggle API configured\n",
        "        # !pip install -q kaggle\n",
        "        # !mkdir -p ~/.kaggle\n",
        "        # !cp /content/kaggle.json ~/.kaggle/\n",
        "        # !chmod 600 ~/.kaggle/kaggle.json\n",
        "        # !kaggle datasets download -d jealousleopard/goodreadsbooks\n",
        "        # !unzip -q goodreadsbooks.zip\n",
        "        # books_df = pd.read_csv('books.csv')\n",
        "        # If the above doesn't work, use the fallback option below\n",
        "        raise Exception(\"Using fallback dataset\")\n",
        "    except:\n",
        "        # Option 2: Direct URL to a sample books dataset (fallback)\n",
        "        print(\"Using fallback dataset source...\")\n",
        "        url = \"https://raw.githubusercontent.com/zygmuntz/goodbooks-10k/master/books.csv\"\n",
        "        books_df = pd.read_csv(url)\n",
        "\n",
        "    # Clean and preprocess the dataset\n",
        "    # Rename columns to match the expected format\n",
        "    column_mapping = {\n",
        "        'authors': 'Authors',\n",
        "        'title': 'Title',\n",
        "        'original_publication_year': 'Publish Date (Year)',\n",
        "        'original_title': 'Description'  # Using original_title as Description for demo purposes\n",
        "    }\n",
        "\n",
        "    # Map columns and handle missing values\n",
        "    for old_col, new_col in column_mapping.items():\n",
        "        if old_col in books_df.columns:\n",
        "            books_df[new_col] = books_df[old_col]\n",
        "\n",
        "    # Add a Category column if missing\n",
        "    if 'Category' not in books_df.columns:\n",
        "        # Create simple categories based on book titles (just for demonstration)\n",
        "        def assign_category(title):\n",
        "            if not isinstance(title, str):\n",
        "                return \"Unknown\"\n",
        "            title = title.lower()\n",
        "            categories = {\n",
        "                'fiction': ['novel', 'story', 'tales', 'fiction'],\n",
        "                'fantasy': ['fantasy', 'magic', 'wizard', 'dragon', 'myth'],\n",
        "                'science fiction': ['space', 'planet', 'star', 'galaxy', 'future', 'robot'],\n",
        "                'mystery': ['mystery', 'detective', 'crime', 'thriller', 'murder'],\n",
        "                'romance': ['love', 'romance', 'heart', 'passion'],\n",
        "                'biography': ['life', 'memoir', 'biography', 'autobiography'],\n",
        "                'history': ['history', 'historical', 'century', 'war'],\n",
        "                'self-help': ['self', 'help', 'success', 'habit', 'productivity']\n",
        "            }\n",
        "\n",
        "            for category, keywords in categories.items():\n",
        "                if any(keyword in title for keyword in keywords):\n",
        "                    return category\n",
        "            return \"General\"\n",
        "\n",
        "        books_df['Category'] = books_df['Title'].apply(assign_category)\n",
        "\n",
        "    # Ensure we have necessary columns and limit to a manageable size for Colab\n",
        "    required_columns = ['Title', 'Authors', 'Category', 'Publish Date (Year)', 'Description']\n",
        "    for col in required_columns:\n",
        "        if col not in books_df.columns:\n",
        "            books_df[col] = \"\"\n",
        "\n",
        "    # Limit dataset size to avoid memory issues\n",
        "    if len(books_df) > 5000:\n",
        "        books_df = books_df.head(5000)\n",
        "\n",
        "    print(f\"Dataset prepared with {len(books_df)} books.\")\n",
        "    return books_df\n",
        "\n",
        "# Now let's run the full pipeline\n",
        "def run_book_recommender_pipeline():\n",
        "    \"\"\"Complete pipeline for training and using the book recommender system.\"\"\"\n",
        "\n",
        "    print(\"Starting Book Recommender Pipeline\")\n",
        "    print(\"==================================\")\n",
        "\n",
        "    # 1. Initialize the recommender\n",
        "    print(\"\\n1. Initializing the book recommender...\")\n",
        "    recommender = BookRecommender(model_name='all-mpnet-base-v2')\n",
        "\n",
        "    # 2. Check if a saved model exists and load it\n",
        "    model_dir = \"recommender_model\"\n",
        "    if os.path.exists(model_dir):\n",
        "        print(\"\\n2. Found existing model, loading...\")\n",
        "        success = recommender.load_model(model_dir)\n",
        "        if success:\n",
        "            print(\"Model loaded successfully!\")\n",
        "        else:\n",
        "            print(\"Failed to load model, will train a new one.\")\n",
        "    else:\n",
        "        print(\"\\n2. No existing model found, will train a new one.\")\n",
        "\n",
        "    # 3. If no model was loaded, get data and train\n",
        "    if recommender.df is None or recommender.book_embeddings is None:\n",
        "        print(\"\\n3. Preparing training data...\")\n",
        "        books_df = download_sample_books_dataset()\n",
        "\n",
        "        print(\"\\n4. Training model...\")\n",
        "        # Train with smaller batch size for Colab\n",
        "        training_success = recommender.train(\n",
        "            books_df=books_df,\n",
        "            text_columns=['Title', 'Description', 'Category'],\n",
        "            batch_size=16  # Smaller batch size for Colab\n",
        "        )\n",
        "\n",
        "        if training_success:\n",
        "            print(\"Training completed successfully!\")\n",
        "\n",
        "            # Save the trained model\n",
        "            print(\"\\n5. Saving model...\")\n",
        "            recommender.save_model(model_dir)\n",
        "            print(f\"Model saved to {model_dir}\")\n",
        "        else:\n",
        "            print(\"Training failed.\")\n",
        "            return\n",
        "\n",
        "    # 4. Display some dataset statistics\n",
        "    print(\"\\n6. Dataset Statistics:\")\n",
        "    print(f\"Number of books: {len(recommender.df)}\")\n",
        "    if 'Category' in recommender.df.columns:\n",
        "        category_counts = recommender.df['Category'].value_counts().head(10)\n",
        "        print(\"Top 10 categories:\")\n",
        "        for category, count in category_counts.items():\n",
        "            print(f\"  - {category}: {count} books\")\n",
        "\n",
        "    # 5. Test the recommender with sample queries\n",
        "    print(\"\\n7. Testing recommendation functionality...\")\n",
        "    sample_queries = [\n",
        "        \"adventure in a magical world\",\n",
        "        \"science fiction with robots\",\n",
        "        \"romantic love story\",\n",
        "        \"historical biography\"\n",
        "    ]\n",
        "\n",
        "    for query in sample_queries:\n",
        "        print(f\"\\nQuery: '{query}'\")\n",
        "        recommendations = recommender.recommend_books(query, top_n=3)\n",
        "\n",
        "        if recommendations:\n",
        "            print(\"Top 3 recommendations:\")\n",
        "            for book in recommendations:\n",
        "                print(f\"  - {book['title']} by {book['author']} ({book['category']}) - Score: {book['relevance_score']:.2f}\")\n",
        "        else:\n",
        "            print(\"No recommendations found.\")\n",
        "\n",
        "    # 6. Test keyword search functionality\n",
        "    print(\"\\n8. Testing keyword search functionality...\")\n",
        "    keywords = \"mystery detective\"\n",
        "    print(f\"Keywords: '{keywords}'\")\n",
        "    keyword_results = recommender.search_by_keywords(keywords, top_n=3)\n",
        "\n",
        "    if keyword_results:\n",
        "        print(\"Top 3 keyword matches:\")\n",
        "        for book in keyword_results:\n",
        "            print(f\"  - {book['title']} by {book['author']} - Match score: {book['keyword_match_score']}\")\n",
        "    else:\n",
        "        print(\"No keyword matches found.\")\n",
        "\n",
        "    # 7. Find similar books\n",
        "    print(\"\\n9. Testing 'similar books' functionality...\")\n",
        "    # Use the first book in the dataset as an example\n",
        "    if len(recommender.df) > 0:\n",
        "        example_book_id = 0\n",
        "        example_book = recommender.df.iloc[example_book_id]\n",
        "        print(f\"Finding books similar to: {example_book['Title']} by {example_book.get('Authors', 'Unknown')}\")\n",
        "\n",
        "        similar_books = recommender.get_similar_books(example_book_id, top_n=3)\n",
        "\n",
        "        if similar_books:\n",
        "            print(\"Top 3 similar books:\")\n",
        "            for book in similar_books:\n",
        "                print(f\"  - {book['title']} by {book['author']} - Similarity: {book['relevance_score']:.2f}\")\n",
        "        else:\n",
        "            print(\"No similar books found.\")\n",
        "\n",
        "    print(\"\\nBook Recommender Pipeline completed successfully!\")\n",
        "    return recommender\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    recommender = run_book_recommender_pipeline()\n",
        "\n",
        "    # Interactive recommendation loop\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Interactive Book Recommender\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"Enter your book preferences or 'exit' to quit\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nWhat kind of books are you looking for? \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        recommendations = recommender.recommend_books(user_input, top_n=5)\n",
        "\n",
        "        if recommendations:\n",
        "            print(\"\\nRecommended Books:\")\n",
        "            print(\"-\" * 80)\n",
        "            for book in recommendations:\n",
        "                print(f\"#{book['rank']} - {book['title']} by {book['author']}\")\n",
        "                print(f\"    Category: {book['category']}\")\n",
        "                if book.get('year'):\n",
        "                    print(f\"    Year: {book['year']}\")\n",
        "                print(f\"    Relevance Score: {book['relevance_score']:.4f}\")\n",
        "                if book.get('description'):\n",
        "                    print(f\"    Description: {book['description']}\")\n",
        "                print(\"-\" * 80)\n",
        "        else:\n",
        "            print(\"\\nNo recommendations found for your query. Try something else!\")\n",
        "\n",
        "        # Additional options\n",
        "        print(\"\\nAdditional options:\")\n",
        "        print(\"1. Get similar books to a recommendation\")\n",
        "        print(\"2. Search by keywords\")\n",
        "        print(\"3. Continue with a new query\")\n",
        "\n",
        "        option = input(\"Enter option (1-3): \")\n",
        "\n",
        "        if option == '1':\n",
        "            book_num = input(\"Enter recommendation number to find similar books: \")\n",
        "            try:\n",
        "                book_num = int(book_num)\n",
        "                if 1 <= book_num <= len(recommendations):\n",
        "                    # Get the book's ID from the original DataFrame\n",
        "                    book_title = recommendations[book_num-1]['title']\n",
        "                    book_author = recommendations[book_num-1]['author']\n",
        "\n",
        "                    # Find the book ID in the DataFrame\n",
        "                    matching_books = recommender.df[(recommender.df['Title'] == book_title) &\n",
        "                                                   (recommender.df['Authors'] == book_author)]\n",
        "\n",
        "                    if not matching_books.empty:\n",
        "                        book_id = matching_books.index[0]\n",
        "                        similar_books = recommender.get_similar_books(book_id, top_n=5)\n",
        "\n",
        "                        if similar_books:\n",
        "                            print(\"\\nSimilar Books:\")\n",
        "                            print(\"-\" * 80)\n",
        "                            for book in similar_books:\n",
        "                                print(f\"#{book['rank']} - {book['title']} by {book['author']}\")\n",
        "                                print(f\"    Category: {book['category']}\")\n",
        "                                if book.get('year'):\n",
        "                                    print(f\"    Year: {book['year']}\")\n",
        "                                print(f\"    Similarity Score: {book['relevance_score']:.4f}\")\n",
        "                                if book.get('description'):\n",
        "                                    print(f\"    Description: {book['description']}\")\n",
        "                                print(\"-\" * 80)\n",
        "                        else:\n",
        "                            print(\"No similar books found.\")\n",
        "                    else:\n",
        "                        print(\"Book not found in database.\")\n",
        "                else:\n",
        "                    print(\"Invalid recommendation number.\")\n",
        "            except ValueError:\n",
        "                print(\"Please enter a valid number.\")\n",
        "\n",
        "        elif option == '2':\n",
        "            keywords = input(\"Enter keywords to search for: \")\n",
        "            keyword_results = recommender.search_by_keywords(keywords, top_n=5)\n",
        "\n",
        "            if keyword_results:\n",
        "                print(\"\\nKeyword Search Results:\")\n",
        "                print(\"-\" * 80)\n",
        "                for book in keyword_results:\n",
        "                    print(f\"#{book['rank']} - {book['title']} by {book['author']}\")\n",
        "                    print(f\"    Category: {book['category']}\")\n",
        "                    if book.get('year'):\n",
        "                        print(f\"    Year: {book['year']}\")\n",
        "                    print(f\"    Match Score: {book['keyword_match_score']}\")\n",
        "                    if book.get('description'):\n",
        "                        print(f\"    Description: {book['description']}\")\n",
        "                    print(\"-\" * 80)\n",
        "            else:\n",
        "                print(\"No books found matching those keywords.\")\n",
        "\n",
        "        elif option == '3':\n",
        "            continue\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid option. Continuing with a new query.\")\n",
        "\n",
        "    print(\"\\nThank you for using the Book Recommender System!\")\n",
        "\n",
        "    # Optional: Visualization of recommendation scores\n",
        "    try:\n",
        "        # Create a simple bar chart of the last recommendations\n",
        "        if 'recommendations' in locals() and recommendations:\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            titles = [book['title'][:20] + '...' if len(book['title']) > 20 else book['title']\n",
        "                     for book in recommendations]\n",
        "            scores = [book['relevance_score'] for book in recommendations]\n",
        "\n",
        "            plt.bar(range(len(scores)), scores, color='skyblue')\n",
        "            plt.xticks(range(len(scores)), titles, rotation=45, ha='right')\n",
        "            plt.xlabel('Book')\n",
        "            plt.ylabel('Relevance Score')\n",
        "            plt.title(f'Recommendation Scores for Query: \"{user_input}\"')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"Could not create visualization: {str(e)}\")"
      ],
      "metadata": {
        "id": "0hBOGazL5ADV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}