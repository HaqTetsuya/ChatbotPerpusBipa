# -*- coding: utf-8 -*-
"""RusdiChatbotNLTK2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UzuHJzFX6yCe8b2L2HiP6aUHTmNChUVs
"""


# @title Import Dependencies
import os
import json
import nltk
import re
import difflib
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.tag import pos_tag
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from fuzzywuzzy import fuzz
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from flask import Flask, request, jsonify
from flask_cors import CORS  

app = Flask(__name__)
CORS(app)  

# @title Function: Config dan load data


def debug_message(msg):
    """Print debugging message"""
    print(msg)

THRESHOLD_INTENT_RECOGNITION = 100 # @param {"type":"integer"}
FUZZY_MIN_THRESHOLD = 80  # @param {"type":"integer"} Minimum acceptable fuzzy score
UNMATCHED_PENALTY = 0.5   # @param {"type":"integer"} Penalty for unmatched tokens
THRESHOLD_COURSE_MATCH = 80 # @param {"type":"integer"}
THRESHOLD_DAY_MATCH = 80 # @param {"type":"integer"}

IMPORTANT_STOP_WORDS = {'hari', 'ini', 'besok', 'kapan', 'siapa', 'dimana', 'berapa', 'sekarang'}

POS_WEIGHTS = {'NN': 2.0, 'VB': 1.5, 'JJ': 1.2, 'default': 1.0}

def load_data(file_path):
    """
    Load JSON data from a file.

    Args:
        file_path (str): The path to the JSON file.

    Returns:
        dict: Loaded JSON data.
    """
    with open(file_path, 'r') as f:
        return json.load(f)

stemmer = StemmerFactory().create_stemmer()

# @title Function: Preprocess Input
def preprocess_input(text, courses):
    """
    Preprocess user input by tokenizing, removing stopwords, stemming, and performing POS tagging.

    Args:
        text (str): The user's input text.
        courses (dict): Dictionary where each course has a list of possible names/aliases.

    Returns:
        tuple: A tuple containing stemmed tokens, POS tags, and course tokens.
    """
    if not text.strip():
        debug_message("Input kosong atau hanya berisi spasi.")
        return [], [], []

    debug_message(f"Raw input: '{text}'")

    # Remove special characters and punctuation using regex
    text = re.sub(r'[^\w\s]', '', text)  # This removes all non-alphanumeric characters except spaces

    # Prepare course matching (case-insensitive)
    course_tokens = []
    for course, aliases in courses.items():
        # Check if any alias is present in the input text
        for alias in aliases:
            # Use \b to ensure full-word matching
            if re.search(rf'\b{re.escape(alias.lower())}\b', text.lower()):
                course_tokens.append(course)  # Store the original course name
                # Replace the alias in the text with a placeholder (or just remove it)
                text = re.sub(rf'\b{re.escape(alias)}\b', '', text, flags=re.IGNORECASE)

    # Tokenize the remaining cleaned text (which no longer contains the detected courses)
    tokens = word_tokenize(text.lower())
    stop_words = set(stopwords.words('indonesian'))

    # Filter tokens (excluding course tokens)
    filtered_tokens = [word for word in tokens if word not in stop_words or word in IMPORTANT_STOP_WORDS]
    debug_message(f"Tokens setelah filtering stopwords: {filtered_tokens}")

    # Perform stemming (excluding course tokens)
    if course_tokens:
      stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens] + ["course"]
    else:
      stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]
    debug_message(f"Tokens setelah stemming: {stemmed_tokens}")
    debug_message(f"Tokens Courses: {course_tokens}")

    try:
        pos_tags = pos_tag(stemmed_tokens)
        debug_message(f"POS tags: {pos_tags}")
    except Exception as e:
        debug_message(f"Error saat POS tagging: {e}")
        pos_tags = []

    return stemmed_tokens, pos_tags, course_tokens

# @title Function: Recognize Intent

def get_pos_weight(pos):
    return POS_WEIGHTS.get(pos[:2], POS_WEIGHTS['default'])

def recognize_intent_weighted(tokens, pos_tags, intents, threshold=THRESHOLD_INTENT_RECOGNITION):
    """
    Recognize the user's intent using a refined weighted token similarity approach with support for grouped keywords.

    Args:
        tokens (list): List of stemmed tokens.
        pos_tags (list): List of POS-tagged tokens.
        intents (dict): Dictionary of intents and their keywords (supporting groups).
        threshold (int): Minimum similarity score to consider a match.

    Returns:
        str: The recognized intent or 'unknown_intent' if no match is found.
    """
    # Step 1: Filter tokens based on POS tags to focus on important tokens
    important_tokens = [token for token, pos in pos_tags if pos.startswith(('NN', 'VB', 'JJ'))]
    debug_message(f"\nImportant Tokens (filtered by POS tags): {important_tokens}")

    intent_scores = {}

    # Step 2: Loop through all intents and compute scores
    for intent, keywords in intents.items():
        total_score = 0
        unmatched_tokens = 0
        matched_keywords = 0
        total_keywords = len(keywords)

        debug_message(f"\nChecking intent: '{intent}' with keywords: {keywords}")

        # Step 3: For each keyword or group of keywords in the intent, try to match it with tokens
        for keyword_data in keywords:
            keyword = keyword_data['word']  # This can now be a list of keywords
            keyword_weight = keyword_data.get('weight', 1.0)

            keyword_score = 0  # Score for this specific keyword or group
            matched = False

            # If keyword is a list (group of synonyms), iterate through each synonym
            if isinstance(keyword, list):
                for synonym in keyword:
                    # Try to match each synonym from the group
                    matched, score = match_keyword(tokens, pos_tags, synonym, keyword_weight)
                    if matched:
                        keyword_score += score
                        matched_keywords += 1
                        break  # Stop once we match one synonym in the group

            else:
                # Handle single keyword matching
                matched, score = match_keyword(tokens, pos_tags, keyword, keyword_weight)
                if matched:
                    keyword_score += score
                    matched_keywords += 1

            # Step 5: If no match is found for a keyword, increase unmatched tokens
            if not matched:
                unmatched_tokens += 1
                debug_message(f"  - Keyword '{keyword}' was not matched by any token or phrase\n")

            # Add the keyword score to the total score for this intent
            total_score += keyword_score

        # Step 6: Apply a penalty for unmatched important tokens (reduce penalty weight)
        penalty = unmatched_tokens * UNMATCHED_PENALTY * 0.5
        total_score -= penalty
        debug_message(f"Penalty for unmatched tokens: {penalty}")

        # Step 7: Normalize the total score based on matched vs total keywords
        if matched_keywords > 0:
            total_score /= total_keywords
            debug_message(f"Normalized total score for intent '{intent}': {total_score}")

        # Step 8: Save the score for this intent
        intent_scores[intent] = total_score
        debug_message(f"Final score for intent '{intent}': {total_score} (after penalty and normalization)\n")

    # Step 9: Debug the summary of all intent scores
    debug_message("Summary of intent scores:")
    for intent, score in intent_scores.items():
        debug_message(f"  - {intent}: {score}")

    # Step 10: Determine the best intent or return 'unknown_intent'
    if not intent_scores or max(intent_scores.values()) < threshold:
        debug_message("No intent exceeded the threshold. Returning 'unknown_intent'.\n")
        return 'unknown_intent'

    best_intent = max(intent_scores, key=intent_scores.get)
    debug_message(f"\nIntent with the highest score: {best_intent}")

    return best_intent

def match_keyword(tokens, pos_tags, keyword, keyword_weight):
    """
    Match a keyword or phrase to the token list and return whether it matched and the score.

    Args:
        tokens (list): List of tokens from the user input.
        pos_tags (list): List of POS-tagged tokens.
        keyword (str): The keyword or synonym to match against.
        keyword_weight (float): Weight assigned to the keyword for scoring.

    Returns:
        (bool, float): A tuple of whether it matched and the calculated score.
    """
    keyword_score = 0
    matched = False

    for i in range(len(tokens) - len(keyword.split()) + 1):
        token_phrase = " ".join(tokens[i:i + len(keyword.split())])

        # Fuzzy matching with a minimum similarity threshold
        similarity_score = fuzz.ratio(token_phrase, keyword)
        if similarity_score >= FUZZY_MIN_THRESHOLD:
            # Check if i is within the bounds of pos_tags
            if i < len(pos_tags):
                pos_weight = get_pos_weight(pos_tags[i][1])
            else:
                pos_weight = POS_WEIGHTS['default'] # Assign default weight if pos_tags is empty or i out of range

            score = similarity_score * pos_weight * keyword_weight
            matched = True
            debug_message(f"  - Token phrase: '{token_phrase}' matched with keyword: '{keyword}'")
            debug_message(f"    Fuzzy score: {similarity_score}")
            debug_message(f"    POS weight: {pos_weight}")
            debug_message(f"    Keyword weight: {keyword_weight}")
            debug_message(f"    Calculated score for this match: {score}\n")
            return matched, score

    return matched, keyword_score

# @title Function: Match Course
def match_course(course_tokens, courses):
    """
    Match the user's course tokens to a course using fuzzy string matching and course aliases.

    Args:
        course_tokens (list): List of extracted course tokens from user input.
        course_aliases (dict): Dictionary where each course has a list of possible names/aliases.

    Returns:
        list: A list of matched courses, or ['none'] if no match is found.
    """
    debug_message(f"Course tokens: {course_tokens}\n")
    matched_courses = []

    # Start matching process
    for course, aliases in courses.items():
        debug_message(f"\nChecking course: '{course}' with aliases {aliases}")
        course_matched = False

        # Compare each course token with course aliases
        for token in course_tokens:
            for alias in aliases:
                match_ratio = fuzz.ratio(token.lower(), alias.lower())  # Ensure case-insensitivity
                debug_message(f"  - Matching token '{token}' with alias '{alias}'")
                debug_message(f"    Fuzzy match ratio: {match_ratio}")

                if match_ratio > THRESHOLD_COURSE_MATCH:
                    debug_message(f"    -> Match found with alias '{alias}'! (Match ratio: {match_ratio})")
                    matched_courses.append(course)  # Add the original course name
                    course_matched = True
                    break
            if course_matched:
                break

        if course_matched:
            debug_message(f"  -> Course '{course}' matched based on token '{token}'\n")
        else:
            debug_message(f"  -> No tokens matched for course '{course}'\n")

    # Provide final results
    if matched_courses:
        debug_message(f"\nMatched courses: {matched_courses}\n")
        return matched_courses
    else:
        debug_message("\nNo matches found. Returning 'none'.\n")
        return 
    
# @title Function: Match Day
def match_day(tokens, days):
    """
    Match the user's input to a day using fuzzy string matching.

    Args:
        input_text (str): The user's input text.
        days (list): List of days to match (e.g., ['Senin', 'Selasa', 'Hari Ini', ...]).

    Returns:
        str: The matched day or 'unknown' if no match is found.
    """
    for day in days:
        for token in tokens:
            match_ratio = fuzz.ratio(token, day.lower())  # Perform fuzzy matching
            debug_message(f"  - Matching token '{token}' with day '{day}'")
            debug_message(f"    Fuzzy match ratio: {match_ratio}")
            if match_ratio > THRESHOLD_DAY_MATCH:  # Adjust the threshold as needed
                matched_day = day  # If a match is found, set the matched day
                print(f"Hari terdeteksi: {matched_day}")  # Show detected day
                return matched_day

    # If no match is found, return 'unknown'
    return 

# @title Function: Main Loop
@app.route('/analyze', methods=['POST'])
def analyze():
    # Load data from JSON
    data = load_data('data.json')
    intents = data['intents']
    courses = data['courses']
    days = [day.lower() for day in data['day']]  # Normalisasi hari ke lowercase

    # Intent relevan untuk pencocokan course dan hari
    relevant_course_intents = ["jadwal_mata_kuliah", "nilai_mata_kuliah", "dosen_mata_kuliah", "ruang_kelas"]
    relevant_day_intents = ["jadwal_hari"]

    # Ambil input user dari request JSON
    data_input = request.get_json()
    input_text = data_input.get('text', '')  # FIXED: Ambil dari data_input

    debug_message(f"\nUser input received: '{input_text}'")

    # Preprocessing input
    tokens, pos_tags, course_tokens = preprocess_input(input_text, courses)
    debug_message(f"Preprocessed tokens: {tokens}")

    # Intent Recognition
    intent = recognize_intent_weighted(tokens, pos_tags, intents)

    response = {"intent": intent, "course": None, "day": None}  # Default response

    # Jika intent terkait dengan mata kuliah
    if intent in relevant_course_intents:
        matched_courses = match_course(course_tokens, courses)
        response["course"] = matched_courses
        debug_message(f"Intent terdeteksi: {intent}")
        debug_message(f"Mata kuliah terdeteksi: {matched_courses}\n")

    # Jika intent terkait dengan hari
    elif intent in relevant_day_intents:
        matched_day = match_day(tokens, days)
        response["day"] = matched_day
        debug_message(f"Intent terdeteksi: {intent}")
        debug_message(f"Hari terdeteksi: {matched_day}\n")

    else:
        debug_message(f"Intent terdeteksi: {intent}")
        debug_message(f"Intent {intent} tidak relevan untuk pencocokan mata kuliah atau hari.\n")

    return jsonify(response)  # Mengembalikan response sebagai JSON


# @title Run the Program
if __name__ == '__main__':
    app.run(debug=True)

