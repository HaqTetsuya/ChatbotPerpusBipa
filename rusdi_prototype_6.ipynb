{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HaqTetsuya/rusdi-prototype-1/blob/main/rusdi_prototype_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baq_vbCGocRh",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title download dependancy\n",
        "# Cell 1: Instalasi library yang diperlukan\n",
        "!pip install transformers torch pandas scikit-learn matplotlib seaborn tqdm deep-translator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-8eolkt8hn4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86eb2b41-365f-4e9b-959e-06e64683470d",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'rusdi-prototype-1'...\n",
            "remote: Enumerating objects: 536, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 536 (delta 12), reused 9 (delta 3), pack-reused 502 (from 1)\u001b[K\n",
            "Receiving objects: 100% (536/536), 29.44 MiB | 16.16 MiB/s, done.\n",
            "Resolving deltas: 100% (267/267), done.\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# @title import dependency, load drive, and github {\"form-width\":\"20%\"}\n",
        "!git clone https://github.com/HaqTetsuya/rusdi-prototype-1.git\n",
        "\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import seaborn as sns\n",
        "from google.colab import drive, files\n",
        "from plotly.subplots import make_subplots\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
        "#from tqdm import tqdm\n",
        "from tqdm.auto import tqdm  # If you need both tqdm and tqdm.auto\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "FName = \"RusdiIntent\" #@param {type:\"string\"}\n",
        "\n",
        "# Update MODEL_SAVE_PATH with user input\n",
        "MODEL_SAVE_PATH = f\"/content/drive/MyDrive/{FName}\"\n",
        "\n",
        "\n",
        "# Cell 4: Kelas Dataset untuk IndoBERT\n",
        "class IntentDataset(Dataset):\n",
        "    \"\"\"Dataset untuk klasifikasi intent dengan IndoBERT\"\"\"\n",
        "\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Convert dict of tensors to flat tensors\n",
        "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
        "        item['labels'] = torch.tensor(label)\n",
        "\n",
        "        return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qqQ8ahtVzC6r",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title load data\n",
        "\n",
        "def load_csv_data(csv_path, label_encoder=None, show_distribution=False):\n",
        "    \"\"\"Memuat data intent dari file CSV. Bisa untuk train/test tanpa split.\"\"\"\n",
        "    print(f\"\\nMemuat data dari: {csv_path}\")\n",
        "\n",
        "    if not os.path.exists(csv_path):\n",
        "        raise FileNotFoundError(f\"File tidak ditemukan: {csv_path}\")\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    if 'text' not in df.columns or 'intent' not in df.columns:\n",
        "        raise ValueError(\"Kolom 'text' dan 'intent' harus ada di CSV\")\n",
        "\n",
        "    if label_encoder is None:\n",
        "        label_encoder = LabelEncoder()\n",
        "        df['intent_encoded'] = label_encoder.fit_transform(df['intent'])\n",
        "        intent_classes = label_encoder.classes_\n",
        "        print(f\"Label encoder baru dibuat dari data {csv_path}\")\n",
        "    else:\n",
        "        df['intent_encoded'] = label_encoder.transform(df['intent'])\n",
        "        intent_classes = label_encoder.classes_\n",
        "        print(f\"Menggunakan label encoder yang sudah ada\")\n",
        "\n",
        "    if show_distribution:\n",
        "        intent_counts = df['intent'].value_counts()\n",
        "        print(\"\\nDistribusi intent:\")\n",
        "        for intent, count in intent_counts.items():\n",
        "            print(f\"  {intent}: {count}\")\n",
        "\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        sns.barplot(x=intent_counts.index, y=intent_counts.values, palette=\"viridis\")\n",
        "        plt.xlabel(\"Intent\")\n",
        "        plt.ylabel(\"Jumlah Sampel\")\n",
        "        plt.title(\"Distribusi Intent\")\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.pie(intent_counts, labels=intent_counts.index, autopct='%1.1f%%', colors=sns.color_palette(\"viridis\", len(intent_counts)))\n",
        "        plt.title(\"Proporsi Intent\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    texts = df['text'].values\n",
        "    labels = df['intent_encoded'].values\n",
        "\n",
        "    return texts, labels, intent_classes, label_encoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "VMP8y1EczkWo"
      },
      "outputs": [],
      "source": [
        "# @title  setup model IndoBERT\n",
        "def setup_indobert_for_intent(num_labels):\n",
        "    \"\"\"Load model IndoBERT untuk klasifikasi intent\"\"\"\n",
        "\n",
        "    print(\"Memuat model IndoBERT...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        \"indobenchmark/indobert-base-p1\",\n",
        "        num_labels=num_labels\n",
        "    )\n",
        "    print(\"Model berhasil dimuat\")\n",
        "\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title OOD\n",
        "def enhanced_calibrate_ood(model, tokenizer, val_dataloader, save_path, temperature=1.0, percentile=70, margin=0.1):\n",
        "    \"\"\"\n",
        "    Calibrate and save OOD thresholds with adjustable tolerance\n",
        "    \"\"\"\n",
        "    print(\"Calibrating threshold for OOD detection...\")\n",
        "    thresholds = calibrate_ood_detection(model, tokenizer, val_dataloader,\n",
        "                                        temperature=temperature,\n",
        "                                        percentile=percentile,\n",
        "                                        margin=margin)\n",
        "\n",
        "    print(f\"Energy threshold: {thresholds['energy_threshold']:.4f}\")\n",
        "    print(f\"MSP threshold: {thresholds['msp_threshold']:.4f}\")\n",
        "\n",
        "    # Save thresholds\n",
        "    save_ood_thresholds(thresholds, save_path)\n",
        "\n",
        "    return thresholds\n",
        "\n",
        "def calibrate_ood_detection(model, tokenizer, dataloader, temperature=1.0, percentile=70, margin=0.1):\n",
        "    \"\"\"\n",
        "    Calibrate thresholds for OOD detection using in-distribution data\n",
        "    with Energy-based and MSP (Maximum Softmax Probability) methods\n",
        "    \"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "\n",
        "    # For Energy method and MSP method\n",
        "    energy_scores = []\n",
        "    msp_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Calibrating OOD detection\"):\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            # Energy score (higher values for OOD)\n",
        "            energy = -temperature * torch.logsumexp(logits / temperature, dim=1)\n",
        "            energy_scores.extend(energy.cpu().numpy())\n",
        "\n",
        "            # MSP score (lower values for OOD)\n",
        "            softmax_probs = F.softmax(logits, dim=1)\n",
        "            max_probs, _ = torch.max(softmax_probs, dim=1)\n",
        "            msp_scores.extend(max_probs.cpu().numpy())\n",
        "\n",
        "    # Calculate threshold for Energy with margin (make more tolerant)\n",
        "    base_energy_threshold = np.percentile(energy_scores, percentile)\n",
        "    # Apply margin to make more tolerant (increase threshold)\n",
        "    energy_threshold = base_energy_threshold * (1 + margin)\n",
        "\n",
        "    # Calculate threshold for MSP with margin\n",
        "    base_msp_threshold = np.percentile(msp_scores, 100 - percentile)\n",
        "    # Apply margin to make more tolerant (decrease threshold)\n",
        "    msp_threshold = base_msp_threshold * (1 - margin)\n",
        "\n",
        "    return {\n",
        "        \"energy_threshold\": float(energy_threshold),\n",
        "        \"msp_threshold\": float(msp_threshold)\n",
        "    }\n",
        "\n",
        "def predict_with_ood_detection(model, tokenizer, text, thresholds, temperature=1.0, tolerance_factor=1.0):\n",
        "    \"\"\"\n",
        "    Predict with adjustable OOD detection tolerance\n",
        "    \"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "\n",
        "    # Apply the tolerance factor to thresholds\n",
        "    energy_threshold = thresholds[\"energy_threshold\"] * tolerance_factor\n",
        "    msp_threshold = thresholds[\"msp_threshold\"] / tolerance_factor if thresholds[\"msp_threshold\"] else None\n",
        "\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Energy score\n",
        "        energy = -temperature * torch.logsumexp(logits / temperature, dim=1)\n",
        "        energy_score = energy.item()\n",
        "\n",
        "        # MSP score\n",
        "        softmax_probs = F.softmax(logits, dim=1)\n",
        "        max_probs, predicted_class = torch.max(softmax_probs, dim=1)\n",
        "        msp_score = max_probs.item()\n",
        "\n",
        "        # OOD detection\n",
        "        is_ood_energy = energy_score > energy_threshold\n",
        "        is_ood_msp = msp_score < msp_threshold if msp_threshold else False\n",
        "\n",
        "        # Combined OOD detection (can adjust this logic for tolerance)\n",
        "        is_ood = is_ood_energy  # You can use different combinations\n",
        "\n",
        "        return {\n",
        "            \"prediction\": predicted_class.item(),\n",
        "            \"confidence\": msp_score,\n",
        "            \"energy_score\": energy_score,\n",
        "            \"is_ood\": is_ood,\n",
        "            \"is_ood_energy\": is_ood_energy,\n",
        "            \"is_ood_msp\": is_ood_msp\n",
        "        }\n",
        "\n",
        "def save_ood_thresholds(thresholds, save_path):\n",
        "    \"\"\"\n",
        "    Save OOD thresholds to JSON file\n",
        "    \"\"\"\n",
        "    threshold_file = os.path.join(save_path, \"ood_thresholds.json\")\n",
        "    with open(threshold_file, 'w') as f:\n",
        "        json.dump(thresholds, f, indent=4)\n",
        "    print(f\"OOD thresholds saved at {threshold_file}\")\n",
        "    return threshold_file\n",
        "\n",
        "def load_ood_thresholds(model_path):\n",
        "    \"\"\"\n",
        "    Load OOD thresholds from JSON file\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(os.path.join(model_path, \"ood_thresholds.json\"), 'r') as f:\n",
        "            thresholds = json.load(f)\n",
        "            return thresholds\n",
        "    except FileNotFoundError:\n",
        "        try:\n",
        "            with open(os.path.join(model_path, \"ood_threshold.json\"), 'r') as f:\n",
        "                threshold_data = json.load(f)\n",
        "                return {\n",
        "                    \"energy_threshold\": threshold_data[\"energy_threshold\"],\n",
        "                    \"msp_threshold\": None\n",
        "                }\n",
        "        except FileNotFoundError:\n",
        "            print(\"Warning: OOD threshold files not found. Using default thresholds.\")\n",
        "            return {\n",
        "                \"energy_threshold\": 0.0,\n",
        "                \"msp_threshold\": 0.5\n",
        "            }"
      ],
      "metadata": {
        "id": "APTL6Ht96u9Q",
        "cellView": "form"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fNCPJ3uFzp2V"
      },
      "outputs": [],
      "source": [
        "#@title Training function\n",
        "\n",
        "def train_intent_classifier(model, tokenizer, train_texts, train_labels, val_texts, val_labels,\n",
        "                           batch_size=16, epochs=10, learning_rate=2e-5, weight_decay=0.01,\n",
        "                           save_path=MODEL_SAVE_PATH, use_class_weights=True, patience=3, class_names=None):\n",
        "    \"\"\"\n",
        "    Melatih model IndoBERT untuk klasifikasi intent dengan perbaikan:\n",
        "    - Enhanced visualization (interactive and static)\n",
        "    - Per-class metric tracking\n",
        "    - Confusion matrix generation\n",
        "    - Batch-level metric tracking\n",
        "    - Learning rate visualization\n",
        "    \"\"\"\n",
        "\n",
        "    # Persiapkan dataset\n",
        "    print(\"Menyiapkan dataset...\")\n",
        "    train_dataset = IntentDataset(train_texts, train_labels, tokenizer)\n",
        "    val_dataset = IntentDataset(val_texts, val_labels, tokenizer)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Compute class weights if needed\n",
        "    class_weights = None\n",
        "    if use_class_weights:\n",
        "        unique_classes = np.unique(train_labels)\n",
        "        weights = compute_class_weight(\n",
        "            class_weight='balanced',\n",
        "            classes=unique_classes,\n",
        "            y=train_labels\n",
        "        )\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        class_weights = torch.FloatTensor(weights).to(device)\n",
        "        print(f\"Menggunakan class weights: {weights}\")\n",
        "\n",
        "    # Optimizer dengan weight decay untuk regularisasi\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    # Scheduler dengan warmup\n",
        "    num_training_steps = len(train_dataloader) * epochs\n",
        "    num_warmup_steps = int(0.1 * num_training_steps)  # 10% warmup\n",
        "    scheduler = get_scheduler(\"cosine\", optimizer=optimizer,\n",
        "                             num_warmup_steps=num_warmup_steps,\n",
        "                             num_training_steps=num_training_steps)\n",
        "\n",
        "    # Cek untuk GPU\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Menggunakan device: {device}\")\n",
        "    model.to(device)\n",
        "    print(f\"Mulai pelatihan model...\")\n",
        "    print(f\"Total epoch: {epochs}, batch size: {batch_size}, learning rate: {learning_rate}, weight decay: {weight_decay}\")\n",
        "\n",
        "    # Create loss function with class weights if needed\n",
        "    loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights) if class_weights is not None else torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Initialize training history\n",
        "    history = initialize_training_history()\n",
        "    best_val_loss = float('inf')\n",
        "    counter = 0  # Counter for early stopping\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        train_loss = run_training_epoch(model, device, train_dataloader, optimizer,\n",
        "                                        scheduler, loss_fn, epoch, epochs, history)\n",
        "        history['train_loss'].append(train_loss)\n",
        "\n",
        "        # Validation phase\n",
        "        val_metrics = run_validation_epoch(model, device, val_dataloader, loss_fn, epoch, epochs)\n",
        "        update_history_with_validation_metrics(history, val_metrics)\n",
        "\n",
        "        # Update per-class metrics if class names are provided\n",
        "        if class_names is not None:\n",
        "            update_per_class_metrics(history, val_metrics['all_labels'], val_metrics['all_preds'])\n",
        "\n",
        "            # Generate confusion matrix for this epoch\n",
        "            plot_confusion_matrix(val_metrics['all_labels'], val_metrics['all_preds'], class_names, epoch, save_path)\n",
        "\n",
        "        # Print detailed metrics\n",
        "        print_epoch_metrics(epoch, epochs, train_loss, val_metrics)\n",
        "\n",
        "        # Early stopping and model saving logic\n",
        "        counter = handle_early_stopping(model, tokenizer, val_metrics['avg_val_loss'],\n",
        "                                      best_val_loss, counter, patience,\n",
        "                                      save_path, val_metrics['all_labels'],\n",
        "                                      val_metrics['all_preds'])\n",
        "\n",
        "        if counter >= patience:\n",
        "            print(f\"Early stopping triggered setelah {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "        # Update best validation loss if improved\n",
        "        if val_metrics['avg_val_loss'] < best_val_loss:\n",
        "            best_val_loss = val_metrics['avg_val_loss']\n",
        "\n",
        "    print(f\"Pelatihan selesai! Model terbaik disimpan di {save_path}\")\n",
        "\n",
        "    # Generate enhanced visualizations\n",
        "    enhanced_plot_training_results(history, save_path, class_names=class_names)\n",
        "\n",
        "    # Simpan history ke file JSON\n",
        "    save_enhanced_history(history, save_path)\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def initialize_training_history():\n",
        "    \"\"\"Initialize the training history dictionary with all required keys\"\"\"\n",
        "    return {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'val_accuracy': [],\n",
        "        'val_f1': [],\n",
        "        'val_precision': [],\n",
        "        'val_recall': [],\n",
        "        'batch_metrics': {\n",
        "            'iteration': [],\n",
        "            'loss': [],\n",
        "            'epoch': [],\n",
        "            'progress': [],\n",
        "            'learning_rates': []\n",
        "        },\n",
        "        'class_f1': [],  # Per-class F1 scores\n",
        "        'class_precision': [],  # Per-class precision\n",
        "        'class_recall': []  # Per-class recall\n",
        "    }\n",
        "\n",
        "def run_training_epoch(model, device, train_dataloader, optimizer, scheduler, loss_fn, epoch, epochs, history):\n",
        "    \"\"\"Run a single training epoch and return average loss\"\"\"\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    print(f\"\\nEpoch {epoch+1}/{epochs} - Training dimulai...\")\n",
        "    progress_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader),\n",
        "                      desc=f\"Epoch {epoch+1}/{epochs} [Training]\", leave=False)\n",
        "\n",
        "    for batch_idx, batch in progress_bar:\n",
        "        try:\n",
        "            # Pindahkan batch ke device\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Forward pass with custom loss function\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(**inputs)\n",
        "            loss = loss_fn(outputs.logits, labels)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Track batch-level metrics\n",
        "            update_batch_metrics(history, batch_idx, loss.item(), epoch, len(train_dataloader), optimizer)\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            if \"out of memory\" in str(e):\n",
        "                print(\"Peringatan: Kehabisan memori! Membersihkan cache...\")\n",
        "                torch.cuda.empty_cache()\n",
        "                continue\n",
        "            else:\n",
        "                raise e\n",
        "\n",
        "    return train_loss / len(train_dataloader)\n",
        "\n",
        "def update_batch_metrics(history, batch_idx, loss_item, epoch, iterations_per_epoch, optimizer):\n",
        "    \"\"\"Update history with batch-level metrics\"\"\"\n",
        "    global_iteration = epoch * iterations_per_epoch + batch_idx\n",
        "    progress = (epoch + (batch_idx / iterations_per_epoch)) * 100\n",
        "\n",
        "    history['batch_metrics']['iteration'].append(global_iteration)\n",
        "    history['batch_metrics']['loss'].append(loss_item)\n",
        "    history['batch_metrics']['epoch'].append(epoch)\n",
        "    history['batch_metrics']['progress'].append(progress)\n",
        "    history['batch_metrics']['learning_rates'].append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "def run_validation_epoch(model, device, val_dataloader, loss_fn, epoch, epochs):\n",
        "    \"\"\"Run a single validation epoch and return metrics\"\"\"\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Validasi dimulai...\")\n",
        "    progress_bar = tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Validation]\", leave=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in progress_bar:\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            loss = loss_fn(outputs.logits, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            predictions = torch.argmax(outputs.logits, dim=1)\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            # Collect predictions and labels for metrics\n",
        "            all_preds.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
        "\n",
        "    # Calculate metrics\n",
        "    avg_val_loss = val_loss / len(val_dataloader)\n",
        "    accuracy = correct / total\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "\n",
        "    return {\n",
        "        'avg_val_loss': avg_val_loss,\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'all_preds': all_preds,\n",
        "        'all_labels': all_labels\n",
        "    }\n",
        "\n",
        "def update_history_with_validation_metrics(history, val_metrics):\n",
        "    \"\"\"Update history with validation metrics\"\"\"\n",
        "    history['val_loss'].append(val_metrics['avg_val_loss'])\n",
        "    history['val_accuracy'].append(val_metrics['accuracy'])\n",
        "    history['val_f1'].append(val_metrics['f1'])\n",
        "    history['val_precision'].append(val_metrics['precision'])\n",
        "    history['val_recall'].append(val_metrics['recall'])\n",
        "\n",
        "def update_per_class_metrics(history, all_labels, all_preds):\n",
        "    \"\"\"Update per-class metrics in history\"\"\"\n",
        "    class_f1 = f1_score(all_labels, all_preds, average=None, zero_division=0)\n",
        "    class_precision = precision_score(all_labels, all_preds, average=None, zero_division=0)\n",
        "    class_recall = recall_score(all_labels, all_preds, average=None, zero_division=0)\n",
        "\n",
        "    history['class_f1'].append(class_f1.tolist())\n",
        "    history['class_precision'].append(class_precision.tolist())\n",
        "    history['class_recall'].append(class_recall.tolist())\n",
        "\n",
        "def print_epoch_metrics(epoch, epochs, avg_train_loss, val_metrics):\n",
        "    \"\"\"Print detailed metrics for an epoch\"\"\"\n",
        "    print(f\"Epoch {epoch+1}/{epochs}:\")\n",
        "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
        "    print(f\"  Val Loss: {val_metrics['avg_val_loss']:.4f}, Val Accuracy: {val_metrics['accuracy']*100:.2f}%\")\n",
        "    print(f\"  Val F1: {val_metrics['f1']:.4f}, Val Precision: {val_metrics['precision']:.4f}, Val Recall: {val_metrics['recall']:.4f}\")\n",
        "\n",
        "def handle_early_stopping(model, tokenizer, avg_val_loss, best_val_loss, counter, patience, save_path, all_labels, all_preds):\n",
        "    \"\"\"Handle early stopping logic and model saving\"\"\"\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        counter = 0  # Reset early stopping counter\n",
        "\n",
        "        if not os.path.exists(save_path):\n",
        "            os.makedirs(save_path)\n",
        "\n",
        "        print(f\"Menyimpan model terbaik ke {save_path}\")\n",
        "        model.save_pretrained(save_path)\n",
        "        tokenizer.save_pretrained(save_path)\n",
        "\n",
        "        # Save classification report for best model\n",
        "        report = classification_report(all_labels, all_preds, output_dict=True)\n",
        "        with open(os.path.join(save_path, \"classification_report.json\"), 'w') as f:\n",
        "            json.dump(report, f, indent=4)\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f\"Validation loss tidak membaik. Early stopping counter: {counter}/{patience}\")\n",
        "\n",
        "    return counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pWFt6WSazwgT",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Evaluation\n",
        "def evaluate_model_enhanced(model, tokenizer, val_texts, val_labels, intent_classes, save_path):\n",
        "    \"\"\"Enhanced model evaluation with better visualizations\"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    # Prepare dataset and dataloader\n",
        "    val_dataset = IntentDataset(val_texts, val_labels, tokenizer)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Evaluation loop\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_dataloader, desc=\"Evaluasi Model\"):\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            predictions = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "            all_preds.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Generate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Generate classification report\n",
        "    report = classification_report(all_labels, all_preds,\n",
        "                                 target_names=intent_classes,\n",
        "                                 output_dict=True)\n",
        "\n",
        "    # Create dataframe for visualization\n",
        "    report_df = pd.DataFrame(report).transpose()\n",
        "    report_df = report_df.round(3)\n",
        "\n",
        "    # Filter for class metrics only (exclude summary rows)\n",
        "    class_df = report_df.loc[intent_classes]\n",
        "    metrics = ['precision', 'recall', 'f1-score']\n",
        "\n",
        "    # Save classification metrics as CSV\n",
        "    report_df.to_csv(os.path.join(save_path, \"classification_report.csv\"))\n",
        "\n",
        "    # Create visualizations\n",
        "    create_static_visualizations(cm, class_df, metrics, intent_classes, save_path)\n",
        "    create_interactive_visualizations(cm, class_df, metrics, intent_classes, save_path)\n",
        "\n",
        "    # Print report summary\n",
        "    print(\"\\nModel Evaluation Report:\")\n",
        "    print(f\"Overall Accuracy: {report['accuracy']:.4f}\")\n",
        "    print(f\"Macro F1-score: {report['macro avg']['f1-score']:.4f}\")\n",
        "    print(f\"Weighted F1-score: {report['weighted avg']['f1-score']:.4f}\")\n",
        "\n",
        "    return report, cm\n",
        "\n",
        "def create_static_visualizations(cm, class_df, metrics, intent_classes, save_path):\n",
        "    \"\"\"Create and save static matplotlib visualizations\"\"\"\n",
        "    # Confusion matrix\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "               xticklabels=intent_classes, yticklabels=intent_classes)\n",
        "    plt.title('Confusion Matrix - Final Model')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_path, \"final_confusion_matrix.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    # Classification report heatmap\n",
        "    plt.figure(figsize=(12, len(intent_classes)*0.5 + 3))\n",
        "    sns.heatmap(class_df[metrics], annot=True, cmap='YlGnBu', fmt='.3f',\n",
        "               yticklabels=intent_classes, cbar=True)\n",
        "    plt.title('Performance Metrics by Intent Class')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_path, \"class_performance_metrics.png\"))\n",
        "    plt.close()\n",
        "\n",
        "def create_interactive_visualizations(cm, class_df, metrics, intent_classes, save_path):\n",
        "    \"\"\"Create and save interactive plotly visualizations\"\"\"\n",
        "    # Interactive confusion matrix\n",
        "    fig_cm = px.imshow(cm,\n",
        "                   labels=dict(x=\"Predicted Label\", y=\"True Label\", color=\"Count\"),\n",
        "                   x=intent_classes, y=intent_classes,\n",
        "                   text_auto=True,\n",
        "                   color_continuous_scale='Blues')\n",
        "\n",
        "    fig_cm.update_layout(\n",
        "        title='Confusion Matrix (Interactive)',\n",
        "        width=900,\n",
        "        height=800\n",
        "    )\n",
        "    fig_cm.write_html(os.path.join(save_path, \"interactive_confusion_matrix.html\"))\n",
        "\n",
        "    # Interactive performance metrics\n",
        "    fig_perf = px.imshow(class_df[metrics],\n",
        "                   labels=dict(x=\"Metric\", y=\"Intent Class\", color=\"Score\"),\n",
        "                   x=metrics, y=intent_classes,\n",
        "                   text_auto=True,\n",
        "                   color_continuous_scale='YlGnBu')\n",
        "\n",
        "    fig_perf.update_layout(\n",
        "        title='Performance Metrics by Intent Class (Interactive)',\n",
        "        width=800,\n",
        "        height=max(400, len(intent_classes)*30)\n",
        "    )\n",
        "    fig_perf.write_html(os.path.join(save_path, \"interactive_class_performance.html\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Upp9G0xn0fN7",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title PredictionIntent\n",
        "def predict_intent_with_enhanced_ood(text, model, tokenizer, intent_classes,\n",
        "                                     energy_threshold, msp_threshold=None,\n",
        "                                     temperature=1.0, method='combined',\n",
        "                                     label_encoder=None, device=None,\n",
        "                                     return_logits=False):\n",
        "    \"\"\"\n",
        "    Memprediksi intent dari teks input dengan deteksi Out-of-Distribution yang ditingkatkan\n",
        "\n",
        "    Args:\n",
        "        text: Teks input untuk diprediksi\n",
        "        model: Model yang sudah dilatih\n",
        "        tokenizer: Tokenizer untuk model\n",
        "        intent_classes: List nama intent\n",
        "        energy_threshold: Threshold untuk energy-based OOD detection\n",
        "        msp_threshold: Threshold untuk MSP-based OOD detection\n",
        "        temperature: Parameter temperature untuk energy\n",
        "        method: Metode deteksi OOD - 'energy', 'msp', atau 'combined'\n",
        "        label_encoder: Label encoder untuk intent classes\n",
        "        device: Device untuk inference\n",
        "        return_logits: Jika True, mengembalikan logits asli\n",
        "\n",
        "    Returns:\n",
        "        dict: Hasil prediksi dengan detail OOD detection\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    if isinstance(text, str):\n",
        "        text = [text]  # Convert single text to list\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenisasi input\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # List untuk menyimpan hasil setiap input\n",
        "    results = []\n",
        "\n",
        "    # Prediksi\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Hitung energy score: -T*log(sum(exp(logits/T)))\n",
        "        energy = -temperature * torch.logsumexp(logits / temperature, dim=1)\n",
        "\n",
        "        # Hitung confidence dengan softmax\n",
        "        probabilities = torch.softmax(logits, dim=1)\n",
        "        predictions = torch.argmax(probabilities, dim=1)\n",
        "        max_probs = torch.max(probabilities, dim=1)[0]\n",
        "\n",
        "        for i in range(len(text)):\n",
        "            prediction = predictions[i].item()\n",
        "            energy_score = energy[i].item()\n",
        "            confidence = max_probs[i].item()\n",
        "\n",
        "            # Deteksi OOD berdasarkan metode yang dipilih\n",
        "            is_ood_energy = energy_score > energy_threshold if energy_threshold is not None else False\n",
        "            is_ood_msp = confidence < msp_threshold if msp_threshold is not None else False\n",
        "\n",
        "            if method == 'energy':\n",
        "                is_ood = is_ood_energy\n",
        "            elif method == 'msp':\n",
        "                is_ood = is_ood_msp\n",
        "            else:  # 'combined'\n",
        "                is_ood = is_ood_energy and is_ood_msp\n",
        "\n",
        "            # Tentukan intent berdasarkan hasil OOD detection\n",
        "            if is_ood:\n",
        "                predicted_intent = \"unknown\"\n",
        "                topk_intents = [(\"unknown\", 1.0)]  # Unknown intent dengan confidence 100%\n",
        "            else:\n",
        "                predicted_intent = intent_classes[prediction]\n",
        "\n",
        "                # Dapatkan top 3 intent dengan confidence tertinggi\n",
        "                top_k = min(3, len(intent_classes))\n",
        "                topk_values, topk_indices = torch.topk(probabilities[i], top_k)\n",
        "                topk_intents = [(intent_classes[idx.item()], val.item())\n",
        "                                for idx, val in zip(topk_indices, topk_values)]\n",
        "\n",
        "            # Buat hasil untuk input ini\n",
        "            result = {\n",
        "                \"text\": text[i],\n",
        "                \"intent\": predicted_intent,\n",
        "                \"confidence\": confidence,\n",
        "                \"energy_score\": energy_score,\n",
        "                \"is_ood\": is_ood,\n",
        "                \"is_ood_energy\": is_ood_energy,\n",
        "                \"is_ood_msp\": is_ood_msp if msp_threshold is not None else None,\n",
        "                \"top_intents\": topk_intents\n",
        "            }\n",
        "\n",
        "            if return_logits:\n",
        "                result[\"logits\"] = logits[i].cpu().numpy()\n",
        "\n",
        "            results.append(result)\n",
        "\n",
        "    # Jika hanya satu input, kembalikan hasil langsung tanpa list\n",
        "    if len(text) == 1:\n",
        "        return results[0]\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDr3k0Ir0rOU"
      },
      "outputs": [],
      "source": [
        "# @title Run pipeline\n",
        "def run_full_pipeline_enhanced(use_drive=True, percentile=95, ood_method='combined', split_dataset=\"no\", val_split=0.2):\n",
        "    \"\"\"Jalankan pipeline lengkap dengan enhanced OOD detection dan visualisasi yang ditingkatkan\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    use_drive : bool\n",
        "        Apakah menggunakan Google Drive untuk penyimpanan\n",
        "    percentile : int\n",
        "        Persentil untuk threshold OOD detection\n",
        "    ood_method : str\n",
        "        Metode OOD detection ('msp', 'energy', 'combined')\n",
        "    split_dataset : str\n",
        "        Mode pemisahan dataset (\"yes\" untuk split dari train.csv, \"no\" untuk file terpisah)\n",
        "    val_split : float\n",
        "        Proporsi data validasi jika split_dataset=\"yes\" (default: 0.2)\n",
        "    \"\"\"\n",
        "\n",
        "    # Buat folder untuk simpan model\n",
        "    os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
        "\n",
        "    # Load and prepare data based on selected mode\n",
        "    train_texts, train_labels, val_texts, val_labels, intent_classes, label_encoder = prepare_data(\n",
        "        split_dataset, val_split)\n",
        "\n",
        "    num_labels = len(intent_classes)\n",
        "\n",
        "    # Setup model\n",
        "    model, tokenizer = setup_indobert_for_intent(num_labels)\n",
        "\n",
        "    # Train model with enhanced visualization\n",
        "    model, history = train_intent_classifier(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        train_texts,\n",
        "        train_labels,\n",
        "        val_texts,\n",
        "        val_labels,\n",
        "        class_names=intent_classes,\n",
        "        batch_size=16,\n",
        "        epochs=10,\n",
        "        learning_rate=2e-5,\n",
        "        weight_decay=0.01,\n",
        "        patience=3\n",
        "    )\n",
        "\n",
        "    # OOD calibration and model evaluation\n",
        "    thresholds = calibrate_and_evaluate(model, tokenizer, val_texts, val_labels,\n",
        "                                       intent_classes, percentile)\n",
        "\n",
        "    # Save model artifacts\n",
        "    save_model_artifacts(model, tokenizer, intent_classes, label_encoder, history)\n",
        "\n",
        "    # Print summary information\n",
        "    print_summary(num_labels, intent_classes, thresholds)\n",
        "\n",
        "    # Run prediction demo\n",
        "    run_prediction_demo_enhanced(model, tokenizer, intent_classes, label_encoder, method=ood_method)\n",
        "\n",
        "    return model, tokenizer, intent_classes, label_encoder\n",
        "\n",
        "def prepare_data(split_dataset, val_split):\n",
        "    \"\"\"Prepare training and validation data based on split mode\"\"\"\n",
        "    train_csv = \"train.csv\"\n",
        "    val_csv = \"val.csv\"\n",
        "\n",
        "    if split_dataset.lower() == \"yes\":\n",
        "        # Load all data from train.csv and split\n",
        "        all_texts, all_labels, intent_classes, label_encoder = load_csv_data(train_csv, show_distribution=True)\n",
        "\n",
        "        # Split data into train and validation\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "            all_texts, all_labels, test_size=val_split, random_state=42, stratify=all_labels\n",
        "        )\n",
        "\n",
        "        print(f\"\\n✅ Dataset telah dibagi: {len(train_texts)} data training dan {len(val_texts)} data validasi\")\n",
        "    else:\n",
        "        # Use separate files mode\n",
        "        train_texts, train_labels, intent_classes, label_encoder = load_csv_data(train_csv, show_distribution=True)\n",
        "        val_texts, val_labels, _, _ = load_csv_data(val_csv, label_encoder=label_encoder, show_distribution=True)\n",
        "\n",
        "    return train_texts, train_labels, val_texts, val_labels, intent_classes, label_encoder\n",
        "\n",
        "def calibrate_and_evaluate(model, tokenizer, val_texts, val_labels, intent_classes, percentile):\n",
        "    \"\"\"Calibrate OOD detection and evaluate model\"\"\"\n",
        "    # Prepare validation dataloader for OOD calibration\n",
        "    val_dataset = IntentDataset(val_texts, val_labels, tokenizer)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "    # Calibrate OOD detection\n",
        "    thresholds = enhanced_calibrate_ood(model, tokenizer, val_dataloader, MODEL_SAVE_PATH, percentile=percentile)\n",
        "\n",
        "    # Evaluate model and generate visualizations\n",
        "    report, cm = evaluate_model_enhanced(model, tokenizer, val_texts, val_labels, intent_classes, MODEL_SAVE_PATH)\n",
        "\n",
        "    return thresholds\n",
        "\n",
        "def save_model_artifacts(model, tokenizer, intent_classes, label_encoder, history):\n",
        "    \"\"\"Save model artifacts and visualizations\"\"\"\n",
        "    # Generate enhanced visualizations for the final model\n",
        "    enhanced_plot_training_results(history, MODEL_SAVE_PATH, class_names=intent_classes)\n",
        "    save_enhanced_history(history, MODEL_SAVE_PATH)\n",
        "\n",
        "    # Save intent classes & label encoder\n",
        "    with open(f\"{MODEL_SAVE_PATH}/intent_classes.pkl\", \"wb\") as f:\n",
        "        pickle.dump(intent_classes, f)\n",
        "\n",
        "    with open(f\"{MODEL_SAVE_PATH}/label_encoder.pkl\", \"wb\") as f:\n",
        "        pickle.dump(label_encoder, f)\n",
        "\n",
        "    print(f\"\\n✅ Model telah berhasil dilatih dan disimpan di {MODEL_SAVE_PATH}\")\n",
        "\n",
        "def print_summary(num_labels, intent_classes, thresholds):\n",
        "    \"\"\"Print summary information about the trained model\"\"\"\n",
        "    print(f\"Jumlah intent: {num_labels}\")\n",
        "    print(f\"Intent yang didukung: {', '.join(intent_classes)}\")\n",
        "    print(f\"OOD detection thresholds: Energy={thresholds['energy_threshold']:.4f}, MSP={thresholds['msp_threshold']:.4f}\")\n",
        "    print(f\"Visualisasi training telah disimpan di {MODEL_SAVE_PATH}\")\n",
        "    print(f\"- Interactive plots dapat dibuka pada file HTML di folder tersebut\")\n",
        "    print(f\"- Static plots tersedia dalam format PNG\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dq-WV98s0zgH"
      },
      "outputs": [],
      "source": [
        "# @title Run Prediksi\n",
        "def run_prediction_demo_enhanced(model=None, tokenizer=None, intent_classes=None, label_encoder=None, model_path=None, method='combined', test_texts=None):\n",
        "    \"\"\"Jalankan demo prediksi intent dengan model yang telah dilatih dan enhanced OOD detection\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : Model object, optional\n",
        "        Model yang sudah dilatih\n",
        "    tokenizer : Tokenizer object, optional\n",
        "        Tokenizer yang sesuai dengan model\n",
        "    intent_classes : list, optional\n",
        "        Daftar kelas intent\n",
        "    label_encoder : LabelEncoder, optional\n",
        "        Label encoder yang digunakan saat training\n",
        "    model_path : str, optional\n",
        "        Path ke model tersimpan (digunakan jika model=None)\n",
        "    method : str, optional\n",
        "        Metode OOD detection ('msp', 'energy', 'combined')\n",
        "    test_texts : list, optional\n",
        "        Daftar teks untuk diprediksi secara batch. Setelah batch, akan lanjut ke mode interaktif.\n",
        "    \"\"\"\n",
        "\n",
        "    if model_path is None:\n",
        "        model_path = MODEL_SAVE_PATH\n",
        "\n",
        "    # Jika model tidak diberikan, muat dari path penyimpanan\n",
        "    if model is None or tokenizer is None or intent_classes is None:\n",
        "        if not os.path.exists(model_path):\n",
        "            print(f\"Error: Model tidak ditemukan di {model_path}\")\n",
        "            print(\"Jalankan run_full_pipeline() terlebih dahulu untuk melatih model\")\n",
        "            return\n",
        "\n",
        "        # Muat model dan tokenizer\n",
        "        print(f\"Memuat model dari {model_path}...\")\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "        # Muat intent classes\n",
        "        import pickle\n",
        "        with open(f\"{model_path}/intent_classes.pkl\", \"rb\") as f:\n",
        "            intent_classes = pickle.load(f)\n",
        "            print(f\"Intent yang didukung: {', '.join(intent_classes)}\")\n",
        "\n",
        "    # Load OOD thresholds\n",
        "    thresholds = load_ood_thresholds(model_path)\n",
        "    energy_threshold = thresholds[\"energy_threshold\"]\n",
        "    msp_threshold = thresholds.get(\"msp_threshold\")\n",
        "\n",
        "    if msp_threshold is not None:\n",
        "        print(f\"OOD thresholds loaded: Energy={energy_threshold:.4f}, MSP={msp_threshold:.4f}\")\n",
        "    else:\n",
        "        print(f\"OOD thresholds loaded: Energy={energy_threshold:.4f}, MSP=None\")\n",
        "\n",
        "    print(f\"Menggunakan metode deteksi OOD: {method}\")\n",
        "\n",
        "    print(\"\\nDemo Prediksi Intent dengan Enhanced OOD Detection:\")\n",
        "    print(\"====================================================\")\n",
        "\n",
        "    # Helper function untuk memprediksi dan menampilkan hasil\n",
        "    def predict_and_display(text):\n",
        "        result = predict_intent_with_enhanced_ood(\n",
        "            text,\n",
        "            model,\n",
        "            tokenizer,\n",
        "            intent_classes,\n",
        "            energy_threshold,\n",
        "            msp_threshold,\n",
        "            method=method\n",
        "        )\n",
        "\n",
        "        if result[\"is_ood\"]:\n",
        "            print(f\"⚠️ Intent terdeteksi: unknown\")\n",
        "            print(f\"   Energy score: {result['energy_score']:.4f} (threshold: {energy_threshold:.4f})\")\n",
        "            if msp_threshold:\n",
        "                print(f\"   Confidence score: {result['confidence']:.4f} (threshold: {msp_threshold:.4f})\")\n",
        "        else:\n",
        "            print(f\"✓ Intent terdeteksi: {result['intent']} (confidence: {result['confidence']:.4f})\")\n",
        "\n",
        "        print(\"\\nTop 3 intent:\")\n",
        "        for i, (intent_name, score) in enumerate(result[\"top_intents\"]):\n",
        "            print(f\"  {i+1}. {intent_name}: {score:.4f}\")\n",
        "\n",
        "        print(\"\\nDetail OOD detection:\")\n",
        "        print(f\"  Energy-based: {'OOD' if result['is_ood_energy'] else 'In-Distribution'} ({result['energy_score']:.4f})\")\n",
        "        if msp_threshold:\n",
        "            print(f\"  MSP-based: {'OOD' if result['is_ood_msp'] else 'In-Distribution'} ({result['confidence']:.4f})\")\n",
        "        print(f\"  Final decision: {'OOD' if result['is_ood'] else 'In-Distribution'}\")\n",
        "\n",
        "    # Jika test_texts diberikan, lakukan prediksi batch\n",
        "    if test_texts is not None and isinstance(test_texts, list) and len(test_texts) > 0:\n",
        "        print(f\"\\nMemprediksi {len(test_texts)} contoh teks:\")\n",
        "        print(\"----------------------------\")\n",
        "\n",
        "        for i, text in enumerate(test_texts):\n",
        "            print(f\"\\nContoh #{i+1}: \\\"{text}\\\"\")\n",
        "            predict_and_display(text)\n",
        "\n",
        "        print(\"\\n----------------------------\")\n",
        "        print(\"Selesai memprediksi contoh teks. Beralih ke mode interaktif.\")\n",
        "\n",
        "    # Mode interaktif\n",
        "    print(\"\\nMode Interaktif - Masukkan teks untuk prediksi intent\")\n",
        "    print(\"Ketik 'exit' untuk keluar\")\n",
        "    print(\"----------------------------\")\n",
        "\n",
        "    # Prediksi input pengguna\n",
        "    while True:\n",
        "        user_input = input(\"\\nMasukkan teks: \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        predict_and_display(user_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpUFrU9D-qPt"
      },
      "outputs": [],
      "source": [
        "# Cell 13: Jalankan pipeline (uncomment untuk menjalankan)\n",
        "\n",
        "model, tokenizer, intent_classes, label_encoder = run_full_pipeline_enhanced(\n",
        "    use_drive=True,               # Whether to use Google Drive for storage\n",
        "    percentile=95,                # Percentile for OOD threshold\n",
        "    ood_method='combined',        # OOD detection method ('msp', 'energy', or 'combined')\n",
        "    split_dataset=\"no\",           # \"yes\" for splitting train.csv, \"no\" for separate files\n",
        "    val_split=0.18                 # Validation split ratio if split_dataset=\"yes\"\n",
        ")\n",
        "\n",
        "\n",
        "# Cell 14: Jalankan demo prediksi (uncomment untuk menjalankan)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Teks judul default\n",
        "test_sentences = [\n",
        "    # GREETING\n",
        "    \"Halo, selamat pagi!\",\n",
        "    \"Apa kabar?\",\n",
        "    \"Hai, bot!\",\n",
        "    \"Permisi, boleh bertanya?\",\n",
        "    \"Yo, ada orang di sana?\",\n",
        "\n",
        "    # GOODBYE\n",
        "    \"Terima kasih, sampai jumpa.\",\n",
        "    \"Ok, saya pergi dulu.\",\n",
        "    \"Sampai nanti!\",\n",
        "    \"Dadah, bot.\",\n",
        "    \"Aku akan kembali nanti.\",\n",
        "\n",
        "    # CONFIRM\n",
        "    \"Iya, benar.\",\n",
        "    \"Betul sekali.\",\n",
        "    \"Ya, saya setuju.\",\n",
        "    \"Tentu saja.\",\n",
        "    \"Itu yang saya maksud.\",\n",
        "\n",
        "    # DENIED\n",
        "    \"Tidak, bukan itu.\",\n",
        "    \"Salah.\",\n",
        "    \"Bukan, maksud saya yang lain.\",\n",
        "    \"Enggak.\",\n",
        "    \"Saya tidak yakin dengan itu.\",\n",
        "\n",
        "    # AMBIGUOUS (bisa mengecoh)\n",
        "    \"Saya rasa tidak perlu, tapi ya juga boleh.\",\n",
        "    \"Mungkin... tapi entahlah.\",\n",
        "    \"Terserah kamu aja deh.\",\n",
        "    \"Boleh iya, boleh juga tidak.\",\n",
        "    \"Ya tapi tidak juga sih...\",\n",
        "    \"p\",\n",
        "    \"test\",\n",
        "    \"y\",\n",
        "    \"g\",\n",
        "    \"N\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "P9gOO73x_oxo",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oS_b2pfqxA8Z",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title Teks judul default\n",
        "# If you want to load an existing model and run predictions\n",
        "run_prediction_demo_enhanced(\n",
        "    model_path=MODEL_SAVE_PATH,  # Your MODEL_SAVE_PATH\n",
        "    method='combined',  # Which OOD detection method to use\n",
        "    test_texts=test_sentences\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66tjkPIApbrL",
        "outputId": "dfa0dd5e-5fc6-414d-b15c-f5504050fcfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[✓] Selesai! Original: 420 | Total setelah augmentasi: 1335 → augmented_datasets.csv\n"
          ]
        }
      ],
      "source": [
        "# @title Augmented intent\n",
        "\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import nltk\n",
        "import torch\n",
        "from nltk.corpus import wordnet\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Download WordNet data (if not already downloaded)\n",
        "nltk.download('wordnet')\n",
        "# === KONFIGURASI ===\n",
        "INPUT_CSV = '/content/train.csv'\n",
        "OUTPUT_CSV = 'augmented_datasets.csv'\n",
        "VARIATIONS_PER_SENTENCE = 5\n",
        "USE_PARAPHRASE = True  # aktifkan atau matikan paraphrase\n",
        "\n",
        "# === BACA DATASET ===\n",
        "df = pd.read_csv(INPUT_CSV)\n",
        "df = df.dropna()\n",
        "\n",
        "# === INDO PARAPHRASE SETUP ===\n",
        "if USE_PARAPHRASE:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"Wikidepia/IndoT5-base-paraphrase\")\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(\"Wikidepia/IndoT5-base-paraphrase\")\n",
        "\n",
        "def indo_paraphrase(text, n=3):\n",
        "    input_text = \"paraphrase: \" + text + \" </s>\"\n",
        "    encoding = tokenizer(input_text, padding='longest', return_tensors=\"pt\")\n",
        "    outputs = model.generate(\n",
        "        input_ids=encoding[\"input_ids\"], attention_mask=encoding[\"attention_mask\"],\n",
        "        max_length=128,\n",
        "        do_sample=True,\n",
        "        top_k=200,\n",
        "        top_p=0.95,\n",
        "        early_stopping=True,\n",
        "        num_return_sequences=n\n",
        "    )\n",
        "    return [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "\n",
        "# === SINONIM (WordNet) ===\n",
        "def synonym_replace(text):\n",
        "    words = text.split()\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        syns = wordnet.synsets(word)\n",
        "        if syns and len(word) > 3:\n",
        "            lemmas = set(l.name().replace('_', ' ') for s in syns for l in s.lemmas())\n",
        "            if lemmas:\n",
        "                synonym = random.choice(list(lemmas))\n",
        "                if synonym.lower() != word.lower():\n",
        "                    new_words.append(synonym)\n",
        "                    continue\n",
        "        new_words.append(word)\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "# === GAYA GAUL / SINGKATAN ===\n",
        "slang_dict = {\n",
        "    # JAM LAYANAN\n",
        "    'perpustakaan': 'perpus',\n",
        "    'buka': 'open',\n",
        "    'tutup': 'close',\n",
        "    'sampai': 'sampe',\n",
        "    'jam': 'jm',\n",
        "    'informasi': 'info',\n",
        "    'hari ini': 'hr ini',\n",
        "    'kapan': 'kpn',\n",
        "    'jadwal': 'jdwl',\n",
        "    'operasional': 'ops',\n",
        "    'layanan': 'lyn',\n",
        "    'masih': 'msih',\n",
        "    'sekarang': 'skrg',\n",
        "    'minggu': 'mg',\n",
        "    'hari': 'hri',\n",
        "    'apakah': 'apkh',\n",
        "    'pukul': 'pkl',\n",
        "\n",
        "    # CARI BUKU\n",
        "    'mencari': 'nyari',\n",
        "    'mau mencari': 'mau nyari',\n",
        "    'ingin mencari': 'pengen nyari',\n",
        "    'tolong carikan': 'cariin',\n",
        "    'tolong bantu cari': 'bantuin cari',\n",
        "    'mencarikan': 'cariin',\n",
        "    'buku': 'book',\n",
        "    'butuh': 'need',\n",
        "    'melihat': 'liat',\n",
        "    'daftar': 'list',\n",
        "    'akses': 'akses',\n",
        "    'temukan': 'nemu',\n",
        "    'mencoba': 'nyoba',\n",
        "    'mengakses': 'akses',\n",
        "    'gunakan': 'make use',\n",
        "    'fitur': 'fitr',\n",
        "    'pencarian': 'search',\n",
        "    'referensi': 'ref',\n",
        "    'bantuan': 'bntuan',\n",
        "    'bisa': 'bs',\n",
        "    'dimana': 'dmn',\n",
        "    'bagaimana': 'gmn',\n",
        "    'cek': 'check',\n",
        "    'lihat-lihat': 'liat2',\n",
        "    'cari': 'search',\n",
        "    'mencari buku': 'nyari book',\n",
        "\n",
        "    'halo': 'haloo',\n",
        "    'hai': 'hay',\n",
        "    'hello': 'helo',\n",
        "    'selamat pagi': 'slmt pagi',\n",
        "    'selamat siang': 'slmt siang',\n",
        "    'selamat sore': 'slmt sore',\n",
        "    'selamat malam': 'slmt malam',\n",
        "    'apa kabar': 'apa kbr',\n",
        "    'assalamualaikum': 'asswrwb',\n",
        "    'permisi': 'permizz',\n",
        "    'hai bot': 'hey bot',\n",
        "    'bot': 'bt',\n",
        "    'selamat datang': 'slmt dtg',\n",
        "\n",
        "    'terima kasih': 'makasih',\n",
        "    'goodbye' : 'gudbai',\n",
        "    'makasih': 'mksh',\n",
        "    'makasih ya': 'thx ya',\n",
        "    'sampai jumpa': 'sampe jmpa',\n",
        "    'dadah': 'daah',\n",
        "    'bye': 'byee',\n",
        "    'sampai nanti': 'sampe ntar',\n",
        "    'see you': 'cu',\n",
        "    'thanks': 'thx',\n",
        "    'thank you': 'tq',\n",
        "    'sekian': 'skian',\n",
        "    'itu saja': 'itu aj',\n",
        "\n",
        "    'boleh': 'blh',\n",
        "    'ya': 'iya',\n",
        "    'benar': 'bener',\n",
        "    'betul': 'btl',\n",
        "    'oke': 'ok',\n",
        "    'baik': 'sip',\n",
        "    'siap': 'sip',\n",
        "    'setuju': 'stju',\n",
        "    'bener': 'bnr',\n",
        "    'iya benar': 'ya bnr',\n",
        "    'okey': 'okeyy',\n",
        "    'ok deh': 'okedeh',\n",
        "\n",
        "    'tidak': 'gak',\n",
        "    'ga': 'gx',\n",
        "    'bukan': 'bkn',\n",
        "    'tidak mau': 'gak mau',\n",
        "    'ga mau': 'gk mw',\n",
        "    'tidak setuju': 'gak setuju',\n",
        "    'saya tidak': 'aku ga',\n",
        "    'nggak perlu': 'ga perlu',\n",
        "    'ga usah': 'rasah',\n",
        "    'tidak perlu': 'gak usah',\n",
        "    'no': 'nope',\n",
        "}\n",
        "\n",
        "\n",
        "def apply_slang_typo(text):\n",
        "    for k, v in slang_dict.items():\n",
        "        text = re.sub(rf'\\b{k}\\b', v, text, flags=re.IGNORECASE)\n",
        "    return text\n",
        "\n",
        "# === AUGMENTATION LOOP ===\n",
        "augmented_data = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    intent = row['intent']\n",
        "    sentence = row['text']\n",
        "    augmented_data.append((intent, sentence))  # original\n",
        "\n",
        "    for _ in range(VARIATIONS_PER_SENTENCE):\n",
        "        strategy = random.choice(['synonym', 'slang', 'paraphrase'])\n",
        "\n",
        "        if strategy == 'synonym':\n",
        "            aug = synonym_replace(sentence)\n",
        "        elif strategy == 'slang':\n",
        "            aug = apply_slang_typo(sentence)\n",
        "        elif strategy == 'paraphrase' and USE_PARAPHRASE:\n",
        "            try:\n",
        "                aug_list = indo_paraphrase(sentence, n=1)\n",
        "                aug = aug_list[0] if aug_list else sentence\n",
        "            except Exception as e:\n",
        "                print(f\"[!] Paraphrase failed for: {sentence}\\n{e}\")\n",
        "                aug = sentence\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        if aug.lower() != sentence.lower():\n",
        "            augmented_data.append((intent, aug))\n",
        "\n",
        "# === SIMPAN HASIL ===\n",
        "aug_df = pd.DataFrame(augmented_data, columns=['intent', 'text'])\n",
        "aug_df.drop_duplicates(inplace=True)\n",
        "aug_df.to_csv(OUTPUT_CSV, index=False)\n",
        "\n",
        "print(f\"[✓] Selesai! Original: {len(df)} | Total setelah augmentasi: {len(aug_df)} → {OUTPUT_CSV}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers nltk fuzzywuzzy python-Levenshtein"
      ],
      "metadata": {
        "id": "yJaqYdjaxKZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUYgABG0S6lf",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title kontol\n",
        "import pandas as pd\n",
        "import random\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "from deep_translator import GoogleTranslator\n",
        "from collections import defaultdict\n",
        "import re\n",
        "\n",
        "# =========[ IMPROVED SYNONYM DICTIONARY ]=========\n",
        "# Custom Indonesian synonym dictionary for common words\n",
        "id_synonyms = {\n",
        "    # Greeting related words\n",
        "    \"saya\": [\"aku\", \"gue\", \"gua\", \"hamba\", \"beta\"],\n",
        "    \"kamu\": [\"anda\", \"engkau\", \"kalian\", \"elu\", \"dikau\"],\n",
        "    \"halo\": [\"hai\", \"hey\", \"hi\", \"selamat jumpa\", \"salam\", \"assalamualaikum\"],\n",
        "    \"selamat\": [\"bahagia\", \"sukses\", \"sejahtera\"],\n",
        "    \"senang\": [\"bahagia\", \"gembira\", \"ceria\", \"suka\", \"girang\"],\n",
        "    \"bertemu\": [\"berjumpa\", \"ketemu\", \"bersua\", \"menjumpai\"],\n",
        "    \"rindu\": [\"kangen\", \"merindukan\", \"nostalgia\", \"damba\"],\n",
        "\n",
        "    # Goodbye related words\n",
        "    \"sampai\": [\"hingga\", \"sampai dengan\", \"sehingga\"],\n",
        "    \"jumpa\": [\"ketemu\", \"bersua\", \"bertemu\", \"berjumpa\"],\n",
        "    \"pamit\": [\"izin\", \"permisi\", \"undur diri\", \"mohon diri\"],\n",
        "    \"tinggal\": [\"berdiam\", \"bermukim\", \"menetap\"],\n",
        "    \"pergi\": [\"berangkat\", \"meninggalkan\", \"menuju\", \"bepergian\"],\n",
        "    \"pulang\": [\"kembali\", \"balik\", \"mudik\", \"kembali ke rumah\"],\n",
        "\n",
        "    # Confirm related words\n",
        "    \"ya\": [\"iya\", \"benar\", \"betul\", \"sungguh\", \"memang\"],\n",
        "    \"setuju\": [\"sepakat\", \"akur\", \"mufakat\", \"sependapat\", \"sepaham\"],\n",
        "    \"benar\": [\"betul\", \"tepat\", \"akurat\", \"sahih\", \"valid\"],\n",
        "    \"sudah\": [\"telah\", \"usai\", \"selesai\", \"rampung\", \"beres\"],\n",
        "    \"bisa\": [\"mampu\", \"dapat\", \"sanggup\", \"berdaya\", \"berkemampuan\"],\n",
        "    \"pasti\": [\"tentu\", \"yakin\", \"meyakinkan\", \"positif\", \"terang\"],\n",
        "    \"siap\": [\"bersedia\", \"bersiap\", \"sedia\", \"bersiaga\", \"tersedia\"],\n",
        "    \"jadi\": [\"menjadi\", \"terlaksana\", \"terjadi\", \"berlangsung\"],\n",
        "    \"lanjut\": [\"terus\", \"melanjutkan\", \"meneruskan\", \"berlanjut\"],\n",
        "    \"mantap\": [\"hebat\", \"bagus\", \"keren\", \"mengagumkan\", \"luar biasa\"],\n",
        "    \"bagus\": [\"baik\", \"indah\", \"elok\", \"cantik\", \"menarik\"],\n",
        "    \"tentu\": [\"pasti\", \"yakin\", \"meyakinkan\", \"terang\", \"jelas\"],\n",
        "\n",
        "    # Denied related words\n",
        "    \"tidak\": [\"bukan\", \"tiada\", \"tak\", \"belum\", \"tanpa\"],\n",
        "    \"jangan\": [\"larang\", \"cegah\", \"hindari\", \"batalkan\"],\n",
        "    \"belum\": [\"masih\", \"sedang\", \"tengah\", \"dalam proses\"],\n",
        "    \"batal\": [\"urung\", \"gagal\", \"berhenti\", \"dibatalkan\", \"tak jadi\"],\n",
        "    \"maaf\": [\"ampun\", \"pemaafan\", \"pengampunan\", \"mohon maaf\"],\n",
        "    \"menolak\": [\"menampik\", \"mengelak\", \"menghindar\", \"melarang\"],\n",
        "    \"mustahil\": [\"tidak mungkin\", \"tidak bisa\", \"sulit\", \"sukar\"],\n",
        "    \"salah\": [\"keliru\", \"alpa\", \"sesat\", \"khilaf\", \"menyimpang\"],\n",
        "    \"gagal\": [\"batal\", \"tidak berhasil\", \"kalah\", \"tumbang\"],\n",
        "    \"susah\": [\"sulit\", \"rumit\", \"kompleks\", \"tidak mudah\", \"berat\"],\n",
        "\n",
        "    # Common words\n",
        "    \"tolong\": [\"bantu\", \"sokong\", \"dukung\", \"membantu\"],\n",
        "    \"besok\": [\"esok\", \"hari berikutnya\", \"keesokan hari\"],\n",
        "    \"cukup\": [\"memadai\", \"layak\", \"pantas\", \"patut\", \"lumayan\"],\n",
        "    \"untuk\": [\"bagi\", \"demi\", \"guna\", \"buat\", \"kepada\"],\n",
        "    \"ini\": [\"hal ini\", \"hal tersebut\", \"sekarang\", \"saat ini\"],\n",
        "    \"kita\": [\"kami\", \"kamu dan saya\", \"kelompok kita\", \"tim kita\"],\n",
        "    \"fix\": [\"tetap\", \"pasti\", \"permanen\", \"tidak berubah\"],\n",
        "    \"nigga\": [\"nigger\", \"hitam\", \"negro\"]\n",
        "\n",
        "}\n",
        "\n",
        "# =========[ FILE PATHS ]=========\n",
        "PilihData = \"train\"  # @param [\"train\",\"val\"]\n",
        "if PilihData == \"train\":\n",
        "    file_path = \"/content/rusdi-prototype-1/val.xlsx\"\n",
        "    csv_path = \"/content/train.csv\"\n",
        "    output_path_balanced = \"/content/train.csv\"\n",
        "elif PilihData == \"val\":\n",
        "    file_path = \"/content/val.xlsx\"\n",
        "    csv_path = \"/content/val.csv\"\n",
        "    output_path_balanced = \"/content/val.csv\"\n",
        "\n",
        "# =========[ READ & CONVERT XLSX ]=========\n",
        "if file_path.endswith(\".xlsx\"):\n",
        "    df = pd.read_excel(file_path)\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"✅ File dikonversi ke CSV: {csv_path}\")\n",
        "else:\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "# =========[ AUGMENTATION METHODS ]=========\n",
        "\n",
        "def get_better_synonym(word):\n",
        "    \"\"\"Get synonym from custom dictionary or return the original word\"\"\"\n",
        "    word_lower = word.lower()\n",
        "    if word_lower in id_synonyms:\n",
        "        synonyms = id_synonyms[word_lower]\n",
        "        return random.choice(synonyms)\n",
        "    return word\n",
        "\n",
        "def replace_with_synonym(sentence):\n",
        "    \"\"\"Replace words with synonyms while preserving capitalization\"\"\"\n",
        "    words = sentence.split()\n",
        "    new_words = []\n",
        "\n",
        "    for word in words:\n",
        "        synonym = get_better_synonym(word)\n",
        "        # Preserve capitalization\n",
        "        if word and word[0].isupper() and synonym:\n",
        "            synonym = synonym[0].upper() + synonym[1:]\n",
        "        new_words.append(synonym)\n",
        "\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "def back_translate(sentence):\n",
        "    \"\"\"Translate to English and back to Indonesian\"\"\"\n",
        "    try:\n",
        "        # First to English\n",
        "        translated = GoogleTranslator(source='id', target='en').translate(sentence)\n",
        "        # Then back to Indonesian\n",
        "        back_translated = GoogleTranslator(source='en', target='id').translate(translated)\n",
        "\n",
        "        # Only return if result is different but not completely unrelated\n",
        "        if back_translated != sentence and len(back_translated.split()) >= len(sentence.split()) * 0.5:\n",
        "            return back_translated\n",
        "        return sentence\n",
        "    except Exception:\n",
        "        return sentence\n",
        "\n",
        "def add_typo(sentence):\n",
        "    \"\"\"Add a single typo by replacing a character\"\"\"\n",
        "    chars = list(sentence)\n",
        "    if len(chars) > 3:\n",
        "        idx = random.randint(0, len(chars) - 1)\n",
        "        if chars[idx].isalpha():  # Only replace letters\n",
        "            chars[idx] = random.choice(\"abcdefghijklmnopqrstuvwxyz\")\n",
        "    return \"\".join(chars)\n",
        "\n",
        "def random_deletion(sentence, p=0.2):\n",
        "    \"\"\"Delete words with probability p\"\"\"\n",
        "    words = sentence.split()\n",
        "    if len(words) <= 3:  # Don't delete from very short sentences\n",
        "        return sentence\n",
        "\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if random.uniform(0, 1) > p:\n",
        "            new_words.append(word)\n",
        "\n",
        "    # Make sure we don't delete everything\n",
        "    if not new_words:\n",
        "        return sentence\n",
        "\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "def random_swap(sentence, n=1):\n",
        "    \"\"\"Swap n pairs of words\"\"\"\n",
        "    words = sentence.split()\n",
        "    if len(words) < 2:\n",
        "        return sentence\n",
        "\n",
        "    for _ in range(min(n, len(words)//2)):  # Ensure we don't try too many swaps\n",
        "        idx1, idx2 = random.sample(range(len(words)), 2)\n",
        "        words[idx1], words[idx2] = words[idx2], words[idx1]\n",
        "\n",
        "    return \" \".join(words)\n",
        "\n",
        "def phonetic_augmentation(sentence):\n",
        "    \"\"\"Apply phonetic substitutions common in Indonesian chat\"\"\"\n",
        "    phonetic_dict = {\n",
        "        # Greeting related\n",
        "        \"saya\": [\"sy\", \"saia\", \"ane\", \"ana\", \"w\", \"gw\", \"q\", \"aq\"],\n",
        "        \"kamu\": [\"km\", \"kamyu\", \"u\", \"lo\", \"lu\", \"l\", \"ngana\", \"sampeyan\", \"antum\", \"ente\"],\n",
        "        \"halo\": [\"hlo\", \"hallo\", \"helo\", \"haloo\", \"hellow\", \"hy\", \"hyy\", \"p\", \"ping\"],\n",
        "        \"selamat\": [\"slmt\", \"slamat\", \"met\", \"slam\"],\n",
        "        \"pagi\": [\"pgi\", \"morning\", \"pg\", \"subuh\"],\n",
        "        \"siang\": [\"siang\", \"afternoon\", \"siank\", \"siyang\"],\n",
        "        \"malam\": [\"mlm\", \"malem\", \"mlem\", \"night\", \"evening\", \"mlem\", \"mlm\"],\n",
        "        \"apa\": [\"ap\", \"ape\", \"apah\", \"pa\"],\n",
        "        \"kabar\": [\"kbr\", \"kabare\", \"kbar\", \"kabbar\"],\n",
        "        \"gimana\": [\"gmn\", \"bgmn\", \"gmana\", \"gimane\", \"gmn\"],\n",
        "\n",
        "        # Goodbye related\n",
        "        \"sampai\": [\"smp\", \"sampe\", \"smpe\", \"smpei\", \"sampeyan\"],\n",
        "        \"jumpa\": [\"jpa\", \"jmpa\", \"ktmu\", \"jumpe\"],\n",
        "        \"dadah\": [\"byebye\", \"bye\", \"bay\", \"byee\", \"bbye\", \"bye2\", \"dadah\"],\n",
        "        \"pamit\": [\"pmt\", \"pamitt\", \"off\", \"out\", \"cabut\", \"cbut\"],\n",
        "        \"tinggal\": [\"tgl\", \"tnggal\", \"tinggel\", \"tggal\", \"tinggelin\"],\n",
        "        \"duluan\": [\"dlu\", \"duluan\", \"dluan\", \"dluan ya\", \"ahead\"],\n",
        "        \"pergi\": [\"pgi\", \"pegi\", \"prgi\", \"out\", \"keluar\"],\n",
        "        \"pulang\": [\"plg\", \"plng\", \"balik\", \"blk\", \"mudik\"],\n",
        "\n",
        "        # Confirm related\n",
        "        \"ya\": [\"y\", \"yah\", \"iye\", \"yoi\", \"yups\", \"yes\", \"yess\", \"yesss\", \"okey\", \"okeh\", \"oks\"],\n",
        "        \"setuju\": [\"stju\", \"acc\", \"accept\", \"approved\", \"approve\", \"deal\", \"oke\", \"ok\", \"sip\"],\n",
        "        \"benar\": [\"bnr\", \"bner\", \"bener\", \"bnr\", \"yoi\", \"correct\"],\n",
        "        \"sudah\": [\"sdh\", \"dah\", \"udh\", \"done\", \"wes\", \"uwes\", \"udah\", \"sdah\"],\n",
        "        \"bisa\": [\"bs\", \"bsa\", \"biza\", \"bsa\", \"biza\", \"ok\"],\n",
        "        \"pasti\": [\"pst\", \"psti\", \"pastii\", \"sure\", \"certain\"],\n",
        "        \"siap\": [\"sp\", \"ready\", \"sip\", \"sp\", \"roger\", \"on\", \"online\"],\n",
        "        \"jadi\": [\"jd\", \"jdi\", \"jdnya\", \"jdiin\", \"proceed\"],\n",
        "        \"lanjut\": [\"lnjt\", \"lanjt\", \"next\", \"go\"],\n",
        "        \"mantap\": [\"mntap\", \"mantab\", \"mntb\", \"top\", \"mantul\", \"josss\", \"kerennn\"],\n",
        "        \"bagus\": [\"bgs\", \"bgus\", \"nice\", \"naiss\", \"keren\", \"top\"],\n",
        "\n",
        "        # Denied related\n",
        "        \"tidak\": [\"tdk\", \"gak\", \"ga\", \"g\", \"nggak\", \"ngga\", \"nope\", \"no\", \"kagak\", \"kaga\", \"kgk\"],\n",
        "        \"jangan\": [\"jgn\", \"jngn\", \"don't\", \"dont\", \"jgn\", \"ga usah\", \"tdk usah\", \"gausa\", \"gausah\"],\n",
        "        \"belum\": [\"blm\", \"blom\", \"belom\", \"not yet\", \"durung\", \"durong\", \"belm\"],\n",
        "        \"batal\": [\"btl\", \"cancel\", \"cansel\", \"urungkan\", \"batalin\", \"gajadi\"],\n",
        "        \"maaf\": [\"sorry\", \"sori\", \"maf\", \"maap\", \"maaaaf\", \"mrff\", \"sry\", \"srry\"],\n",
        "        \"menolak\": [\"tlk\", \"reject\", \"decline\", \"dtolak\", \"nolak\", \"gak mau\", \"gamau\"],\n",
        "        \"mustahil\": [\"impossible\", \"ga mungkin\", \"g mungkin\", \"tdk mungkin\", \"gak bs\"],\n",
        "        \"salah\": [\"slh\", \"wrong\", \"error\", \"eror\", \"salh\", \"fail\"],\n",
        "        \"gagal\": [\"ggl\", \"fail\", \"failed\", \"error\", \"gagak\", \"failll\"],\n",
        "\n",
        "        # Common words\n",
        "        \"terima\": [\"trma\", \"thanks\", \"thx\", \"trims\", \"tq\", \"tyvm\", \"makasih\", \"mksih\"],\n",
        "        \"kasih\": [\"ksh\", \"ksih\", \"thx\", \"makasih\", \"mksih\", \"thanks\"],\n",
        "        \"tolong\": [\"tlng\", \"help\", \"tlg\", \"tulung\", \"bantu\", \"bantuin\"],\n",
        "        \"please\": [\"plz\", \"plis\", \"pliss\", \"plisss\", \"pliissss\", \"tolong\"],\n",
        "        \"besok\": [\"bsk\", \"bsok\", \"besok\", \"tmrw\", \"esok\", \"besuk\"],\n",
        "        \"waktu\": [\"wkt\", \"waktu\", \"time\", \"tm\", \"jam\"],\n",
        "        \"cukup\": [\"ckp\", \"enough\", \"cukuppp\", \"cukups\", \"ckup\"],\n",
        "        \"melihat\": [\"lihat\", \"liat\", \"look\", \"see\", \"watching\", \"ngeliat\"],\n",
        "        \"alasan\": [\"alsan\", \"reason\", \"why\", \"alesan\", \"alsn\"],\n",
        "        \"untuk\": [\"utk\", \"buat\", \"bwt\", \"4\", \"tuk\", \"2\", \"to\"],\n",
        "        \"melakukan\": [\"lakukan\", \"do\", \"lakuin\", \"melakuin\", \"ngerjain\"],\n",
        "        \"ini\": [\"ni\", \"this\", \"these\", \"iki\", \"nih\", \"ne\"],\n",
        "        \"kita\": [\"kta\", \"we\", \"us\", \"w\", \"kt\"],\n",
        "        \"hari\": [\"hr\", \"day\", \"hri\", \"days\", \"dayy\", \"harii\"],\n",
        "        \"bro\": [\"broh\", \"brow\", \"brother\", \"mas\", \"bang\", \"bor\", \"omm\"],\n",
        "        \"sis\": [\"sist\", \"sister\", \"mbak\", \"mba\", \"nte\", \"ceu\", \"teh\"]\n",
        "    }\n",
        "\n",
        "    words = sentence.split()\n",
        "    new_words = []\n",
        "\n",
        "    for word in words:\n",
        "        word_lower = word.lower()\n",
        "        if word_lower in phonetic_dict:\n",
        "            new_word = random.choice(phonetic_dict[word_lower])\n",
        "            # Preserve capitalization\n",
        "            if word and word[0].isupper():\n",
        "                new_word = new_word[0].upper() + new_word[1:]\n",
        "            new_words.append(new_word)\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "def add_common_phrase(sentence):\n",
        "    \"\"\"Add a common Indonesian chat phrase\"\"\"\n",
        "    common_phrases = [\"sih\", \"ya\", \"anjay\", \"dong\", \"cuy\", \"bro\", \"lah\", \"plis\", \"eh\",\n",
        "                      \"nih\", \"gitu\", \"kan\", \"yah\", \"deh\", \"banget\"]\n",
        "    return sentence + \" \" + random.choice(common_phrases)\n",
        "\n",
        "def validate_augmentation(original, augmented):\n",
        "    \"\"\"Validate if augmentation is reasonable\"\"\"\n",
        "    # Check if augmentation is too different\n",
        "    if len(augmented.split()) < len(original.split()) * 0.5:\n",
        "        return False\n",
        "\n",
        "    # Check if augmentation is just the original\n",
        "    if augmented == original:\n",
        "        return False\n",
        "\n",
        "    # Check if augmentation contains too many non-Indonesian characters\n",
        "    non_indo_pattern = re.compile(r'[^a-zA-Z0-9\\s.,?!\\'\"-:;()[\\]{}]')\n",
        "    if len(non_indo_pattern.findall(augmented)) > 2:\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "# Combined augmentations\n",
        "def augment_data(text):\n",
        "    \"\"\"Generate multiple augmentations for a text\"\"\"\n",
        "    methods = [\n",
        "        replace_with_synonym,\n",
        "        back_translate,\n",
        "        add_typo,\n",
        "        random_deletion,\n",
        "        random_swap,\n",
        "        phonetic_augmentation,\n",
        "        add_common_phrase\n",
        "    ]\n",
        "\n",
        "    augmented = set()\n",
        "    for method in methods:\n",
        "        try:\n",
        "            result = method(text)\n",
        "            if validate_augmentation(text, result):\n",
        "                augmented.add(result)\n",
        "        except Exception as e:\n",
        "            print(f\"Error applying {method.__name__}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return list(augmented)\n",
        "\n",
        "# =========[ BALANCED AUGMENTATION ]=========\n",
        "target_aug_per_class = 700  # Can be changed (e.g., 100 augmented per class)\n",
        "intent_buckets = defaultdict(list)\n",
        "\n",
        "# Group original data by intent\n",
        "for _, row in df.iterrows():\n",
        "    intent_buckets[row['intent']].append(row['text'])\n",
        "\n",
        "df_augmented_balanced = []\n",
        "\n",
        "# Augment per class until target reached\n",
        "for intent, texts in intent_buckets.items():\n",
        "    current_aug_count = 0\n",
        "    index = 0\n",
        "    existing_aug_set = set()\n",
        "    print(f\"🔄 Augmenting class '{intent}'...\")\n",
        "\n",
        "    # Limit iterations to prevent infinite loops\n",
        "    max_iterations = 1000\n",
        "    iterations = 0\n",
        "\n",
        "    while current_aug_count < target_aug_per_class and iterations < max_iterations:\n",
        "        original_text = texts[index % len(texts)]\n",
        "        augmented_variants = augment_data(original_text)\n",
        "\n",
        "        for aug in augmented_variants:\n",
        "            if aug not in existing_aug_set:\n",
        "                df_augmented_balanced.append([aug, intent])\n",
        "                existing_aug_set.add(aug)\n",
        "                current_aug_count += 1\n",
        "                if current_aug_count >= target_aug_per_class:\n",
        "                    break\n",
        "\n",
        "        index += 1\n",
        "        iterations += 1\n",
        "\n",
        "    print(f\"  ✓ Added {current_aug_count} augmentations for '{intent}'\")\n",
        "\n",
        "# Combine original + augmented\n",
        "df_original = df[['text', 'intent']]\n",
        "df_augmented_balanced = pd.DataFrame(df_augmented_balanced, columns=[\"text\", \"intent\"])\n",
        "df_final = pd.concat([df_original, df_augmented_balanced], ignore_index=True)\n",
        "\n",
        "# Save to CSV\n",
        "df_final.to_csv(output_path_balanced, index=False)\n",
        "print(f\"\\n✅ Augmentasi selesai dan dataset seimbang disimpan di: {output_path_balanced}\")\n",
        "\n",
        "# =========[ PLOTTING DISTRIBUTIONS ]=========\n",
        "def plot_distribution(data, title):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    data['intent'].value_counts().sort_index().plot(kind='bar', color='skyblue')\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Intent\")\n",
        "    plt.ylabel(\"Jumlah Sampel\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Show before/after plots\n",
        "print(\"\\n📊 Distribusi Sebelum Augmentasi:\")\n",
        "plot_distribution(df_original, \"Distribusi Intent Sebelum Augmentasi\")\n",
        "\n",
        "print(\"\\n📊 Distribusi Setelah Augmentasi:\")\n",
        "plot_distribution(df_final, \"Distribusi Intent Setelah Augmentasi (Seimbang)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "e6RbBAsCTxZD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09326eaf-6abf-4a71-9d98-e68ada8d3811"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1807, 2)\n",
            "            text    intent\n",
            "0          hallo  greeting\n",
            "1           helo  greeting\n",
            "2  selamat siang  greeting\n",
            "3  selamat malam  greeting\n",
            "4   selamat pagi  greeting\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load kedua dataset\n",
        "df1 = pd.read_csv('/content/train.csv')\n",
        "df2 = pd.read_csv('/content/new_augmented_datasets.csv')\n",
        "\n",
        "# Gabungkan dua dataframe\n",
        "df_combined = pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "# (Opsional) Cek hasil\n",
        "print(df_combined.shape)\n",
        "print(df_combined.head())\n",
        "\n",
        "# Simpan hasil gabungan kalau perlu\n",
        "df_combined.to_csv('dataset_gabungan.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: ubah ini ke csv /content/rusdi-prototype-1/train.xlsx\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Path to your Excel file\n",
        "excel_file_path = \"/content/rusdi-prototype-1/train.xlsx\"\n",
        "\n",
        "# Path to save the CSV file\n",
        "csv_file_path = \"/content/train.csv\"\n",
        "\n",
        "try:\n",
        "    # Read the Excel file into a pandas DataFrame\n",
        "    df = pd.read_excel(excel_file_path)\n",
        "\n",
        "    # Convert the DataFrame to a CSV file\n",
        "    df.to_csv(csv_file_path, index=False)  # index=False prevents writing row indices\n",
        "\n",
        "    print(f\"Successfully converted '{excel_file_path}' to '{csv_file_path}'\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at '{excel_file_path}'\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "y-i6zTOdym6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07f1dcf9-3540-4bcc-d860-1843b18a1987"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully converted '/content/rusdi-prototype-1/train.xlsx' to '/content/train.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7cRi5Nhv_9S",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Teks judul default\n",
        "# prompt: the model already trained and saved to drive, but can i test the model without running thee training.\n",
        "\n",
        "# Path ke folder model di Google Drive\n",
        "model_path = \"/content/drive/MyDrive/indobert_intent_model\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "# Load intent classes\n",
        "with open(f\"{model_path}/intent_classes.pkl\", \"rb\") as f:\n",
        "    intent_classes = pickle.load(f)\n",
        "\n",
        "# Load label encoder (jika diperlukan)\n",
        "with open(f\"{model_path}/label_encoder.pkl\", \"rb\") as f:\n",
        "    label_encoder = pickle.load(f)\n",
        "\n",
        "\n",
        "def predict_intent(text, model, tokenizer, intent_classes, device=None):\n",
        "    \"\"\"Memprediksi intent dari teks input\"\"\"\n",
        "\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenisasi input\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Prediksi\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probabilities = torch.softmax(logits, dim=1)\n",
        "        prediction = torch.argmax(probabilities, dim=1).item()\n",
        "        confidence = probabilities[0][prediction].item()\n",
        "\n",
        "    predicted_intent = intent_classes[prediction]\n",
        "\n",
        "    # Dapatkan top 3 intent dengan confidence tertinggi\n",
        "    top_k = 3\n",
        "    if len(intent_classes) < top_k:\n",
        "        top_k = len(intent_classes)\n",
        "\n",
        "    topk_values, topk_indices = torch.topk(probabilities, top_k, dim=1)\n",
        "    topk_intents = [(intent_classes[idx.item()], val.item()) for idx, val in zip(topk_indices[0], topk_values[0])]\n",
        "\n",
        "    return predicted_intent, confidence, topk_intents\n",
        "\n",
        "\n",
        "# Contoh penggunaan\n",
        "    while True:\n",
        "        user_input = input(\"\\nMasukkan teks untuk prediksi intent (ketik 'exit' untuk keluar): \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        intent, confidence, topk = predict_intent(user_input, model, tokenizer, intent_classes)\n",
        "        print(f\"Intent terdeteksi: {intent} (confidence: {confidence:.4f})\")\n",
        "        print(\"Top 3 intent:\")\n",
        "        for i, (intent_name, score) in enumerate(topk):\n",
        "            print(f\"  {i+1}. {intent_name}: {score:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2bICI1U4QZLy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}